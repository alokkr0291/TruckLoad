{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d10e8a17-f60d-4df7-9dd2-992018ca0bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SKU_1L': 0, 'SKU_4L': 1, 'SKU_10L_DTS': 2, 'SKU_10L_NDTS': 3, 'SKU_20L_DTS': 4, 'SKU_20L_NDTS': 5}\n",
      "X_train shape: (167, 9)\n",
      "y_train shape: (167, 5508)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Class names and mapping for one-hot encoding\n",
    "class_name = [\"SKU_1L\", \"SKU_4L\", \"SKU_10L_DTS\", \"SKU_10L_NDTS\", \"SKU_20L_DTS\", \"SKU_20L_NDTS\"]\n",
    "class_mapping = {class_name[i]: i for i in range(len(class_name))}\n",
    "print(class_mapping)\n",
    "\n",
    "# Folder path containing JSON files\n",
    "folder_path = 'Json'\n",
    "\n",
    "def parse_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        parsed_data = json.load(file)\n",
    "    return parsed_data\n",
    "\n",
    "def extract_features_targets(parsed_data):\n",
    "    # Extract truck dimensions and total used SKUs\n",
    "    truck_dimension = {item['key']: int(item['value']) for item in parsed_data['truckDimension']}\n",
    "    total_used_skus = {item['skuName']: int(item['totalUsedSkusCount']) for item in parsed_data['totalUsedSkus']}\n",
    "    \n",
    "    # Extract SKU information\n",
    "    sku_info_data = []\n",
    "    for item in parsed_data['skusInfoData']:\n",
    "        sku_name = item['skuName']\n",
    "        skuLocalPosition = {pos['key']: float(pos['value']) for pos in item['skuLocalPosition']}\n",
    "        \n",
    "        one_hot = [0] * len(class_name)\n",
    "        if sku_name in class_mapping:\n",
    "            one_hot[class_mapping[sku_name]] = 1\n",
    "        \n",
    "        sku_info = [one_hot, [round(skuLocalPosition['x'], 2), round(skuLocalPosition['y'], 2), round(skuLocalPosition['z'], 2)]]\n",
    "        sku_info_data.append(sku_info)\n",
    "    \n",
    "    # Create X_train entry\n",
    "    X_train_entry = [\n",
    "        truck_dimension['Length'], \n",
    "        truck_dimension['Width'], \n",
    "        truck_dimension['Height'], \n",
    "        total_used_skus['SKU_1L'],\n",
    "        total_used_skus['SKU_4L'],\n",
    "        total_used_skus['SKU_10L_DTS'],\n",
    "        total_used_skus['SKU_10L_NDTS'],\n",
    "        total_used_skus['SKU_20L_DTS'],\n",
    "        total_used_skus['SKU_20L_NDTS']\n",
    "    ]\n",
    "    \n",
    "    return X_train_entry, sku_info_data\n",
    "\n",
    "def process_json_files(folder_path):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        file_path = os.path.join(folder_path, json_file)\n",
    "        parsed_data = parse_json_file(file_path)\n",
    "        X_train_entry, sku_info_data = extract_features_targets(parsed_data)\n",
    "        \n",
    "        X_train.append(X_train_entry)\n",
    "        \n",
    "        # Flatten positions and append to y_train\n",
    "        positions = []\n",
    "        for item in sku_info_data:\n",
    "            positions.extend(item[1])\n",
    "        y_train.append(positions)\n",
    "    \n",
    "    return np.array(X_train), y_train\n",
    "\n",
    "# Process JSON files and prepare data\n",
    "X_train, y_train = process_json_files(folder_path)\n",
    "\n",
    "# Find the maximum length of positions\n",
    "max_positions_length = max(len(positions) for positions in y_train)\n",
    "\n",
    "# Pad positions to ensure uniform length\n",
    "for i in range(len(y_train)):\n",
    "    while len(y_train[i]) < max_positions_length:\n",
    "        y_train[i].append(0)  # padding the positions\n",
    "\n",
    "# Convert y_train to a numpy array\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "max_y_train_value = np.max(y_train)\n",
    "\n",
    "# Normalize data\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "y_train = tf.keras.utils.normalize(y_train, axis=1)\n",
    "\n",
    "# Print shapes of the data\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb341a6-1e89-45c9-89aa-330234c27f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TF-Metal/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-06-12 17:59:51.049776: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-06-12 17:59:51.049795: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-06-12 17:59:51.049800: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-06-12 17:59:51.049819: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-12 17:59:51.049831: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,098,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5508</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">710,532</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │        \u001b[38;5;34m20,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m2,098,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5508\u001b[0m)           │       \u001b[38;5;34m710,532\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,518,212</span> (13.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,518,212\u001b[0m (13.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,518,212</span> (13.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,518,212\u001b[0m (13.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(2048, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(y_train.shape[1], activation='linear')\n",
    "])\n",
    "\n",
    "model1.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Print model summary\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef01f6d-48bd-4453-9979-aa07ec798b85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 17:59:51.533703: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - loss: 1.8681e-04 - mae: 0.0090 - val_loss: 1.3510e-04 - val_mae: 0.0070\n",
      "Epoch 2/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.2866e-04 - mae: 0.0069 - val_loss: 9.9091e-05 - val_mae: 0.0061\n",
      "Epoch 3/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.5233e-05 - mae: 0.0060 - val_loss: 7.5850e-05 - val_mae: 0.0055\n",
      "Epoch 4/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.5170e-05 - mae: 0.0055 - val_loss: 6.1476e-05 - val_mae: 0.0051\n",
      "Epoch 5/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.2493e-05 - mae: 0.0052 - val_loss: 5.2194e-05 - val_mae: 0.0048\n",
      "Epoch 6/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.6313e-05 - mae: 0.0049 - val_loss: 4.6451e-05 - val_mae: 0.0044\n",
      "Epoch 7/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.0705e-05 - mae: 0.0046 - val_loss: 4.3428e-05 - val_mae: 0.0042\n",
      "Epoch 8/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7458e-05 - mae: 0.0044 - val_loss: 4.2034e-05 - val_mae: 0.0041\n",
      "Epoch 9/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9078e-05 - mae: 0.0044 - val_loss: 4.1403e-05 - val_mae: 0.0040\n",
      "Epoch 10/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7637e-05 - mae: 0.0042 - val_loss: 4.1235e-05 - val_mae: 0.0040\n",
      "Epoch 11/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6447e-05 - mae: 0.0042 - val_loss: 4.1310e-05 - val_mae: 0.0040\n",
      "Epoch 12/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9630e-05 - mae: 0.0043 - val_loss: 4.1383e-05 - val_mae: 0.0040\n",
      "Epoch 13/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7803e-05 - mae: 0.0042 - val_loss: 4.1401e-05 - val_mae: 0.0040\n",
      "Epoch 14/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7513e-05 - mae: 0.0042 - val_loss: 4.1285e-05 - val_mae: 0.0040\n",
      "Epoch 15/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7570e-05 - mae: 0.0043 - val_loss: 4.1282e-05 - val_mae: 0.0041\n",
      "Epoch 16/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8088e-05 - mae: 0.0043 - val_loss: 4.1266e-05 - val_mae: 0.0040\n",
      "Epoch 17/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7990e-05 - mae: 0.0043 - val_loss: 4.1284e-05 - val_mae: 0.0040\n",
      "Epoch 18/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6327e-05 - mae: 0.0042 - val_loss: 4.1321e-05 - val_mae: 0.0040\n",
      "Epoch 19/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7190e-05 - mae: 0.0042 - val_loss: 4.1210e-05 - val_mae: 0.0040\n",
      "Epoch 20/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7662e-05 - mae: 0.0042 - val_loss: 4.1149e-05 - val_mae: 0.0040\n",
      "Epoch 21/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7194e-05 - mae: 0.0043 - val_loss: 4.1074e-05 - val_mae: 0.0040\n",
      "Epoch 22/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.0503e-05 - mae: 0.0044 - val_loss: 4.1080e-05 - val_mae: 0.0040\n",
      "Epoch 23/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8176e-05 - mae: 0.0043 - val_loss: 4.1198e-05 - val_mae: 0.0041\n",
      "Epoch 24/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4200e-05 - mae: 0.0042 - val_loss: 4.1169e-05 - val_mae: 0.0040\n",
      "Epoch 25/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6840e-05 - mae: 0.0042 - val_loss: 4.1299e-05 - val_mae: 0.0040\n",
      "Epoch 26/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5896e-05 - mae: 0.0042 - val_loss: 4.1298e-05 - val_mae: 0.0041\n",
      "Epoch 27/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9131e-05 - mae: 0.0044 - val_loss: 4.1273e-05 - val_mae: 0.0041\n",
      "Epoch 28/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5839e-05 - mae: 0.0042 - val_loss: 4.1158e-05 - val_mae: 0.0040\n",
      "Epoch 29/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7324e-05 - mae: 0.0042 - val_loss: 4.1204e-05 - val_mae: 0.0040\n",
      "Epoch 30/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7332e-05 - mae: 0.0043 - val_loss: 4.1253e-05 - val_mae: 0.0041\n",
      "Epoch 31/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6299e-05 - mae: 0.0043 - val_loss: 4.1232e-05 - val_mae: 0.0041\n",
      "Epoch 32/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4995e-05 - mae: 0.0042 - val_loss: 4.1174e-05 - val_mae: 0.0040\n",
      "Epoch 33/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6637e-05 - mae: 0.0042 - val_loss: 4.1299e-05 - val_mae: 0.0041\n",
      "Epoch 34/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8278e-05 - mae: 0.0044 - val_loss: 4.1430e-05 - val_mae: 0.0041\n",
      "Epoch 35/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8607e-05 - mae: 0.0044 - val_loss: 4.1271e-05 - val_mae: 0.0041\n",
      "Epoch 36/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7932e-05 - mae: 0.0043 - val_loss: 4.1182e-05 - val_mae: 0.0040\n",
      "Epoch 37/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6657e-05 - mae: 0.0042 - val_loss: 4.1125e-05 - val_mae: 0.0040\n",
      "Epoch 38/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6802e-05 - mae: 0.0042 - val_loss: 4.1148e-05 - val_mae: 0.0041\n",
      "Epoch 39/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7666e-05 - mae: 0.0043 - val_loss: 4.1203e-05 - val_mae: 0.0041\n",
      "Epoch 40/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7725e-05 - mae: 0.0043 - val_loss: 4.1152e-05 - val_mae: 0.0040\n",
      "Epoch 41/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4681e-05 - mae: 0.0041 - val_loss: 4.1167e-05 - val_mae: 0.0040\n",
      "Epoch 42/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5296e-05 - mae: 0.0042 - val_loss: 4.1346e-05 - val_mae: 0.0040\n",
      "Epoch 43/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6685e-05 - mae: 0.0043 - val_loss: 4.1266e-05 - val_mae: 0.0041\n",
      "Epoch 44/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6570e-05 - mae: 0.0043 - val_loss: 4.1241e-05 - val_mae: 0.0040\n",
      "Epoch 45/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6856e-05 - mae: 0.0042 - val_loss: 4.1330e-05 - val_mae: 0.0040\n",
      "Epoch 46/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6630e-05 - mae: 0.0042 - val_loss: 4.1281e-05 - val_mae: 0.0040\n",
      "Epoch 47/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.0141e-05 - mae: 0.0044 - val_loss: 4.1301e-05 - val_mae: 0.0040\n",
      "Epoch 48/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6753e-05 - mae: 0.0042 - val_loss: 4.1234e-05 - val_mae: 0.0040\n",
      "Epoch 49/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7608e-05 - mae: 0.0043 - val_loss: 4.1246e-05 - val_mae: 0.0040\n",
      "Epoch 50/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7851e-05 - mae: 0.0043 - val_loss: 4.1248e-05 - val_mae: 0.0040\n",
      "Epoch 51/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7076e-05 - mae: 0.0042 - val_loss: 4.1126e-05 - val_mae: 0.0040\n",
      "Epoch 52/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8318e-05 - mae: 0.0042 - val_loss: 4.1168e-05 - val_mae: 0.0040\n",
      "Epoch 53/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8077e-05 - mae: 0.0043 - val_loss: 4.1354e-05 - val_mae: 0.0041\n",
      "Epoch 54/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5154e-05 - mae: 0.0042 - val_loss: 4.1488e-05 - val_mae: 0.0041\n",
      "Epoch 55/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6518e-05 - mae: 0.0043 - val_loss: 4.1455e-05 - val_mae: 0.0041\n",
      "Epoch 56/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.0537e-05 - mae: 0.0044 - val_loss: 4.1547e-05 - val_mae: 0.0041\n",
      "Epoch 57/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5562e-05 - mae: 0.0042 - val_loss: 4.1373e-05 - val_mae: 0.0040\n",
      "Epoch 58/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6912e-05 - mae: 0.0042 - val_loss: 4.1389e-05 - val_mae: 0.0040\n",
      "Epoch 59/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6367e-05 - mae: 0.0042 - val_loss: 4.1268e-05 - val_mae: 0.0040\n",
      "Epoch 60/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7300e-05 - mae: 0.0042 - val_loss: 4.1288e-05 - val_mae: 0.0040\n",
      "Epoch 61/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.0202e-05 - mae: 0.0043 - val_loss: 4.1309e-05 - val_mae: 0.0040\n",
      "Epoch 62/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8481e-05 - mae: 0.0043 - val_loss: 4.1169e-05 - val_mae: 0.0040\n",
      "Epoch 63/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9123e-05 - mae: 0.0043 - val_loss: 4.1178e-05 - val_mae: 0.0041\n",
      "Epoch 64/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9196e-05 - mae: 0.0044 - val_loss: 4.1238e-05 - val_mae: 0.0041\n",
      "Epoch 65/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6435e-05 - mae: 0.0043 - val_loss: 4.1200e-05 - val_mae: 0.0040\n",
      "Epoch 66/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7433e-05 - mae: 0.0043 - val_loss: 4.1190e-05 - val_mae: 0.0040\n",
      "Epoch 67/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6575e-05 - mae: 0.0042 - val_loss: 4.1196e-05 - val_mae: 0.0040\n",
      "Epoch 68/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8741e-05 - mae: 0.0043 - val_loss: 4.1199e-05 - val_mae: 0.0040\n",
      "Epoch 69/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8227e-05 - mae: 0.0043 - val_loss: 4.1209e-05 - val_mae: 0.0040\n",
      "Epoch 70/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5585e-05 - mae: 0.0042 - val_loss: 4.1207e-05 - val_mae: 0.0040\n",
      "Epoch 71/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8340e-05 - mae: 0.0043 - val_loss: 4.1165e-05 - val_mae: 0.0040\n",
      "Epoch 72/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.1403e-05 - mae: 0.0044 - val_loss: 4.1189e-05 - val_mae: 0.0041\n",
      "Epoch 73/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9801e-05 - mae: 0.0044 - val_loss: 4.1228e-05 - val_mae: 0.0040\n",
      "Epoch 74/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9664e-05 - mae: 0.0043 - val_loss: 4.1256e-05 - val_mae: 0.0040\n",
      "Epoch 75/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8164e-05 - mae: 0.0043 - val_loss: 4.1203e-05 - val_mae: 0.0041\n",
      "Epoch 76/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6417e-05 - mae: 0.0042 - val_loss: 4.1162e-05 - val_mae: 0.0041\n",
      "Epoch 77/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8105e-05 - mae: 0.0043 - val_loss: 4.1166e-05 - val_mae: 0.0040\n",
      "Epoch 78/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8374e-05 - mae: 0.0043 - val_loss: 4.1306e-05 - val_mae: 0.0040\n",
      "Epoch 79/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5220e-05 - mae: 0.0041 - val_loss: 4.1447e-05 - val_mae: 0.0040\n",
      "Epoch 80/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8885e-05 - mae: 0.0043 - val_loss: 4.1453e-05 - val_mae: 0.0040\n",
      "Epoch 81/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6510e-05 - mae: 0.0042 - val_loss: 4.1265e-05 - val_mae: 0.0041\n",
      "Epoch 82/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8952e-05 - mae: 0.0043 - val_loss: 4.1474e-05 - val_mae: 0.0041\n",
      "Epoch 83/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7717e-05 - mae: 0.0043 - val_loss: 4.1814e-05 - val_mae: 0.0040\n",
      "Epoch 84/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7115e-05 - mae: 0.0043 - val_loss: 4.1595e-05 - val_mae: 0.0041\n",
      "Epoch 85/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8714e-05 - mae: 0.0043 - val_loss: 4.1543e-05 - val_mae: 0.0040\n",
      "Epoch 86/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8214e-05 - mae: 0.0042 - val_loss: 4.1549e-05 - val_mae: 0.0040\n",
      "Epoch 87/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7821e-05 - mae: 0.0042 - val_loss: 4.1268e-05 - val_mae: 0.0040\n",
      "Epoch 88/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8519e-05 - mae: 0.0042 - val_loss: 4.1160e-05 - val_mae: 0.0040\n",
      "Epoch 89/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6950e-05 - mae: 0.0042 - val_loss: 4.1062e-05 - val_mae: 0.0040\n",
      "Epoch 90/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8562e-05 - mae: 0.0042 - val_loss: 4.1083e-05 - val_mae: 0.0040\n",
      "Epoch 91/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8675e-05 - mae: 0.0043 - val_loss: 4.1252e-05 - val_mae: 0.0041\n",
      "Epoch 92/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6244e-05 - mae: 0.0043 - val_loss: 4.1497e-05 - val_mae: 0.0042\n",
      "Epoch 93/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.0055e-05 - mae: 0.0044 - val_loss: 4.1604e-05 - val_mae: 0.0041\n",
      "Epoch 94/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7378e-05 - mae: 0.0043 - val_loss: 4.1539e-05 - val_mae: 0.0040\n",
      "Epoch 95/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7669e-05 - mae: 0.0043 - val_loss: 4.1450e-05 - val_mae: 0.0040\n",
      "Epoch 96/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7337e-05 - mae: 0.0042 - val_loss: 4.1276e-05 - val_mae: 0.0040\n",
      "Epoch 97/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6043e-05 - mae: 0.0041 - val_loss: 4.1134e-05 - val_mae: 0.0040\n",
      "Epoch 98/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9614e-05 - mae: 0.0044 - val_loss: 4.1267e-05 - val_mae: 0.0041\n",
      "Epoch 99/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7354e-05 - mae: 0.0043 - val_loss: 4.1270e-05 - val_mae: 0.0040\n",
      "Epoch 100/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5881e-05 - mae: 0.0042 - val_loss: 4.1402e-05 - val_mae: 0.0040\n",
      "Epoch 101/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7949e-05 - mae: 0.0043 - val_loss: 4.1607e-05 - val_mae: 0.0041\n",
      "Epoch 102/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5607e-05 - mae: 0.0043 - val_loss: 4.1619e-05 - val_mae: 0.0041\n",
      "Epoch 103/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8813e-05 - mae: 0.0043 - val_loss: 4.1793e-05 - val_mae: 0.0040\n",
      "Epoch 104/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5862e-05 - mae: 0.0042 - val_loss: 4.1401e-05 - val_mae: 0.0041\n",
      "Epoch 105/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.7269e-05 - mae: 0.0042 - val_loss: 4.1132e-05 - val_mae: 0.0040\n",
      "Epoch 106/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8423e-05 - mae: 0.0043 - val_loss: 4.1093e-05 - val_mae: 0.0040\n",
      "Epoch 107/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6956e-05 - mae: 0.0042 - val_loss: 4.1337e-05 - val_mae: 0.0041\n",
      "Epoch 108/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9480e-05 - mae: 0.0044 - val_loss: 4.1408e-05 - val_mae: 0.0041\n",
      "Epoch 109/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7725e-05 - mae: 0.0043 - val_loss: 4.1242e-05 - val_mae: 0.0041\n",
      "Epoch 110/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6598e-05 - mae: 0.0042 - val_loss: 4.1207e-05 - val_mae: 0.0041\n",
      "Epoch 111/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7122e-05 - mae: 0.0042 - val_loss: 4.1201e-05 - val_mae: 0.0041\n",
      "Epoch 112/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8104e-05 - mae: 0.0043 - val_loss: 4.1291e-05 - val_mae: 0.0040\n",
      "Epoch 113/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5065e-05 - mae: 0.0041 - val_loss: 4.1355e-05 - val_mae: 0.0040\n",
      "Epoch 114/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6461e-05 - mae: 0.0042 - val_loss: 4.1446e-05 - val_mae: 0.0041\n",
      "Epoch 115/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8838e-05 - mae: 0.0043 - val_loss: 4.1352e-05 - val_mae: 0.0041\n",
      "Epoch 116/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6885e-05 - mae: 0.0042 - val_loss: 4.1158e-05 - val_mae: 0.0040\n",
      "Epoch 117/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8372e-05 - mae: 0.0043 - val_loss: 4.1132e-05 - val_mae: 0.0040\n",
      "Epoch 118/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7218e-05 - mae: 0.0042 - val_loss: 4.1107e-05 - val_mae: 0.0040\n",
      "Epoch 119/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9923e-05 - mae: 0.0043 - val_loss: 4.1151e-05 - val_mae: 0.0040\n",
      "Epoch 120/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8075e-05 - mae: 0.0043 - val_loss: 4.1378e-05 - val_mae: 0.0041\n",
      "Epoch 121/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9829e-05 - mae: 0.0044 - val_loss: 4.1290e-05 - val_mae: 0.0041\n",
      "Epoch 122/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9919e-05 - mae: 0.0044 - val_loss: 4.1254e-05 - val_mae: 0.0040\n",
      "Epoch 123/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.0117e-05 - mae: 0.0043 - val_loss: 4.1304e-05 - val_mae: 0.0040\n",
      "Epoch 124/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6182e-05 - mae: 0.0042 - val_loss: 4.1314e-05 - val_mae: 0.0040\n",
      "Epoch 125/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6307e-05 - mae: 0.0042 - val_loss: 4.1291e-05 - val_mae: 0.0040\n",
      "Epoch 126/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8136e-05 - mae: 0.0042 - val_loss: 4.1573e-05 - val_mae: 0.0040\n",
      "Epoch 127/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7871e-05 - mae: 0.0042 - val_loss: 4.1511e-05 - val_mae: 0.0041\n",
      "Epoch 128/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9545e-05 - mae: 0.0044 - val_loss: 4.1433e-05 - val_mae: 0.0041\n",
      "Epoch 129/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6434e-05 - mae: 0.0043 - val_loss: 4.1503e-05 - val_mae: 0.0041\n",
      "Epoch 130/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6671e-05 - mae: 0.0043 - val_loss: 4.1560e-05 - val_mae: 0.0041\n",
      "Epoch 131/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6630e-05 - mae: 0.0042 - val_loss: 4.1478e-05 - val_mae: 0.0041\n",
      "Epoch 132/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7010e-05 - mae: 0.0043 - val_loss: 4.1543e-05 - val_mae: 0.0041\n",
      "Epoch 133/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.0003e-05 - mae: 0.0044 - val_loss: 4.1954e-05 - val_mae: 0.0041\n",
      "Epoch 134/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6568e-05 - mae: 0.0042 - val_loss: 4.1616e-05 - val_mae: 0.0040\n",
      "Epoch 135/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5692e-05 - mae: 0.0042 - val_loss: 4.1270e-05 - val_mae: 0.0041\n",
      "Epoch 136/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7456e-05 - mae: 0.0043 - val_loss: 4.1338e-05 - val_mae: 0.0041\n",
      "Epoch 137/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7950e-05 - mae: 0.0043 - val_loss: 4.1364e-05 - val_mae: 0.0041\n",
      "Epoch 138/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8416e-05 - mae: 0.0043 - val_loss: 4.1302e-05 - val_mae: 0.0041\n",
      "Epoch 139/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9326e-05 - mae: 0.0044 - val_loss: 4.1164e-05 - val_mae: 0.0040\n",
      "Epoch 140/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8556e-05 - mae: 0.0043 - val_loss: 4.1204e-05 - val_mae: 0.0040\n",
      "Epoch 141/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8035e-05 - mae: 0.0042 - val_loss: 4.1175e-05 - val_mae: 0.0040\n",
      "Epoch 142/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6890e-05 - mae: 0.0043 - val_loss: 4.1366e-05 - val_mae: 0.0041\n",
      "Epoch 143/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7739e-05 - mae: 0.0043 - val_loss: 4.1699e-05 - val_mae: 0.0041\n",
      "Epoch 144/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7888e-05 - mae: 0.0043 - val_loss: 4.1593e-05 - val_mae: 0.0041\n",
      "Epoch 145/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.1208e-05 - mae: 0.0045 - val_loss: 4.1413e-05 - val_mae: 0.0041\n",
      "Epoch 146/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7454e-05 - mae: 0.0043 - val_loss: 4.1270e-05 - val_mae: 0.0040\n",
      "Epoch 147/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5352e-05 - mae: 0.0041 - val_loss: 4.1200e-05 - val_mae: 0.0039\n",
      "Epoch 148/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8851e-05 - mae: 0.0042 - val_loss: 4.1190e-05 - val_mae: 0.0040\n",
      "Epoch 149/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7703e-05 - mae: 0.0042 - val_loss: 4.1301e-05 - val_mae: 0.0041\n",
      "Epoch 150/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8320e-05 - mae: 0.0043 - val_loss: 4.1342e-05 - val_mae: 0.0041\n",
      "Epoch 151/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4824e-05 - mae: 0.0042 - val_loss: 4.1390e-05 - val_mae: 0.0040\n",
      "Epoch 152/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8026e-05 - mae: 0.0042 - val_loss: 4.1922e-05 - val_mae: 0.0041\n",
      "Epoch 153/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7323e-05 - mae: 0.0043 - val_loss: 4.1982e-05 - val_mae: 0.0041\n",
      "Epoch 154/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7021e-05 - mae: 0.0043 - val_loss: 4.2014e-05 - val_mae: 0.0041\n",
      "Epoch 155/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7307e-05 - mae: 0.0043 - val_loss: 4.1907e-05 - val_mae: 0.0042\n",
      "Epoch 156/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7391e-05 - mae: 0.0044 - val_loss: 4.1459e-05 - val_mae: 0.0041\n",
      "Epoch 157/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5876e-05 - mae: 0.0042 - val_loss: 4.1195e-05 - val_mae: 0.0041\n",
      "Epoch 158/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.0308e-05 - mae: 0.0044 - val_loss: 4.1155e-05 - val_mae: 0.0041\n",
      "Epoch 159/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7985e-05 - mae: 0.0043 - val_loss: 4.0991e-05 - val_mae: 0.0040\n",
      "Epoch 160/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8362e-05 - mae: 0.0042 - val_loss: 4.1138e-05 - val_mae: 0.0040\n",
      "Epoch 161/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.0873e-05 - mae: 0.0043 - val_loss: 4.1273e-05 - val_mae: 0.0040\n",
      "Epoch 162/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9494e-05 - mae: 0.0042 - val_loss: 4.1305e-05 - val_mae: 0.0040\n",
      "Epoch 163/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8896e-05 - mae: 0.0043 - val_loss: 4.1404e-05 - val_mae: 0.0040\n",
      "Epoch 164/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5290e-05 - mae: 0.0042 - val_loss: 4.1417e-05 - val_mae: 0.0040\n",
      "Epoch 165/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.0579e-05 - mae: 0.0044 - val_loss: 4.1667e-05 - val_mae: 0.0041\n",
      "Epoch 166/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8318e-05 - mae: 0.0043 - val_loss: 4.1558e-05 - val_mae: 0.0041\n",
      "Epoch 167/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8684e-05 - mae: 0.0043 - val_loss: 4.1537e-05 - val_mae: 0.0041\n",
      "Epoch 168/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5741e-05 - mae: 0.0042 - val_loss: 4.1316e-05 - val_mae: 0.0041\n",
      "Epoch 169/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8741e-05 - mae: 0.0043 - val_loss: 4.1503e-05 - val_mae: 0.0041\n",
      "Epoch 170/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8344e-05 - mae: 0.0043 - val_loss: 4.1392e-05 - val_mae: 0.0041\n",
      "Epoch 171/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8108e-05 - mae: 0.0043 - val_loss: 4.1409e-05 - val_mae: 0.0041\n",
      "Epoch 172/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8771e-05 - mae: 0.0044 - val_loss: 4.1379e-05 - val_mae: 0.0041\n",
      "Epoch 173/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9992e-05 - mae: 0.0044 - val_loss: 4.1347e-05 - val_mae: 0.0041\n",
      "Epoch 174/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7497e-05 - mae: 0.0043 - val_loss: 4.1279e-05 - val_mae: 0.0040\n",
      "Epoch 175/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9270e-05 - mae: 0.0043 - val_loss: 4.1271e-05 - val_mae: 0.0040\n",
      "Epoch 176/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7821e-05 - mae: 0.0042 - val_loss: 4.1128e-05 - val_mae: 0.0040\n",
      "Epoch 177/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6745e-05 - mae: 0.0042 - val_loss: 4.1129e-05 - val_mae: 0.0040\n",
      "Epoch 178/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7014e-05 - mae: 0.0042 - val_loss: 4.1311e-05 - val_mae: 0.0040\n",
      "Epoch 179/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6216e-05 - mae: 0.0042 - val_loss: 4.1437e-05 - val_mae: 0.0040\n",
      "Epoch 180/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7143e-05 - mae: 0.0042 - val_loss: 4.1398e-05 - val_mae: 0.0040\n",
      "Epoch 181/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5834e-05 - mae: 0.0042 - val_loss: 4.1351e-05 - val_mae: 0.0040\n",
      "Epoch 182/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8921e-05 - mae: 0.0043 - val_loss: 4.1440e-05 - val_mae: 0.0041\n",
      "Epoch 183/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8454e-05 - mae: 0.0043 - val_loss: 4.1512e-05 - val_mae: 0.0041\n",
      "Epoch 184/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6557e-05 - mae: 0.0042 - val_loss: 4.1452e-05 - val_mae: 0.0041\n",
      "Epoch 185/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6107e-05 - mae: 0.0042 - val_loss: 4.1264e-05 - val_mae: 0.0041\n",
      "Epoch 186/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7843e-05 - mae: 0.0043 - val_loss: 4.1249e-05 - val_mae: 0.0041\n",
      "Epoch 187/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7918e-05 - mae: 0.0043 - val_loss: 4.1211e-05 - val_mae: 0.0040\n",
      "Epoch 188/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7801e-05 - mae: 0.0042 - val_loss: 4.1399e-05 - val_mae: 0.0040\n",
      "Epoch 189/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9957e-05 - mae: 0.0043 - val_loss: 4.1278e-05 - val_mae: 0.0041\n",
      "Epoch 190/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6394e-05 - mae: 0.0042 - val_loss: 4.1394e-05 - val_mae: 0.0040\n",
      "Epoch 191/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5635e-05 - mae: 0.0042 - val_loss: 4.1626e-05 - val_mae: 0.0040\n",
      "Epoch 192/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7030e-05 - mae: 0.0042 - val_loss: 4.1423e-05 - val_mae: 0.0040\n",
      "Epoch 193/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5328e-05 - mae: 0.0042 - val_loss: 4.1436e-05 - val_mae: 0.0040\n",
      "Epoch 194/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9000e-05 - mae: 0.0043 - val_loss: 4.1519e-05 - val_mae: 0.0041\n",
      "Epoch 195/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7282e-05 - mae: 0.0043 - val_loss: 4.1477e-05 - val_mae: 0.0041\n",
      "Epoch 196/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7983e-05 - mae: 0.0043 - val_loss: 4.1172e-05 - val_mae: 0.0040\n",
      "Epoch 197/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7181e-05 - mae: 0.0042 - val_loss: 4.1017e-05 - val_mae: 0.0040\n",
      "Epoch 198/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9698e-05 - mae: 0.0043 - val_loss: 4.1156e-05 - val_mae: 0.0041\n",
      "Epoch 199/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6913e-05 - mae: 0.0043 - val_loss: 4.1207e-05 - val_mae: 0.0041\n",
      "Epoch 200/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8600e-05 - mae: 0.0043 - val_loss: 4.1263e-05 - val_mae: 0.0041\n",
      "Epoch 201/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8633e-05 - mae: 0.0043 - val_loss: 4.1302e-05 - val_mae: 0.0040\n",
      "Epoch 202/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9015e-05 - mae: 0.0043 - val_loss: 4.1197e-05 - val_mae: 0.0040\n",
      "Epoch 203/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8966e-05 - mae: 0.0043 - val_loss: 4.1172e-05 - val_mae: 0.0040\n",
      "Epoch 204/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7358e-05 - mae: 0.0042 - val_loss: 4.1415e-05 - val_mae: 0.0041\n",
      "Epoch 205/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.0024e-05 - mae: 0.0044 - val_loss: 4.1665e-05 - val_mae: 0.0042\n",
      "Epoch 206/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9345e-05 - mae: 0.0044 - val_loss: 4.1639e-05 - val_mae: 0.0041\n",
      "Epoch 207/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8170e-05 - mae: 0.0044 - val_loss: 4.1643e-05 - val_mae: 0.0041\n",
      "Epoch 208/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8010e-05 - mae: 0.0043 - val_loss: 4.1646e-05 - val_mae: 0.0041\n",
      "Epoch 209/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8397e-05 - mae: 0.0043 - val_loss: 4.1573e-05 - val_mae: 0.0040\n",
      "Epoch 210/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8967e-05 - mae: 0.0043 - val_loss: 4.1552e-05 - val_mae: 0.0040\n",
      "Epoch 211/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7976e-05 - mae: 0.0042 - val_loss: 4.1347e-05 - val_mae: 0.0040\n",
      "Epoch 212/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5752e-05 - mae: 0.0041 - val_loss: 4.1340e-05 - val_mae: 0.0040\n",
      "Epoch 213/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9776e-05 - mae: 0.0043 - val_loss: 4.1540e-05 - val_mae: 0.0041\n",
      "Epoch 214/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7932e-05 - mae: 0.0043 - val_loss: 4.1365e-05 - val_mae: 0.0041\n",
      "Epoch 215/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7959e-05 - mae: 0.0043 - val_loss: 4.1195e-05 - val_mae: 0.0040\n",
      "Epoch 216/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5715e-05 - mae: 0.0042 - val_loss: 4.1118e-05 - val_mae: 0.0041\n",
      "Epoch 217/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.8758e-05 - mae: 0.0043 - val_loss: 4.1226e-05 - val_mae: 0.0041\n",
      "Epoch 218/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.9976e-05 - mae: 0.0043 - val_loss: 4.1484e-05 - val_mae: 0.0041\n",
      "Epoch 219/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6804e-05 - mae: 0.0043 - val_loss: 4.1446e-05 - val_mae: 0.0041\n",
      "Epoch 220/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9889e-05 - mae: 0.0044 - val_loss: 4.1387e-05 - val_mae: 0.0040\n",
      "Epoch 221/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7787e-05 - mae: 0.0042 - val_loss: 4.1203e-05 - val_mae: 0.0040\n",
      "Epoch 222/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8646e-05 - mae: 0.0043 - val_loss: 4.1112e-05 - val_mae: 0.0040\n",
      "Epoch 223/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5720e-05 - mae: 0.0041 - val_loss: 4.1222e-05 - val_mae: 0.0040\n",
      "Epoch 224/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6884e-05 - mae: 0.0042 - val_loss: 4.1247e-05 - val_mae: 0.0041\n",
      "Epoch 225/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8435e-05 - mae: 0.0043 - val_loss: 4.1149e-05 - val_mae: 0.0040\n",
      "Epoch 226/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5828e-05 - mae: 0.0042 - val_loss: 4.1064e-05 - val_mae: 0.0040\n",
      "Epoch 227/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6676e-05 - mae: 0.0042 - val_loss: 4.1129e-05 - val_mae: 0.0040\n",
      "Epoch 228/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7196e-05 - mae: 0.0042 - val_loss: 4.1303e-05 - val_mae: 0.0040\n",
      "Epoch 229/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6816e-05 - mae: 0.0043 - val_loss: 4.1420e-05 - val_mae: 0.0041\n",
      "Epoch 230/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8809e-05 - mae: 0.0044 - val_loss: 4.1490e-05 - val_mae: 0.0041\n",
      "Epoch 231/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7699e-05 - mae: 0.0043 - val_loss: 4.1553e-05 - val_mae: 0.0040\n",
      "Epoch 232/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7156e-05 - mae: 0.0042 - val_loss: 4.1304e-05 - val_mae: 0.0040\n",
      "Epoch 233/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7939e-05 - mae: 0.0043 - val_loss: 4.1264e-05 - val_mae: 0.0041\n",
      "Epoch 234/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6812e-05 - mae: 0.0043 - val_loss: 4.1206e-05 - val_mae: 0.0041\n",
      "Epoch 235/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7016e-05 - mae: 0.0043 - val_loss: 4.1217e-05 - val_mae: 0.0040\n",
      "Epoch 236/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7167e-05 - mae: 0.0042 - val_loss: 4.1394e-05 - val_mae: 0.0041\n",
      "Epoch 237/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5633e-05 - mae: 0.0042 - val_loss: 4.1362e-05 - val_mae: 0.0041\n",
      "Epoch 238/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7139e-05 - mae: 0.0043 - val_loss: 4.1243e-05 - val_mae: 0.0040\n",
      "Epoch 239/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8745e-05 - mae: 0.0043 - val_loss: 4.1203e-05 - val_mae: 0.0040\n",
      "Epoch 240/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8145e-05 - mae: 0.0042 - val_loss: 4.1449e-05 - val_mae: 0.0040\n",
      "Epoch 241/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7815e-05 - mae: 0.0042 - val_loss: 4.1536e-05 - val_mae: 0.0040\n",
      "Epoch 242/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6033e-05 - mae: 0.0042 - val_loss: 4.1205e-05 - val_mae: 0.0040\n",
      "Epoch 243/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5935e-05 - mae: 0.0041 - val_loss: 4.1062e-05 - val_mae: 0.0040\n",
      "Epoch 244/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5819e-05 - mae: 0.0041 - val_loss: 4.1214e-05 - val_mae: 0.0040\n",
      "Epoch 245/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7160e-05 - mae: 0.0042 - val_loss: 4.1678e-05 - val_mae: 0.0041\n",
      "Epoch 246/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7612e-05 - mae: 0.0043 - val_loss: 4.1757e-05 - val_mae: 0.0041\n",
      "Epoch 247/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6382e-05 - mae: 0.0042 - val_loss: 4.1466e-05 - val_mae: 0.0040\n",
      "Epoch 248/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6543e-05 - mae: 0.0042 - val_loss: 4.1275e-05 - val_mae: 0.0041\n",
      "Epoch 249/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6404e-05 - mae: 0.0042 - val_loss: 4.1215e-05 - val_mae: 0.0040\n",
      "Epoch 250/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6618e-05 - mae: 0.0042 - val_loss: 4.1297e-05 - val_mae: 0.0040\n",
      "Epoch 251/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5921e-05 - mae: 0.0042 - val_loss: 4.1566e-05 - val_mae: 0.0041\n",
      "Epoch 252/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6955e-05 - mae: 0.0043 - val_loss: 4.1728e-05 - val_mae: 0.0041\n",
      "Epoch 253/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7744e-05 - mae: 0.0043 - val_loss: 4.1498e-05 - val_mae: 0.0041\n",
      "Epoch 254/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7935e-05 - mae: 0.0043 - val_loss: 4.1576e-05 - val_mae: 0.0041\n",
      "Epoch 255/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8088e-05 - mae: 0.0043 - val_loss: 4.1277e-05 - val_mae: 0.0041\n",
      "Epoch 256/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9052e-05 - mae: 0.0043 - val_loss: 4.1126e-05 - val_mae: 0.0041\n",
      "Epoch 257/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7932e-05 - mae: 0.0043 - val_loss: 4.1024e-05 - val_mae: 0.0041\n",
      "Epoch 258/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8473e-05 - mae: 0.0044 - val_loss: 4.0851e-05 - val_mae: 0.0041\n",
      "Epoch 259/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8094e-05 - mae: 0.0043 - val_loss: 4.0685e-05 - val_mae: 0.0040\n",
      "Epoch 260/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5267e-05 - mae: 0.0042 - val_loss: 4.0707e-05 - val_mae: 0.0040\n",
      "Epoch 261/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6306e-05 - mae: 0.0042 - val_loss: 4.1040e-05 - val_mae: 0.0041\n",
      "Epoch 262/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4769e-05 - mae: 0.0042 - val_loss: 4.0937e-05 - val_mae: 0.0041\n",
      "Epoch 263/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7668e-05 - mae: 0.0044 - val_loss: 4.0724e-05 - val_mae: 0.0041\n",
      "Epoch 264/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6822e-05 - mae: 0.0043 - val_loss: 4.0374e-05 - val_mae: 0.0040\n",
      "Epoch 265/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4408e-05 - mae: 0.0042 - val_loss: 4.0323e-05 - val_mae: 0.0041\n",
      "Epoch 266/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7148e-05 - mae: 0.0043 - val_loss: 4.0877e-05 - val_mae: 0.0041\n",
      "Epoch 267/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4904e-05 - mae: 0.0042 - val_loss: 4.1216e-05 - val_mae: 0.0041\n",
      "Epoch 268/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2500e-05 - mae: 0.0041 - val_loss: 4.1079e-05 - val_mae: 0.0041\n",
      "Epoch 269/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5534e-05 - mae: 0.0042 - val_loss: 4.0002e-05 - val_mae: 0.0040\n",
      "Epoch 270/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4852e-05 - mae: 0.0041 - val_loss: 3.9236e-05 - val_mae: 0.0039\n",
      "Epoch 271/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4503e-05 - mae: 0.0041 - val_loss: 3.8961e-05 - val_mae: 0.0039\n",
      "Epoch 272/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8375e-05 - mae: 0.0042 - val_loss: 3.9689e-05 - val_mae: 0.0039\n",
      "Epoch 273/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.2273e-05 - mae: 0.0040 - val_loss: 3.8978e-05 - val_mae: 0.0039\n",
      "Epoch 274/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4716e-05 - mae: 0.0041 - val_loss: 3.9834e-05 - val_mae: 0.0040\n",
      "Epoch 275/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.0872e-05 - mae: 0.0040 - val_loss: 3.9804e-05 - val_mae: 0.0040\n",
      "Epoch 276/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2650e-05 - mae: 0.0040 - val_loss: 3.8742e-05 - val_mae: 0.0039\n",
      "Epoch 277/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.3616e-05 - mae: 0.0040 - val_loss: 3.9709e-05 - val_mae: 0.0039\n",
      "Epoch 278/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5067e-05 - mae: 0.0041 - val_loss: 4.0331e-05 - val_mae: 0.0039\n",
      "Epoch 279/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2853e-05 - mae: 0.0040 - val_loss: 3.8448e-05 - val_mae: 0.0038\n",
      "Epoch 280/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.3314e-05 - mae: 0.0040 - val_loss: 3.8421e-05 - val_mae: 0.0038\n",
      "Epoch 281/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.3446e-05 - mae: 0.0039 - val_loss: 3.8953e-05 - val_mae: 0.0038\n",
      "Epoch 282/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2689e-05 - mae: 0.0039 - val_loss: 4.1360e-05 - val_mae: 0.0040\n",
      "Epoch 283/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.3169e-05 - mae: 0.0040 - val_loss: 3.8798e-05 - val_mae: 0.0038\n",
      "Epoch 284/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.3386e-05 - mae: 0.0039 - val_loss: 3.9073e-05 - val_mae: 0.0038\n",
      "Epoch 285/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.3303e-05 - mae: 0.0040 - val_loss: 3.9306e-05 - val_mae: 0.0039\n",
      "Epoch 286/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.3400e-05 - mae: 0.0040 - val_loss: 3.8553e-05 - val_mae: 0.0039\n",
      "Epoch 287/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.1749e-05 - mae: 0.0039 - val_loss: 3.9688e-05 - val_mae: 0.0039\n",
      "Epoch 288/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.3401e-05 - mae: 0.0040 - val_loss: 3.9285e-05 - val_mae: 0.0038\n",
      "Epoch 289/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.1843e-05 - mae: 0.0039 - val_loss: 4.0351e-05 - val_mae: 0.0039\n",
      "Epoch 290/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4033e-05 - mae: 0.0040 - val_loss: 3.8482e-05 - val_mae: 0.0038\n",
      "Epoch 291/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.3715e-05 - mae: 0.0039 - val_loss: 3.8498e-05 - val_mae: 0.0038\n",
      "Epoch 292/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.3378e-05 - mae: 0.0040 - val_loss: 4.0634e-05 - val_mae: 0.0040\n",
      "Epoch 293/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.2533e-05 - mae: 0.0040 - val_loss: 4.0813e-05 - val_mae: 0.0039\n",
      "Epoch 294/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.2891e-05 - mae: 0.0040 - val_loss: 3.9972e-05 - val_mae: 0.0039\n",
      "Epoch 295/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2544e-05 - mae: 0.0039 - val_loss: 3.9858e-05 - val_mae: 0.0039\n",
      "Epoch 296/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2286e-05 - mae: 0.0039 - val_loss: 3.8654e-05 - val_mae: 0.0039\n",
      "Epoch 297/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7002e-05 - mae: 0.0037 - val_loss: 3.8603e-05 - val_mae: 0.0039\n",
      "Epoch 298/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2473e-05 - mae: 0.0039 - val_loss: 3.9398e-05 - val_mae: 0.0039\n",
      "Epoch 299/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2845e-05 - mae: 0.0040 - val_loss: 3.9737e-05 - val_mae: 0.0039\n",
      "Epoch 300/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0803e-05 - mae: 0.0038 - val_loss: 3.9052e-05 - val_mae: 0.0039\n",
      "Epoch 301/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0281e-05 - mae: 0.0038 - val_loss: 3.9114e-05 - val_mae: 0.0039\n",
      "Epoch 302/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1724e-05 - mae: 0.0039 - val_loss: 3.8771e-05 - val_mae: 0.0038\n",
      "Epoch 303/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2069e-05 - mae: 0.0039 - val_loss: 3.9232e-05 - val_mae: 0.0039\n",
      "Epoch 304/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7311e-05 - mae: 0.0038 - val_loss: 4.1589e-05 - val_mae: 0.0040\n",
      "Epoch 305/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2390e-05 - mae: 0.0040 - val_loss: 3.9550e-05 - val_mae: 0.0039\n",
      "Epoch 306/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.3187e-05 - mae: 0.0039 - val_loss: 3.9469e-05 - val_mae: 0.0038\n",
      "Epoch 307/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0854e-05 - mae: 0.0039 - val_loss: 3.9368e-05 - val_mae: 0.0039\n",
      "Epoch 308/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9784e-05 - mae: 0.0039 - val_loss: 3.8759e-05 - val_mae: 0.0039\n",
      "Epoch 309/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9339e-05 - mae: 0.0038 - val_loss: 3.7952e-05 - val_mae: 0.0038\n",
      "Epoch 310/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8634e-05 - mae: 0.0038 - val_loss: 3.8747e-05 - val_mae: 0.0039\n",
      "Epoch 311/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9305e-05 - mae: 0.0038 - val_loss: 3.8485e-05 - val_mae: 0.0039\n",
      "Epoch 312/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1094e-05 - mae: 0.0039 - val_loss: 3.7961e-05 - val_mae: 0.0038\n",
      "Epoch 313/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9414e-05 - mae: 0.0038 - val_loss: 3.7949e-05 - val_mae: 0.0038\n",
      "Epoch 314/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9559e-05 - mae: 0.0038 - val_loss: 3.8733e-05 - val_mae: 0.0039\n",
      "Epoch 315/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2692e-05 - mae: 0.0039 - val_loss: 3.9629e-05 - val_mae: 0.0038\n",
      "Epoch 316/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2588e-05 - mae: 0.0038 - val_loss: 4.0441e-05 - val_mae: 0.0038\n",
      "Epoch 317/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1123e-05 - mae: 0.0039 - val_loss: 3.9466e-05 - val_mae: 0.0039\n",
      "Epoch 318/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8843e-05 - mae: 0.0038 - val_loss: 3.8080e-05 - val_mae: 0.0038\n",
      "Epoch 319/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0506e-05 - mae: 0.0038 - val_loss: 3.8151e-05 - val_mae: 0.0039\n",
      "Epoch 320/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1813e-05 - mae: 0.0040 - val_loss: 4.0566e-05 - val_mae: 0.0040\n",
      "Epoch 321/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1605e-05 - mae: 0.0040 - val_loss: 4.0973e-05 - val_mae: 0.0039\n",
      "Epoch 322/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.2919e-05 - mae: 0.0039 - val_loss: 3.9885e-05 - val_mae: 0.0038\n",
      "Epoch 323/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1860e-05 - mae: 0.0038 - val_loss: 3.8769e-05 - val_mae: 0.0039\n",
      "Epoch 324/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.3234e-05 - mae: 0.0039 - val_loss: 3.9103e-05 - val_mae: 0.0039\n",
      "Epoch 325/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.1146e-05 - mae: 0.0040 - val_loss: 4.0983e-05 - val_mae: 0.0040\n",
      "Epoch 326/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2082e-05 - mae: 0.0040 - val_loss: 3.9766e-05 - val_mae: 0.0039\n",
      "Epoch 327/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1764e-05 - mae: 0.0039 - val_loss: 3.8775e-05 - val_mae: 0.0039\n",
      "Epoch 328/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0840e-05 - mae: 0.0039 - val_loss: 3.8644e-05 - val_mae: 0.0038\n",
      "Epoch 329/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7374e-05 - mae: 0.0037 - val_loss: 3.8090e-05 - val_mae: 0.0038\n",
      "Epoch 330/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0187e-05 - mae: 0.0038 - val_loss: 3.8732e-05 - val_mae: 0.0039\n",
      "Epoch 331/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9452e-05 - mae: 0.0038 - val_loss: 3.9131e-05 - val_mae: 0.0039\n",
      "Epoch 332/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9726e-05 - mae: 0.0038 - val_loss: 3.8726e-05 - val_mae: 0.0039\n",
      "Epoch 333/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7551e-05 - mae: 0.0037 - val_loss: 3.7653e-05 - val_mae: 0.0038\n",
      "Epoch 334/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1791e-05 - mae: 0.0039 - val_loss: 3.8195e-05 - val_mae: 0.0038\n",
      "Epoch 335/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.0459e-05 - mae: 0.0038 - val_loss: 4.0264e-05 - val_mae: 0.0039\n",
      "Epoch 336/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7847e-05 - mae: 0.0038 - val_loss: 3.9495e-05 - val_mae: 0.0038\n",
      "Epoch 337/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7386e-05 - mae: 0.0037 - val_loss: 3.7620e-05 - val_mae: 0.0038\n",
      "Epoch 338/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7523e-05 - mae: 0.0037 - val_loss: 3.8245e-05 - val_mae: 0.0038\n",
      "Epoch 339/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9962e-05 - mae: 0.0038 - val_loss: 3.9468e-05 - val_mae: 0.0039\n",
      "Epoch 340/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1668e-05 - mae: 0.0039 - val_loss: 3.9223e-05 - val_mae: 0.0038\n",
      "Epoch 341/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1296e-05 - mae: 0.0039 - val_loss: 3.7396e-05 - val_mae: 0.0038\n",
      "Epoch 342/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9022e-05 - mae: 0.0038 - val_loss: 3.7239e-05 - val_mae: 0.0038\n",
      "Epoch 343/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0288e-05 - mae: 0.0038 - val_loss: 3.8013e-05 - val_mae: 0.0039\n",
      "Epoch 344/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7238e-05 - mae: 0.0037 - val_loss: 3.9751e-05 - val_mae: 0.0039\n",
      "Epoch 345/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8417e-05 - mae: 0.0037 - val_loss: 3.9372e-05 - val_mae: 0.0038\n",
      "Epoch 346/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1437e-05 - mae: 0.0038 - val_loss: 3.8148e-05 - val_mae: 0.0038\n",
      "Epoch 347/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7778e-05 - mae: 0.0037 - val_loss: 3.7376e-05 - val_mae: 0.0038\n",
      "Epoch 348/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9370e-05 - mae: 0.0037 - val_loss: 3.7794e-05 - val_mae: 0.0037\n",
      "Epoch 349/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1297e-05 - mae: 0.0038 - val_loss: 3.7562e-05 - val_mae: 0.0038\n",
      "Epoch 350/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9277e-05 - mae: 0.0037 - val_loss: 3.7991e-05 - val_mae: 0.0038\n",
      "Epoch 351/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9464e-05 - mae: 0.0038 - val_loss: 3.7802e-05 - val_mae: 0.0038\n",
      "Epoch 352/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8272e-05 - mae: 0.0037 - val_loss: 3.8286e-05 - val_mae: 0.0038\n",
      "Epoch 353/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9604e-05 - mae: 0.0037 - val_loss: 3.7513e-05 - val_mae: 0.0038\n",
      "Epoch 354/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8289e-05 - mae: 0.0037 - val_loss: 3.6642e-05 - val_mae: 0.0037\n",
      "Epoch 355/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0080e-05 - mae: 0.0037 - val_loss: 3.7042e-05 - val_mae: 0.0037\n",
      "Epoch 356/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6027e-05 - mae: 0.0036 - val_loss: 3.7558e-05 - val_mae: 0.0038\n",
      "Epoch 357/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0219e-05 - mae: 0.0038 - val_loss: 3.6865e-05 - val_mae: 0.0037\n",
      "Epoch 358/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0015e-05 - mae: 0.0038 - val_loss: 3.6786e-05 - val_mae: 0.0037\n",
      "Epoch 359/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8566e-05 - mae: 0.0037 - val_loss: 3.7107e-05 - val_mae: 0.0037\n",
      "Epoch 360/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6919e-05 - mae: 0.0036 - val_loss: 3.6836e-05 - val_mae: 0.0037\n",
      "Epoch 361/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0987e-05 - mae: 0.0037 - val_loss: 3.6384e-05 - val_mae: 0.0037\n",
      "Epoch 362/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0217e-05 - mae: 0.0037 - val_loss: 3.7416e-05 - val_mae: 0.0038\n",
      "Epoch 363/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8494e-05 - mae: 0.0037 - val_loss: 3.8449e-05 - val_mae: 0.0037\n",
      "Epoch 364/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6421e-05 - mae: 0.0036 - val_loss: 3.8614e-05 - val_mae: 0.0038\n",
      "Epoch 365/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9893e-05 - mae: 0.0037 - val_loss: 3.9864e-05 - val_mae: 0.0039\n",
      "Epoch 366/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2126e-05 - mae: 0.0039 - val_loss: 3.7654e-05 - val_mae: 0.0037\n",
      "Epoch 367/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.1169e-05 - mae: 0.0038 - val_loss: 3.9840e-05 - val_mae: 0.0038\n",
      "Epoch 368/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1856e-05 - mae: 0.0038 - val_loss: 4.1203e-05 - val_mae: 0.0038\n",
      "Epoch 369/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1817e-05 - mae: 0.0038 - val_loss: 4.0944e-05 - val_mae: 0.0038\n",
      "Epoch 370/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9023e-05 - mae: 0.0038 - val_loss: 4.0684e-05 - val_mae: 0.0038\n",
      "Epoch 371/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.1710e-05 - mae: 0.0039 - val_loss: 4.0306e-05 - val_mae: 0.0038\n",
      "Epoch 372/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8034e-05 - mae: 0.0037 - val_loss: 3.8953e-05 - val_mae: 0.0038\n",
      "Epoch 373/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.8739e-05 - mae: 0.0037 - val_loss: 3.7673e-05 - val_mae: 0.0038\n",
      "Epoch 374/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9646e-05 - mae: 0.0037 - val_loss: 3.6609e-05 - val_mae: 0.0038\n",
      "Epoch 375/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.9748e-05 - mae: 0.0037 - val_loss: 3.6727e-05 - val_mae: 0.0037\n",
      "Epoch 376/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8061e-05 - mae: 0.0036 - val_loss: 3.6789e-05 - val_mae: 0.0037\n",
      "Epoch 377/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.7607e-05 - mae: 0.0036 - val_loss: 3.6277e-05 - val_mae: 0.0037\n",
      "Epoch 378/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.1680e-05 - mae: 0.0038 - val_loss: 3.6615e-05 - val_mae: 0.0037\n",
      "Epoch 379/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.9359e-05 - mae: 0.0037 - val_loss: 3.8914e-05 - val_mae: 0.0038\n",
      "Epoch 380/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7795e-05 - mae: 0.0037 - val_loss: 3.6766e-05 - val_mae: 0.0037\n",
      "Epoch 381/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8851e-05 - mae: 0.0037 - val_loss: 3.5697e-05 - val_mae: 0.0037\n",
      "Epoch 382/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9006e-05 - mae: 0.0037 - val_loss: 3.6466e-05 - val_mae: 0.0038\n",
      "Epoch 383/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8748e-05 - mae: 0.0037 - val_loss: 3.5727e-05 - val_mae: 0.0037\n",
      "Epoch 384/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8325e-05 - mae: 0.0037 - val_loss: 3.6461e-05 - val_mae: 0.0037\n",
      "Epoch 385/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6890e-05 - mae: 0.0037 - val_loss: 3.6614e-05 - val_mae: 0.0037\n",
      "Epoch 386/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8497e-05 - mae: 0.0037 - val_loss: 3.7045e-05 - val_mae: 0.0037\n",
      "Epoch 387/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5485e-05 - mae: 0.0035 - val_loss: 3.6540e-05 - val_mae: 0.0037\n",
      "Epoch 388/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7572e-05 - mae: 0.0036 - val_loss: 3.5545e-05 - val_mae: 0.0036\n",
      "Epoch 389/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6323e-05 - mae: 0.0036 - val_loss: 3.4473e-05 - val_mae: 0.0036\n",
      "Epoch 390/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6268e-05 - mae: 0.0036 - val_loss: 3.4300e-05 - val_mae: 0.0036\n",
      "Epoch 391/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7948e-05 - mae: 0.0037 - val_loss: 3.5459e-05 - val_mae: 0.0037\n",
      "Epoch 392/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7231e-05 - mae: 0.0036 - val_loss: 3.6119e-05 - val_mae: 0.0037\n",
      "Epoch 393/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5593e-05 - mae: 0.0035 - val_loss: 3.6035e-05 - val_mae: 0.0036\n",
      "Epoch 394/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5607e-05 - mae: 0.0035 - val_loss: 3.6141e-05 - val_mae: 0.0037\n",
      "Epoch 395/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6483e-05 - mae: 0.0036 - val_loss: 3.5362e-05 - val_mae: 0.0036\n",
      "Epoch 396/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6469e-05 - mae: 0.0036 - val_loss: 3.7256e-05 - val_mae: 0.0036\n",
      "Epoch 397/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.9813e-05 - mae: 0.0037 - val_loss: 3.8954e-05 - val_mae: 0.0038\n",
      "Epoch 398/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.9392e-05 - mae: 0.0038 - val_loss: 3.6800e-05 - val_mae: 0.0037\n",
      "Epoch 399/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0166e-05 - mae: 0.0037 - val_loss: 3.6344e-05 - val_mae: 0.0036\n",
      "Epoch 400/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9789e-05 - mae: 0.0036 - val_loss: 3.6179e-05 - val_mae: 0.0037\n",
      "Epoch 401/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6726e-05 - mae: 0.0036 - val_loss: 3.7249e-05 - val_mae: 0.0037\n",
      "Epoch 402/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9102e-05 - mae: 0.0037 - val_loss: 3.8421e-05 - val_mae: 0.0037\n",
      "Epoch 403/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4393e-05 - mae: 0.0035 - val_loss: 3.7645e-05 - val_mae: 0.0037\n",
      "Epoch 404/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9765e-05 - mae: 0.0037 - val_loss: 3.5687e-05 - val_mae: 0.0036\n",
      "Epoch 405/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1274e-05 - mae: 0.0038 - val_loss: 3.5582e-05 - val_mae: 0.0036\n",
      "Epoch 406/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0654e-05 - mae: 0.0037 - val_loss: 3.6787e-05 - val_mae: 0.0037\n",
      "Epoch 407/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7933e-05 - mae: 0.0037 - val_loss: 4.0161e-05 - val_mae: 0.0038\n",
      "Epoch 408/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7104e-05 - mae: 0.0037 - val_loss: 3.9422e-05 - val_mae: 0.0038\n",
      "Epoch 409/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.9394e-05 - mae: 0.0037 - val_loss: 3.6233e-05 - val_mae: 0.0037\n",
      "Epoch 410/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.9942e-05 - mae: 0.0037 - val_loss: 3.5432e-05 - val_mae: 0.0037\n",
      "Epoch 411/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5827e-05 - mae: 0.0036 - val_loss: 3.5595e-05 - val_mae: 0.0037\n",
      "Epoch 412/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6985e-05 - mae: 0.0036 - val_loss: 3.7920e-05 - val_mae: 0.0038\n",
      "Epoch 413/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6495e-05 - mae: 0.0036 - val_loss: 3.6257e-05 - val_mae: 0.0036\n",
      "Epoch 414/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0546e-05 - mae: 0.0037 - val_loss: 3.5558e-05 - val_mae: 0.0036\n",
      "Epoch 415/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8260e-05 - mae: 0.0036 - val_loss: 3.5298e-05 - val_mae: 0.0036\n",
      "Epoch 416/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5342e-05 - mae: 0.0035 - val_loss: 3.4988e-05 - val_mae: 0.0036\n",
      "Epoch 417/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5722e-05 - mae: 0.0035 - val_loss: 3.6675e-05 - val_mae: 0.0037\n",
      "Epoch 418/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6609e-05 - mae: 0.0036 - val_loss: 3.5411e-05 - val_mae: 0.0036\n",
      "Epoch 419/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7306e-05 - mae: 0.0036 - val_loss: 3.5840e-05 - val_mae: 0.0036\n",
      "Epoch 420/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7506e-05 - mae: 0.0035 - val_loss: 3.6340e-05 - val_mae: 0.0037\n",
      "Epoch 421/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7678e-05 - mae: 0.0036 - val_loss: 3.8400e-05 - val_mae: 0.0038\n",
      "Epoch 422/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8447e-05 - mae: 0.0037 - val_loss: 3.6202e-05 - val_mae: 0.0037\n",
      "Epoch 423/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8955e-05 - mae: 0.0037 - val_loss: 3.6226e-05 - val_mae: 0.0036\n",
      "Epoch 424/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2141e-05 - mae: 0.0037 - val_loss: 3.4686e-05 - val_mae: 0.0036\n",
      "Epoch 425/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.9047e-05 - mae: 0.0037 - val_loss: 3.7341e-05 - val_mae: 0.0037\n",
      "Epoch 426/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.0086e-05 - mae: 0.0037 - val_loss: 3.6233e-05 - val_mae: 0.0036\n",
      "Epoch 427/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7249e-05 - mae: 0.0035 - val_loss: 3.6063e-05 - val_mae: 0.0036\n",
      "Epoch 428/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5681e-05 - mae: 0.0035 - val_loss: 3.6100e-05 - val_mae: 0.0036\n",
      "Epoch 429/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6479e-05 - mae: 0.0036 - val_loss: 3.6443e-05 - val_mae: 0.0037\n",
      "Epoch 430/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6401e-05 - mae: 0.0036 - val_loss: 3.7826e-05 - val_mae: 0.0038\n",
      "Epoch 431/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5162e-05 - mae: 0.0036 - val_loss: 3.4734e-05 - val_mae: 0.0036\n",
      "Epoch 432/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8518e-05 - mae: 0.0036 - val_loss: 3.4002e-05 - val_mae: 0.0035\n",
      "Epoch 433/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7093e-05 - mae: 0.0035 - val_loss: 3.5035e-05 - val_mae: 0.0037\n",
      "Epoch 434/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4650e-05 - mae: 0.0035 - val_loss: 3.4732e-05 - val_mae: 0.0037\n",
      "Epoch 435/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6417e-05 - mae: 0.0036 - val_loss: 3.5481e-05 - val_mae: 0.0036\n",
      "Epoch 436/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3821e-05 - mae: 0.0035 - val_loss: 3.5995e-05 - val_mae: 0.0036\n",
      "Epoch 437/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6446e-05 - mae: 0.0036 - val_loss: 3.4963e-05 - val_mae: 0.0035\n",
      "Epoch 438/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4711e-05 - mae: 0.0035 - val_loss: 3.4339e-05 - val_mae: 0.0035\n",
      "Epoch 439/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4653e-05 - mae: 0.0035 - val_loss: 3.4041e-05 - val_mae: 0.0035\n",
      "Epoch 440/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5865e-05 - mae: 0.0035 - val_loss: 3.3528e-05 - val_mae: 0.0035\n",
      "Epoch 441/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7628e-05 - mae: 0.0036 - val_loss: 3.4122e-05 - val_mae: 0.0035\n",
      "Epoch 442/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4450e-05 - mae: 0.0034 - val_loss: 3.5156e-05 - val_mae: 0.0036\n",
      "Epoch 443/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6129e-05 - mae: 0.0035 - val_loss: 3.3749e-05 - val_mae: 0.0035\n",
      "Epoch 444/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6033e-05 - mae: 0.0035 - val_loss: 3.3256e-05 - val_mae: 0.0035\n",
      "Epoch 445/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4690e-05 - mae: 0.0035 - val_loss: 3.4021e-05 - val_mae: 0.0036\n",
      "Epoch 446/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3971e-05 - mae: 0.0035 - val_loss: 3.3823e-05 - val_mae: 0.0035\n",
      "Epoch 447/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5089e-05 - mae: 0.0035 - val_loss: 3.2788e-05 - val_mae: 0.0035\n",
      "Epoch 448/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7743e-05 - mae: 0.0036 - val_loss: 3.3966e-05 - val_mae: 0.0036\n",
      "Epoch 449/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6944e-05 - mae: 0.0036 - val_loss: 3.4955e-05 - val_mae: 0.0036\n",
      "Epoch 450/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5270e-05 - mae: 0.0035 - val_loss: 3.3728e-05 - val_mae: 0.0035\n",
      "Epoch 451/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4078e-05 - mae: 0.0034 - val_loss: 3.2918e-05 - val_mae: 0.0034\n",
      "Epoch 452/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4922e-05 - mae: 0.0034 - val_loss: 3.3024e-05 - val_mae: 0.0035\n",
      "Epoch 453/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6489e-05 - mae: 0.0035 - val_loss: 3.1896e-05 - val_mae: 0.0034\n",
      "Epoch 454/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4957e-05 - mae: 0.0034 - val_loss: 3.2651e-05 - val_mae: 0.0035\n",
      "Epoch 455/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3435e-05 - mae: 0.0034 - val_loss: 3.2798e-05 - val_mae: 0.0035\n",
      "Epoch 456/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.9608e-05 - mae: 0.0036 - val_loss: 3.4119e-05 - val_mae: 0.0036\n",
      "Epoch 457/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4294e-05 - mae: 0.0035 - val_loss: 3.3472e-05 - val_mae: 0.0036\n",
      "Epoch 458/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5164e-05 - mae: 0.0035 - val_loss: 3.4026e-05 - val_mae: 0.0036\n",
      "Epoch 459/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4552e-05 - mae: 0.0036 - val_loss: 3.3335e-05 - val_mae: 0.0035\n",
      "Epoch 460/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6242e-05 - mae: 0.0036 - val_loss: 3.3808e-05 - val_mae: 0.0035\n",
      "Epoch 461/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5068e-05 - mae: 0.0035 - val_loss: 3.3134e-05 - val_mae: 0.0034\n",
      "Epoch 462/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5121e-05 - mae: 0.0034 - val_loss: 3.2556e-05 - val_mae: 0.0034\n",
      "Epoch 463/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6006e-05 - mae: 0.0035 - val_loss: 3.2710e-05 - val_mae: 0.0035\n",
      "Epoch 464/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5950e-05 - mae: 0.0034 - val_loss: 3.4206e-05 - val_mae: 0.0036\n",
      "Epoch 465/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5019e-05 - mae: 0.0035 - val_loss: 3.2959e-05 - val_mae: 0.0035\n",
      "Epoch 466/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3020e-05 - mae: 0.0034 - val_loss: 3.2737e-05 - val_mae: 0.0035\n",
      "Epoch 467/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2761e-05 - mae: 0.0034 - val_loss: 3.2960e-05 - val_mae: 0.0036\n",
      "Epoch 468/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5842e-05 - mae: 0.0035 - val_loss: 3.2277e-05 - val_mae: 0.0035\n",
      "Epoch 469/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3379e-05 - mae: 0.0035 - val_loss: 3.1861e-05 - val_mae: 0.0035\n",
      "Epoch 470/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5823e-05 - mae: 0.0035 - val_loss: 3.1630e-05 - val_mae: 0.0034\n",
      "Epoch 471/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4169e-05 - mae: 0.0035 - val_loss: 3.1414e-05 - val_mae: 0.0034\n",
      "Epoch 472/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5128e-05 - mae: 0.0034 - val_loss: 3.1556e-05 - val_mae: 0.0034\n",
      "Epoch 473/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8717e-05 - mae: 0.0036 - val_loss: 3.6503e-05 - val_mae: 0.0036\n",
      "Epoch 474/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6982e-05 - mae: 0.0035 - val_loss: 3.5981e-05 - val_mae: 0.0037\n",
      "Epoch 475/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8419e-05 - mae: 0.0037 - val_loss: 3.5419e-05 - val_mae: 0.0036\n",
      "Epoch 476/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7630e-05 - mae: 0.0036 - val_loss: 3.5009e-05 - val_mae: 0.0036\n",
      "Epoch 477/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5227e-05 - mae: 0.0035 - val_loss: 3.5820e-05 - val_mae: 0.0037\n",
      "Epoch 478/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6015e-05 - mae: 0.0036 - val_loss: 3.6171e-05 - val_mae: 0.0037\n",
      "Epoch 479/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.8350e-05 - mae: 0.0037 - val_loss: 3.5295e-05 - val_mae: 0.0036\n",
      "Epoch 480/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7372e-05 - mae: 0.0036 - val_loss: 3.4834e-05 - val_mae: 0.0036\n",
      "Epoch 481/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6606e-05 - mae: 0.0036 - val_loss: 3.5493e-05 - val_mae: 0.0036\n",
      "Epoch 482/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4210e-05 - mae: 0.0035 - val_loss: 3.4250e-05 - val_mae: 0.0035\n",
      "Epoch 483/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4932e-05 - mae: 0.0035 - val_loss: 3.4033e-05 - val_mae: 0.0035\n",
      "Epoch 484/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7400e-05 - mae: 0.0035 - val_loss: 3.3940e-05 - val_mae: 0.0036\n",
      "Epoch 485/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4891e-05 - mae: 0.0035 - val_loss: 3.3870e-05 - val_mae: 0.0036\n",
      "Epoch 486/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4721e-05 - mae: 0.0035 - val_loss: 3.4183e-05 - val_mae: 0.0035\n",
      "Epoch 487/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5411e-05 - mae: 0.0035 - val_loss: 3.4754e-05 - val_mae: 0.0035\n",
      "Epoch 488/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4812e-05 - mae: 0.0034 - val_loss: 3.4769e-05 - val_mae: 0.0035\n",
      "Epoch 489/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3758e-05 - mae: 0.0034 - val_loss: 3.3982e-05 - val_mae: 0.0035\n",
      "Epoch 490/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4339e-05 - mae: 0.0034 - val_loss: 3.4630e-05 - val_mae: 0.0035\n",
      "Epoch 491/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3701e-05 - mae: 0.0034 - val_loss: 3.5052e-05 - val_mae: 0.0036\n",
      "Epoch 492/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3167e-05 - mae: 0.0034 - val_loss: 3.3659e-05 - val_mae: 0.0035\n",
      "Epoch 493/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6203e-05 - mae: 0.0035 - val_loss: 3.4547e-05 - val_mae: 0.0036\n",
      "Epoch 494/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5605e-05 - mae: 0.0035 - val_loss: 3.8250e-05 - val_mae: 0.0038\n",
      "Epoch 495/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5108e-05 - mae: 0.0036 - val_loss: 3.3173e-05 - val_mae: 0.0035\n",
      "Epoch 496/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4930e-05 - mae: 0.0035 - val_loss: 3.4225e-05 - val_mae: 0.0035\n",
      "Epoch 497/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7485e-05 - mae: 0.0035 - val_loss: 3.5948e-05 - val_mae: 0.0037\n",
      "Epoch 498/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7285e-05 - mae: 0.0037 - val_loss: 3.4697e-05 - val_mae: 0.0036\n",
      "Epoch 499/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4337e-05 - mae: 0.0035 - val_loss: 3.3522e-05 - val_mae: 0.0035\n",
      "Epoch 500/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3275e-05 - mae: 0.0035 - val_loss: 3.3044e-05 - val_mae: 0.0035\n",
      "Epoch 501/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4504e-05 - mae: 0.0035 - val_loss: 3.1757e-05 - val_mae: 0.0035\n",
      "Epoch 502/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4756e-05 - mae: 0.0035 - val_loss: 3.4284e-05 - val_mae: 0.0036\n",
      "Epoch 503/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4102e-05 - mae: 0.0035 - val_loss: 3.3168e-05 - val_mae: 0.0035\n",
      "Epoch 504/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4291e-05 - mae: 0.0034 - val_loss: 3.3128e-05 - val_mae: 0.0035\n",
      "Epoch 505/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5132e-05 - mae: 0.0035 - val_loss: 3.3232e-05 - val_mae: 0.0035\n",
      "Epoch 506/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5092e-05 - mae: 0.0035 - val_loss: 3.2282e-05 - val_mae: 0.0034\n",
      "Epoch 507/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2572e-05 - mae: 0.0034 - val_loss: 3.2940e-05 - val_mae: 0.0035\n",
      "Epoch 508/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6312e-05 - mae: 0.0035 - val_loss: 3.4683e-05 - val_mae: 0.0036\n",
      "Epoch 509/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5856e-05 - mae: 0.0035 - val_loss: 3.4902e-05 - val_mae: 0.0036\n",
      "Epoch 510/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3604e-05 - mae: 0.0034 - val_loss: 3.3337e-05 - val_mae: 0.0035\n",
      "Epoch 511/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7797e-05 - mae: 0.0036 - val_loss: 3.2881e-05 - val_mae: 0.0035\n",
      "Epoch 512/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4780e-05 - mae: 0.0035 - val_loss: 3.3508e-05 - val_mae: 0.0036\n",
      "Epoch 513/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4559e-05 - mae: 0.0035 - val_loss: 3.2277e-05 - val_mae: 0.0034\n",
      "Epoch 514/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5486e-05 - mae: 0.0035 - val_loss: 3.3453e-05 - val_mae: 0.0035\n",
      "Epoch 515/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4316e-05 - mae: 0.0035 - val_loss: 3.3009e-05 - val_mae: 0.0035\n",
      "Epoch 516/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5754e-05 - mae: 0.0035 - val_loss: 3.4283e-05 - val_mae: 0.0035\n",
      "Epoch 517/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6719e-05 - mae: 0.0035 - val_loss: 3.7863e-05 - val_mae: 0.0038\n",
      "Epoch 518/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5813e-05 - mae: 0.0036 - val_loss: 3.3130e-05 - val_mae: 0.0035\n",
      "Epoch 519/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6414e-05 - mae: 0.0035 - val_loss: 3.3228e-05 - val_mae: 0.0035\n",
      "Epoch 520/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4648e-05 - mae: 0.0035 - val_loss: 3.4376e-05 - val_mae: 0.0036\n",
      "Epoch 521/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4068e-05 - mae: 0.0035 - val_loss: 3.2830e-05 - val_mae: 0.0035\n",
      "Epoch 522/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5408e-05 - mae: 0.0035 - val_loss: 3.2634e-05 - val_mae: 0.0035\n",
      "Epoch 523/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6281e-05 - mae: 0.0035 - val_loss: 3.2501e-05 - val_mae: 0.0035\n",
      "Epoch 524/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2837e-05 - mae: 0.0033 - val_loss: 3.1933e-05 - val_mae: 0.0035\n",
      "Epoch 525/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7107e-05 - mae: 0.0035 - val_loss: 3.3414e-05 - val_mae: 0.0035\n",
      "Epoch 526/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3229e-05 - mae: 0.0034 - val_loss: 3.5021e-05 - val_mae: 0.0036\n",
      "Epoch 527/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3598e-05 - mae: 0.0034 - val_loss: 3.2644e-05 - val_mae: 0.0034\n",
      "Epoch 528/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3112e-05 - mae: 0.0034 - val_loss: 3.2474e-05 - val_mae: 0.0035\n",
      "Epoch 529/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4759e-05 - mae: 0.0034 - val_loss: 3.2643e-05 - val_mae: 0.0035\n",
      "Epoch 530/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1187e-05 - mae: 0.0033 - val_loss: 3.2320e-05 - val_mae: 0.0034\n",
      "Epoch 531/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1913e-05 - mae: 0.0033 - val_loss: 3.3015e-05 - val_mae: 0.0035\n",
      "Epoch 532/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2755e-05 - mae: 0.0034 - val_loss: 3.3573e-05 - val_mae: 0.0036\n",
      "Epoch 533/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6214e-05 - mae: 0.0036 - val_loss: 3.3197e-05 - val_mae: 0.0035\n",
      "Epoch 534/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1399e-05 - mae: 0.0034 - val_loss: 3.3179e-05 - val_mae: 0.0035\n",
      "Epoch 535/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4327e-05 - mae: 0.0034 - val_loss: 3.3224e-05 - val_mae: 0.0035\n",
      "Epoch 536/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4432e-05 - mae: 0.0034 - val_loss: 3.2995e-05 - val_mae: 0.0034\n",
      "Epoch 537/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2549e-05 - mae: 0.0033 - val_loss: 3.3271e-05 - val_mae: 0.0034\n",
      "Epoch 538/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5229e-05 - mae: 0.0034 - val_loss: 3.4485e-05 - val_mae: 0.0035\n",
      "Epoch 539/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2463e-05 - mae: 0.0034 - val_loss: 3.3309e-05 - val_mae: 0.0035\n",
      "Epoch 540/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1789e-05 - mae: 0.0033 - val_loss: 3.3431e-05 - val_mae: 0.0036\n",
      "Epoch 541/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6509e-05 - mae: 0.0036 - val_loss: 3.2441e-05 - val_mae: 0.0036\n",
      "Epoch 542/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4072e-05 - mae: 0.0035 - val_loss: 3.3270e-05 - val_mae: 0.0036\n",
      "Epoch 543/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5473e-05 - mae: 0.0035 - val_loss: 3.2433e-05 - val_mae: 0.0035\n",
      "Epoch 544/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4079e-05 - mae: 0.0034 - val_loss: 3.2748e-05 - val_mae: 0.0034\n",
      "Epoch 545/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2824e-05 - mae: 0.0034 - val_loss: 3.4709e-05 - val_mae: 0.0035\n",
      "Epoch 546/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4304e-05 - mae: 0.0035 - val_loss: 3.3525e-05 - val_mae: 0.0035\n",
      "Epoch 547/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2432e-05 - mae: 0.0034 - val_loss: 3.2573e-05 - val_mae: 0.0035\n",
      "Epoch 548/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2628e-05 - mae: 0.0034 - val_loss: 3.0829e-05 - val_mae: 0.0034\n",
      "Epoch 549/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6769e-05 - mae: 0.0035 - val_loss: 3.2698e-05 - val_mae: 0.0035\n",
      "Epoch 550/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5409e-05 - mae: 0.0035 - val_loss: 3.4175e-05 - val_mae: 0.0036\n",
      "Epoch 551/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2772e-05 - mae: 0.0034 - val_loss: 3.3804e-05 - val_mae: 0.0035\n",
      "Epoch 552/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6825e-05 - mae: 0.0035 - val_loss: 3.3711e-05 - val_mae: 0.0035\n",
      "Epoch 553/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2726e-05 - mae: 0.0034 - val_loss: 3.3600e-05 - val_mae: 0.0035\n",
      "Epoch 554/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1949e-05 - mae: 0.0034 - val_loss: 3.3316e-05 - val_mae: 0.0035\n",
      "Epoch 555/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3112e-05 - mae: 0.0034 - val_loss: 3.3777e-05 - val_mae: 0.0035\n",
      "Epoch 556/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5803e-05 - mae: 0.0035 - val_loss: 3.3086e-05 - val_mae: 0.0035\n",
      "Epoch 557/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4146e-05 - mae: 0.0034 - val_loss: 3.3058e-05 - val_mae: 0.0035\n",
      "Epoch 558/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6747e-05 - mae: 0.0035 - val_loss: 3.4506e-05 - val_mae: 0.0036\n",
      "Epoch 559/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1124e-05 - mae: 0.0034 - val_loss: 3.3122e-05 - val_mae: 0.0035\n",
      "Epoch 560/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0910e-05 - mae: 0.0033 - val_loss: 3.2714e-05 - val_mae: 0.0034\n",
      "Epoch 561/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6104e-05 - mae: 0.0035 - val_loss: 3.3638e-05 - val_mae: 0.0035\n",
      "Epoch 562/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2798e-05 - mae: 0.0034 - val_loss: 3.4080e-05 - val_mae: 0.0035\n",
      "Epoch 563/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1671e-05 - mae: 0.0034 - val_loss: 3.4221e-05 - val_mae: 0.0035\n",
      "Epoch 564/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3423e-05 - mae: 0.0034 - val_loss: 3.3714e-05 - val_mae: 0.0035\n",
      "Epoch 565/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2922e-05 - mae: 0.0033 - val_loss: 3.1846e-05 - val_mae: 0.0034\n",
      "Epoch 566/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2476e-05 - mae: 0.0033 - val_loss: 3.2431e-05 - val_mae: 0.0035\n",
      "Epoch 567/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6152e-05 - mae: 0.0036 - val_loss: 3.5126e-05 - val_mae: 0.0037\n",
      "Epoch 568/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4355e-05 - mae: 0.0035 - val_loss: 3.3514e-05 - val_mae: 0.0035\n",
      "Epoch 569/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3769e-05 - mae: 0.0034 - val_loss: 3.2943e-05 - val_mae: 0.0034\n",
      "Epoch 570/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2000e-05 - mae: 0.0033 - val_loss: 3.2944e-05 - val_mae: 0.0035\n",
      "Epoch 571/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3239e-05 - mae: 0.0034 - val_loss: 3.3982e-05 - val_mae: 0.0036\n",
      "Epoch 572/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3427e-05 - mae: 0.0035 - val_loss: 3.5128e-05 - val_mae: 0.0036\n",
      "Epoch 573/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7138e-05 - mae: 0.0036 - val_loss: 3.5044e-05 - val_mae: 0.0036\n",
      "Epoch 574/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3060e-05 - mae: 0.0034 - val_loss: 3.3013e-05 - val_mae: 0.0035\n",
      "Epoch 575/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0843e-05 - mae: 0.0033 - val_loss: 3.3221e-05 - val_mae: 0.0035\n",
      "Epoch 576/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3423e-05 - mae: 0.0034 - val_loss: 3.2073e-05 - val_mae: 0.0034\n",
      "Epoch 577/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3415e-05 - mae: 0.0034 - val_loss: 3.2140e-05 - val_mae: 0.0035\n",
      "Epoch 578/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3529e-05 - mae: 0.0034 - val_loss: 3.4605e-05 - val_mae: 0.0036\n",
      "Epoch 579/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4282e-05 - mae: 0.0034 - val_loss: 3.1394e-05 - val_mae: 0.0034\n",
      "Epoch 580/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5911e-05 - mae: 0.0035 - val_loss: 3.2692e-05 - val_mae: 0.0035\n",
      "Epoch 581/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3631e-05 - mae: 0.0035 - val_loss: 3.3827e-05 - val_mae: 0.0035\n",
      "Epoch 582/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3240e-05 - mae: 0.0034 - val_loss: 3.3779e-05 - val_mae: 0.0035\n",
      "Epoch 583/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4542e-05 - mae: 0.0035 - val_loss: 3.6159e-05 - val_mae: 0.0036\n",
      "Epoch 584/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6941e-05 - mae: 0.0036 - val_loss: 3.2730e-05 - val_mae: 0.0035\n",
      "Epoch 585/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6499e-05 - mae: 0.0035 - val_loss: 3.2572e-05 - val_mae: 0.0035\n",
      "Epoch 586/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4619e-05 - mae: 0.0034 - val_loss: 3.4348e-05 - val_mae: 0.0036\n",
      "Epoch 587/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1542e-05 - mae: 0.0034 - val_loss: 3.2849e-05 - val_mae: 0.0035\n",
      "Epoch 588/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4112e-05 - mae: 0.0034 - val_loss: 3.3774e-05 - val_mae: 0.0035\n",
      "Epoch 589/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4503e-05 - mae: 0.0035 - val_loss: 3.3309e-05 - val_mae: 0.0034\n",
      "Epoch 590/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2066e-05 - mae: 0.0034 - val_loss: 3.2091e-05 - val_mae: 0.0034\n",
      "Epoch 591/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2507e-05 - mae: 0.0034 - val_loss: 3.3882e-05 - val_mae: 0.0035\n",
      "Epoch 592/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4000e-05 - mae: 0.0035 - val_loss: 3.1570e-05 - val_mae: 0.0034\n",
      "Epoch 593/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2153e-05 - mae: 0.0034 - val_loss: 3.2530e-05 - val_mae: 0.0035\n",
      "Epoch 594/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2720e-05 - mae: 0.0034 - val_loss: 3.2903e-05 - val_mae: 0.0035\n",
      "Epoch 595/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4825e-05 - mae: 0.0034 - val_loss: 3.1659e-05 - val_mae: 0.0034\n",
      "Epoch 596/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5326e-05 - mae: 0.0035 - val_loss: 3.2132e-05 - val_mae: 0.0035\n",
      "Epoch 597/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5277e-05 - mae: 0.0035 - val_loss: 3.2840e-05 - val_mae: 0.0035\n",
      "Epoch 598/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.8236e-05 - mae: 0.0036 - val_loss: 3.4008e-05 - val_mae: 0.0035\n",
      "Epoch 599/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3959e-05 - mae: 0.0035 - val_loss: 3.4065e-05 - val_mae: 0.0035\n",
      "Epoch 600/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3256e-05 - mae: 0.0034 - val_loss: 3.2659e-05 - val_mae: 0.0034\n",
      "Epoch 601/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6116e-05 - mae: 0.0035 - val_loss: 3.3904e-05 - val_mae: 0.0035\n",
      "Epoch 602/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4363e-05 - mae: 0.0035 - val_loss: 3.3015e-05 - val_mae: 0.0035\n",
      "Epoch 603/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3409e-05 - mae: 0.0034 - val_loss: 3.2335e-05 - val_mae: 0.0035\n",
      "Epoch 604/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3577e-05 - mae: 0.0034 - val_loss: 3.3263e-05 - val_mae: 0.0035\n",
      "Epoch 605/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4689e-05 - mae: 0.0034 - val_loss: 3.3176e-05 - val_mae: 0.0035\n",
      "Epoch 606/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4135e-05 - mae: 0.0034 - val_loss: 3.2724e-05 - val_mae: 0.0034\n",
      "Epoch 607/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.9186e-05 - mae: 0.0036 - val_loss: 3.1180e-05 - val_mae: 0.0034\n",
      "Epoch 608/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1893e-05 - mae: 0.0034 - val_loss: 3.1661e-05 - val_mae: 0.0035\n",
      "Epoch 609/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4444e-05 - mae: 0.0035 - val_loss: 3.2930e-05 - val_mae: 0.0035\n",
      "Epoch 610/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2284e-05 - mae: 0.0035 - val_loss: 3.2388e-05 - val_mae: 0.0034\n",
      "Epoch 611/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2385e-05 - mae: 0.0034 - val_loss: 3.4892e-05 - val_mae: 0.0036\n",
      "Epoch 612/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2183e-05 - mae: 0.0034 - val_loss: 3.2128e-05 - val_mae: 0.0034\n",
      "Epoch 613/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6009e-05 - mae: 0.0034 - val_loss: 3.2683e-05 - val_mae: 0.0034\n",
      "Epoch 614/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3445e-05 - mae: 0.0034 - val_loss: 3.2381e-05 - val_mae: 0.0034\n",
      "Epoch 615/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6446e-05 - mae: 0.0035 - val_loss: 3.2605e-05 - val_mae: 0.0034\n",
      "Epoch 616/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6487e-05 - mae: 0.0035 - val_loss: 3.2827e-05 - val_mae: 0.0034\n",
      "Epoch 617/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7250e-05 - mae: 0.0036 - val_loss: 3.1934e-05 - val_mae: 0.0034\n",
      "Epoch 618/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3878e-05 - mae: 0.0034 - val_loss: 3.1983e-05 - val_mae: 0.0034\n",
      "Epoch 619/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3657e-05 - mae: 0.0034 - val_loss: 3.3289e-05 - val_mae: 0.0035\n",
      "Epoch 620/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3439e-05 - mae: 0.0034 - val_loss: 3.2523e-05 - val_mae: 0.0035\n",
      "Epoch 621/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4866e-05 - mae: 0.0035 - val_loss: 3.3207e-05 - val_mae: 0.0035\n",
      "Epoch 622/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2293e-05 - mae: 0.0034 - val_loss: 3.6292e-05 - val_mae: 0.0037\n",
      "Epoch 623/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6965e-05 - mae: 0.0036 - val_loss: 3.4296e-05 - val_mae: 0.0036\n",
      "Epoch 624/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1941e-05 - mae: 0.0034 - val_loss: 3.3288e-05 - val_mae: 0.0035\n",
      "Epoch 625/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3878e-05 - mae: 0.0034 - val_loss: 3.3161e-05 - val_mae: 0.0035\n",
      "Epoch 626/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.3156e-05 - mae: 0.0034 - val_loss: 3.2955e-05 - val_mae: 0.0035\n",
      "Epoch 627/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2876e-05 - mae: 0.0034 - val_loss: 3.2391e-05 - val_mae: 0.0034\n",
      "Epoch 628/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4530e-05 - mae: 0.0034 - val_loss: 3.3383e-05 - val_mae: 0.0035\n",
      "Epoch 629/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3539e-05 - mae: 0.0035 - val_loss: 3.4481e-05 - val_mae: 0.0036\n",
      "Epoch 630/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5689e-05 - mae: 0.0035 - val_loss: 3.2973e-05 - val_mae: 0.0034\n",
      "Epoch 631/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2774e-05 - mae: 0.0034 - val_loss: 3.2911e-05 - val_mae: 0.0035\n",
      "Epoch 632/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3459e-05 - mae: 0.0034 - val_loss: 3.3008e-05 - val_mae: 0.0035\n",
      "Epoch 633/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3134e-05 - mae: 0.0034 - val_loss: 3.2837e-05 - val_mae: 0.0035\n",
      "Epoch 634/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4615e-05 - mae: 0.0035 - val_loss: 3.1873e-05 - val_mae: 0.0034\n",
      "Epoch 635/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3052e-05 - mae: 0.0034 - val_loss: 3.1413e-05 - val_mae: 0.0034\n",
      "Epoch 636/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0690e-05 - mae: 0.0033 - val_loss: 3.2392e-05 - val_mae: 0.0035\n",
      "Epoch 637/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3829e-05 - mae: 0.0034 - val_loss: 3.2322e-05 - val_mae: 0.0034\n",
      "Epoch 638/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5141e-05 - mae: 0.0035 - val_loss: 3.5524e-05 - val_mae: 0.0036\n",
      "Epoch 639/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3357e-05 - mae: 0.0034 - val_loss: 3.3190e-05 - val_mae: 0.0035\n",
      "Epoch 640/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5359e-05 - mae: 0.0035 - val_loss: 3.4159e-05 - val_mae: 0.0035\n",
      "Epoch 641/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2459e-05 - mae: 0.0034 - val_loss: 3.4396e-05 - val_mae: 0.0036\n",
      "Epoch 642/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4834e-05 - mae: 0.0035 - val_loss: 3.4491e-05 - val_mae: 0.0036\n",
      "Epoch 643/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1677e-05 - mae: 0.0034 - val_loss: 3.2940e-05 - val_mae: 0.0035\n",
      "Epoch 644/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1321e-05 - mae: 0.0033 - val_loss: 3.2691e-05 - val_mae: 0.0035\n",
      "Epoch 645/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.9761e-05 - mae: 0.0033 - val_loss: 3.3304e-05 - val_mae: 0.0036\n",
      "Epoch 646/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5944e-05 - mae: 0.0035 - val_loss: 3.2176e-05 - val_mae: 0.0035\n",
      "Epoch 647/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5146e-05 - mae: 0.0035 - val_loss: 3.2464e-05 - val_mae: 0.0034\n",
      "Epoch 648/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2415e-05 - mae: 0.0033 - val_loss: 3.4643e-05 - val_mae: 0.0036\n",
      "Epoch 649/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5244e-05 - mae: 0.0035 - val_loss: 3.2893e-05 - val_mae: 0.0034\n",
      "Epoch 650/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3936e-05 - mae: 0.0034 - val_loss: 3.3722e-05 - val_mae: 0.0035\n",
      "Epoch 651/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4335e-05 - mae: 0.0034 - val_loss: 3.3534e-05 - val_mae: 0.0035\n",
      "Epoch 652/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6198e-05 - mae: 0.0036 - val_loss: 3.4545e-05 - val_mae: 0.0036\n",
      "Epoch 653/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1696e-05 - mae: 0.0034 - val_loss: 3.2416e-05 - val_mae: 0.0034\n",
      "Epoch 654/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4047e-05 - mae: 0.0034 - val_loss: 3.2800e-05 - val_mae: 0.0034\n",
      "Epoch 655/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2852e-05 - mae: 0.0034 - val_loss: 3.4372e-05 - val_mae: 0.0035\n",
      "Epoch 656/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3131e-05 - mae: 0.0035 - val_loss: 3.3008e-05 - val_mae: 0.0034\n",
      "Epoch 657/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4041e-05 - mae: 0.0034 - val_loss: 3.2278e-05 - val_mae: 0.0034\n",
      "Epoch 658/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1902e-05 - mae: 0.0033 - val_loss: 3.1897e-05 - val_mae: 0.0035\n",
      "Epoch 659/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2038e-05 - mae: 0.0033 - val_loss: 3.2533e-05 - val_mae: 0.0035\n",
      "Epoch 660/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4196e-05 - mae: 0.0034 - val_loss: 3.2223e-05 - val_mae: 0.0035\n",
      "Epoch 661/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3182e-05 - mae: 0.0034 - val_loss: 3.0403e-05 - val_mae: 0.0033\n",
      "Epoch 662/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3651e-05 - mae: 0.0033 - val_loss: 3.2217e-05 - val_mae: 0.0034\n",
      "Epoch 663/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5529e-05 - mae: 0.0034 - val_loss: 3.2329e-05 - val_mae: 0.0034\n",
      "Epoch 664/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3265e-05 - mae: 0.0034 - val_loss: 3.2167e-05 - val_mae: 0.0034\n",
      "Epoch 665/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2264e-05 - mae: 0.0033 - val_loss: 3.2322e-05 - val_mae: 0.0034\n",
      "Epoch 666/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1572e-05 - mae: 0.0033 - val_loss: 3.3591e-05 - val_mae: 0.0035\n",
      "Epoch 667/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1750e-05 - mae: 0.0033 - val_loss: 3.4255e-05 - val_mae: 0.0035\n",
      "Epoch 668/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3370e-05 - mae: 0.0035 - val_loss: 3.4568e-05 - val_mae: 0.0035\n",
      "Epoch 669/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7225e-05 - mae: 0.0036 - val_loss: 3.2339e-05 - val_mae: 0.0034\n",
      "Epoch 670/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4197e-05 - mae: 0.0034 - val_loss: 3.1563e-05 - val_mae: 0.0034\n",
      "Epoch 671/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5562e-05 - mae: 0.0035 - val_loss: 3.3156e-05 - val_mae: 0.0035\n",
      "Epoch 672/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2978e-05 - mae: 0.0034 - val_loss: 3.1531e-05 - val_mae: 0.0034\n",
      "Epoch 673/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1613e-05 - mae: 0.0033 - val_loss: 3.2043e-05 - val_mae: 0.0034\n",
      "Epoch 674/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.9943e-05 - mae: 0.0033 - val_loss: 3.2543e-05 - val_mae: 0.0035\n",
      "Epoch 675/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1465e-05 - mae: 0.0033 - val_loss: 3.1559e-05 - val_mae: 0.0034\n",
      "Epoch 676/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2998e-05 - mae: 0.0033 - val_loss: 3.1720e-05 - val_mae: 0.0035\n",
      "Epoch 677/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0334e-05 - mae: 0.0033 - val_loss: 3.2337e-05 - val_mae: 0.0035\n",
      "Epoch 678/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5320e-05 - mae: 0.0035 - val_loss: 3.2978e-05 - val_mae: 0.0035\n",
      "Epoch 679/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3546e-05 - mae: 0.0035 - val_loss: 3.2870e-05 - val_mae: 0.0035\n",
      "Epoch 680/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3034e-05 - mae: 0.0034 - val_loss: 3.1822e-05 - val_mae: 0.0033\n",
      "Epoch 681/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4864e-05 - mae: 0.0034 - val_loss: 3.1793e-05 - val_mae: 0.0033\n",
      "Epoch 682/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3050e-05 - mae: 0.0034 - val_loss: 3.2368e-05 - val_mae: 0.0034\n",
      "Epoch 683/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3056e-05 - mae: 0.0034 - val_loss: 3.2912e-05 - val_mae: 0.0035\n",
      "Epoch 684/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1850e-05 - mae: 0.0033 - val_loss: 3.2161e-05 - val_mae: 0.0034\n",
      "Epoch 685/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0738e-05 - mae: 0.0033 - val_loss: 3.2197e-05 - val_mae: 0.0034\n",
      "Epoch 686/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2936e-05 - mae: 0.0034 - val_loss: 3.2857e-05 - val_mae: 0.0035\n",
      "Epoch 687/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1730e-05 - mae: 0.0034 - val_loss: 3.4648e-05 - val_mae: 0.0035\n",
      "Epoch 688/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1711e-05 - mae: 0.0033 - val_loss: 3.1370e-05 - val_mae: 0.0034\n",
      "Epoch 689/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2377e-05 - mae: 0.0033 - val_loss: 3.2006e-05 - val_mae: 0.0035\n",
      "Epoch 690/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2766e-05 - mae: 0.0033 - val_loss: 3.2188e-05 - val_mae: 0.0034\n",
      "Epoch 691/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1997e-05 - mae: 0.0034 - val_loss: 3.2899e-05 - val_mae: 0.0035\n",
      "Epoch 692/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2212e-05 - mae: 0.0034 - val_loss: 3.2723e-05 - val_mae: 0.0034\n",
      "Epoch 693/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3418e-05 - mae: 0.0034 - val_loss: 3.2326e-05 - val_mae: 0.0034\n",
      "Epoch 694/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2910e-05 - mae: 0.0034 - val_loss: 3.2841e-05 - val_mae: 0.0035\n",
      "Epoch 695/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2073e-05 - mae: 0.0034 - val_loss: 3.2020e-05 - val_mae: 0.0034\n",
      "Epoch 696/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0576e-05 - mae: 0.0033 - val_loss: 3.2850e-05 - val_mae: 0.0035\n",
      "Epoch 697/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2488e-05 - mae: 0.0034 - val_loss: 3.2656e-05 - val_mae: 0.0035\n",
      "Epoch 698/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5216e-05 - mae: 0.0034 - val_loss: 3.3828e-05 - val_mae: 0.0036\n",
      "Epoch 699/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5700e-05 - mae: 0.0035 - val_loss: 3.2919e-05 - val_mae: 0.0035\n",
      "Epoch 700/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4975e-05 - mae: 0.0034 - val_loss: 3.1726e-05 - val_mae: 0.0034\n",
      "Epoch 701/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0656e-05 - mae: 0.0033 - val_loss: 3.3053e-05 - val_mae: 0.0035\n",
      "Epoch 702/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1499e-05 - mae: 0.0033 - val_loss: 3.2187e-05 - val_mae: 0.0034\n",
      "Epoch 703/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4152e-05 - mae: 0.0034 - val_loss: 3.2189e-05 - val_mae: 0.0035\n",
      "Epoch 704/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1790e-05 - mae: 0.0033 - val_loss: 3.2946e-05 - val_mae: 0.0035\n",
      "Epoch 705/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1086e-05 - mae: 0.0033 - val_loss: 3.3087e-05 - val_mae: 0.0035\n",
      "Epoch 706/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1289e-05 - mae: 0.0033 - val_loss: 3.1661e-05 - val_mae: 0.0034\n",
      "Epoch 707/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4060e-05 - mae: 0.0034 - val_loss: 3.2771e-05 - val_mae: 0.0035\n",
      "Epoch 708/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1410e-05 - mae: 0.0034 - val_loss: 3.2182e-05 - val_mae: 0.0034\n",
      "Epoch 709/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7889e-05 - mae: 0.0036 - val_loss: 3.4657e-05 - val_mae: 0.0035\n",
      "Epoch 710/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4972e-05 - mae: 0.0035 - val_loss: 3.2577e-05 - val_mae: 0.0034\n",
      "Epoch 711/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2192e-05 - mae: 0.0033 - val_loss: 3.1696e-05 - val_mae: 0.0034\n",
      "Epoch 712/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3194e-05 - mae: 0.0034 - val_loss: 3.1775e-05 - val_mae: 0.0034\n",
      "Epoch 713/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3428e-05 - mae: 0.0034 - val_loss: 3.0636e-05 - val_mae: 0.0033\n",
      "Epoch 714/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4028e-05 - mae: 0.0034 - val_loss: 3.1132e-05 - val_mae: 0.0034\n",
      "Epoch 715/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4083e-05 - mae: 0.0034 - val_loss: 3.1865e-05 - val_mae: 0.0034\n",
      "Epoch 716/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3418e-05 - mae: 0.0034 - val_loss: 3.1531e-05 - val_mae: 0.0034\n",
      "Epoch 717/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4936e-05 - mae: 0.0034 - val_loss: 3.1698e-05 - val_mae: 0.0034\n",
      "Epoch 718/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2891e-05 - mae: 0.0033 - val_loss: 3.2758e-05 - val_mae: 0.0034\n",
      "Epoch 719/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2295e-05 - mae: 0.0034 - val_loss: 3.2337e-05 - val_mae: 0.0034\n",
      "Epoch 720/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3864e-05 - mae: 0.0034 - val_loss: 3.1526e-05 - val_mae: 0.0034\n",
      "Epoch 721/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2722e-05 - mae: 0.0033 - val_loss: 3.1593e-05 - val_mae: 0.0034\n",
      "Epoch 722/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2560e-05 - mae: 0.0034 - val_loss: 3.2322e-05 - val_mae: 0.0034\n",
      "Epoch 723/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2659e-05 - mae: 0.0034 - val_loss: 3.1728e-05 - val_mae: 0.0034\n",
      "Epoch 724/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2380e-05 - mae: 0.0033 - val_loss: 3.2448e-05 - val_mae: 0.0034\n",
      "Epoch 725/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6804e-05 - mae: 0.0035 - val_loss: 3.3482e-05 - val_mae: 0.0035\n",
      "Epoch 726/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4747e-05 - mae: 0.0035 - val_loss: 3.4130e-05 - val_mae: 0.0035\n",
      "Epoch 727/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4119e-05 - mae: 0.0035 - val_loss: 3.1448e-05 - val_mae: 0.0034\n",
      "Epoch 728/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0445e-05 - mae: 0.0033 - val_loss: 3.1303e-05 - val_mae: 0.0034\n",
      "Epoch 729/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2618e-05 - mae: 0.0033 - val_loss: 3.1302e-05 - val_mae: 0.0034\n",
      "Epoch 730/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2969e-05 - mae: 0.0034 - val_loss: 3.3466e-05 - val_mae: 0.0035\n",
      "Epoch 731/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4870e-05 - mae: 0.0034 - val_loss: 3.4137e-05 - val_mae: 0.0036\n",
      "Epoch 732/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5944e-05 - mae: 0.0035 - val_loss: 3.3081e-05 - val_mae: 0.0035\n",
      "Epoch 733/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2613e-05 - mae: 0.0033 - val_loss: 3.1311e-05 - val_mae: 0.0034\n",
      "Epoch 734/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1794e-05 - mae: 0.0033 - val_loss: 3.2724e-05 - val_mae: 0.0034\n",
      "Epoch 735/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2242e-05 - mae: 0.0034 - val_loss: 3.4644e-05 - val_mae: 0.0035\n",
      "Epoch 736/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5985e-05 - mae: 0.0035 - val_loss: 3.2569e-05 - val_mae: 0.0035\n",
      "Epoch 737/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2482e-05 - mae: 0.0034 - val_loss: 3.2540e-05 - val_mae: 0.0035\n",
      "Epoch 738/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3481e-05 - mae: 0.0034 - val_loss: 3.2170e-05 - val_mae: 0.0034\n",
      "Epoch 739/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4816e-05 - mae: 0.0035 - val_loss: 3.1703e-05 - val_mae: 0.0034\n",
      "Epoch 740/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2036e-05 - mae: 0.0034 - val_loss: 3.1755e-05 - val_mae: 0.0034\n",
      "Epoch 741/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1571e-05 - mae: 0.0034 - val_loss: 3.3264e-05 - val_mae: 0.0035\n",
      "Epoch 742/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3826e-05 - mae: 0.0035 - val_loss: 3.3522e-05 - val_mae: 0.0035\n",
      "Epoch 743/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0684e-05 - mae: 0.0033 - val_loss: 3.1972e-05 - val_mae: 0.0034\n",
      "Epoch 744/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3054e-05 - mae: 0.0033 - val_loss: 3.1614e-05 - val_mae: 0.0034\n",
      "Epoch 745/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1414e-05 - mae: 0.0033 - val_loss: 3.1554e-05 - val_mae: 0.0034\n",
      "Epoch 746/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3497e-05 - mae: 0.0034 - val_loss: 3.1835e-05 - val_mae: 0.0034\n",
      "Epoch 747/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5472e-05 - mae: 0.0035 - val_loss: 3.1379e-05 - val_mae: 0.0034\n",
      "Epoch 748/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3045e-05 - mae: 0.0034 - val_loss: 3.1026e-05 - val_mae: 0.0033\n",
      "Epoch 749/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5897e-05 - mae: 0.0035 - val_loss: 3.2090e-05 - val_mae: 0.0034\n",
      "Epoch 750/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2872e-05 - mae: 0.0034 - val_loss: 3.1987e-05 - val_mae: 0.0033\n",
      "Epoch 751/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4729e-05 - mae: 0.0034 - val_loss: 3.2714e-05 - val_mae: 0.0034\n",
      "Epoch 752/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1635e-05 - mae: 0.0033 - val_loss: 3.3370e-05 - val_mae: 0.0035\n",
      "Epoch 753/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0783e-05 - mae: 0.0033 - val_loss: 3.2800e-05 - val_mae: 0.0035\n",
      "Epoch 754/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3901e-05 - mae: 0.0034 - val_loss: 3.1744e-05 - val_mae: 0.0034\n",
      "Epoch 755/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4766e-05 - mae: 0.0035 - val_loss: 3.2543e-05 - val_mae: 0.0034\n",
      "Epoch 756/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4805e-05 - mae: 0.0035 - val_loss: 3.5113e-05 - val_mae: 0.0035\n",
      "Epoch 757/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3056e-05 - mae: 0.0034 - val_loss: 3.2726e-05 - val_mae: 0.0034\n",
      "Epoch 758/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4656e-05 - mae: 0.0034 - val_loss: 3.2212e-05 - val_mae: 0.0034\n",
      "Epoch 759/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1477e-05 - mae: 0.0033 - val_loss: 3.3768e-05 - val_mae: 0.0035\n",
      "Epoch 760/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1905e-05 - mae: 0.0034 - val_loss: 3.2855e-05 - val_mae: 0.0034\n",
      "Epoch 761/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1041e-05 - mae: 0.0033 - val_loss: 3.1811e-05 - val_mae: 0.0034\n",
      "Epoch 762/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3244e-05 - mae: 0.0034 - val_loss: 3.2406e-05 - val_mae: 0.0034\n",
      "Epoch 763/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3045e-05 - mae: 0.0034 - val_loss: 3.3188e-05 - val_mae: 0.0034\n",
      "Epoch 764/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3361e-05 - mae: 0.0035 - val_loss: 3.2391e-05 - val_mae: 0.0035\n",
      "Epoch 765/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2961e-05 - mae: 0.0034 - val_loss: 3.2494e-05 - val_mae: 0.0034\n",
      "Epoch 766/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0361e-05 - mae: 0.0033 - val_loss: 3.1956e-05 - val_mae: 0.0034\n",
      "Epoch 767/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1386e-05 - mae: 0.0033 - val_loss: 3.1451e-05 - val_mae: 0.0034\n",
      "Epoch 768/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3247e-05 - mae: 0.0034 - val_loss: 3.1534e-05 - val_mae: 0.0034\n",
      "Epoch 769/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1663e-05 - mae: 0.0034 - val_loss: 3.3743e-05 - val_mae: 0.0035\n",
      "Epoch 770/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3688e-05 - mae: 0.0034 - val_loss: 3.2635e-05 - val_mae: 0.0034\n",
      "Epoch 771/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3364e-05 - mae: 0.0034 - val_loss: 3.3254e-05 - val_mae: 0.0035\n",
      "Epoch 772/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5426e-05 - mae: 0.0035 - val_loss: 3.1706e-05 - val_mae: 0.0034\n",
      "Epoch 773/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0003e-05 - mae: 0.0032 - val_loss: 3.0827e-05 - val_mae: 0.0034\n",
      "Epoch 774/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3516e-05 - mae: 0.0034 - val_loss: 3.1672e-05 - val_mae: 0.0034\n",
      "Epoch 775/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0429e-05 - mae: 0.0033 - val_loss: 3.2775e-05 - val_mae: 0.0035\n",
      "Epoch 776/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3349e-05 - mae: 0.0034 - val_loss: 3.1932e-05 - val_mae: 0.0034\n",
      "Epoch 777/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3197e-05 - mae: 0.0034 - val_loss: 3.2097e-05 - val_mae: 0.0033\n",
      "Epoch 778/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2741e-05 - mae: 0.0034 - val_loss: 3.3469e-05 - val_mae: 0.0035\n",
      "Epoch 779/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2237e-05 - mae: 0.0034 - val_loss: 3.1550e-05 - val_mae: 0.0034\n",
      "Epoch 780/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1066e-05 - mae: 0.0033 - val_loss: 3.0490e-05 - val_mae: 0.0033\n",
      "Epoch 781/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4271e-05 - mae: 0.0034 - val_loss: 3.0562e-05 - val_mae: 0.0033\n",
      "Epoch 782/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2229e-05 - mae: 0.0034 - val_loss: 3.1088e-05 - val_mae: 0.0033\n",
      "Epoch 783/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2183e-05 - mae: 0.0033 - val_loss: 3.3361e-05 - val_mae: 0.0034\n",
      "Epoch 784/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0426e-05 - mae: 0.0033 - val_loss: 3.1101e-05 - val_mae: 0.0033\n",
      "Epoch 785/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1804e-05 - mae: 0.0033 - val_loss: 3.0727e-05 - val_mae: 0.0033\n",
      "Epoch 786/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0696e-05 - mae: 0.0033 - val_loss: 3.1083e-05 - val_mae: 0.0033\n",
      "Epoch 787/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1983e-05 - mae: 0.0033 - val_loss: 3.0890e-05 - val_mae: 0.0033\n",
      "Epoch 788/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3259e-05 - mae: 0.0034 - val_loss: 3.0961e-05 - val_mae: 0.0033\n",
      "Epoch 789/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2855e-05 - mae: 0.0033 - val_loss: 3.2025e-05 - val_mae: 0.0034\n",
      "Epoch 790/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1373e-05 - mae: 0.0034 - val_loss: 3.2490e-05 - val_mae: 0.0035\n",
      "Epoch 791/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1171e-05 - mae: 0.0034 - val_loss: 3.0718e-05 - val_mae: 0.0033\n",
      "Epoch 792/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0497e-05 - mae: 0.0032 - val_loss: 3.0857e-05 - val_mae: 0.0033\n",
      "Epoch 793/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2836e-05 - mae: 0.0034 - val_loss: 3.2786e-05 - val_mae: 0.0035\n",
      "Epoch 794/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2755e-05 - mae: 0.0034 - val_loss: 3.1967e-05 - val_mae: 0.0034\n",
      "Epoch 795/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1255e-05 - mae: 0.0033 - val_loss: 3.1106e-05 - val_mae: 0.0034\n",
      "Epoch 796/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0744e-05 - mae: 0.0033 - val_loss: 3.0804e-05 - val_mae: 0.0034\n",
      "Epoch 797/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3023e-05 - mae: 0.0034 - val_loss: 3.1316e-05 - val_mae: 0.0034\n",
      "Epoch 798/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0267e-05 - mae: 0.0032 - val_loss: 3.0727e-05 - val_mae: 0.0033\n",
      "Epoch 799/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2551e-05 - mae: 0.0033 - val_loss: 3.0845e-05 - val_mae: 0.0033\n",
      "Epoch 800/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2551e-05 - mae: 0.0033 - val_loss: 3.2152e-05 - val_mae: 0.0034\n",
      "Epoch 801/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3741e-05 - mae: 0.0035 - val_loss: 3.1384e-05 - val_mae: 0.0034\n",
      "Epoch 802/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3746e-05 - mae: 0.0034 - val_loss: 3.1994e-05 - val_mae: 0.0034\n",
      "Epoch 803/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3997e-05 - mae: 0.0034 - val_loss: 3.1599e-05 - val_mae: 0.0034\n",
      "Epoch 804/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2142e-05 - mae: 0.0033 - val_loss: 3.0818e-05 - val_mae: 0.0034\n",
      "Epoch 805/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4437e-05 - mae: 0.0034 - val_loss: 3.2262e-05 - val_mae: 0.0035\n",
      "Epoch 806/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2802e-05 - mae: 0.0034 - val_loss: 3.1125e-05 - val_mae: 0.0034\n",
      "Epoch 807/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1073e-05 - mae: 0.0033 - val_loss: 3.1091e-05 - val_mae: 0.0033\n",
      "Epoch 808/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2005e-05 - mae: 0.0033 - val_loss: 3.1326e-05 - val_mae: 0.0034\n",
      "Epoch 809/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2825e-05 - mae: 0.0034 - val_loss: 3.1508e-05 - val_mae: 0.0034\n",
      "Epoch 810/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4134e-05 - mae: 0.0034 - val_loss: 3.3722e-05 - val_mae: 0.0035\n",
      "Epoch 811/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3087e-05 - mae: 0.0034 - val_loss: 3.5137e-05 - val_mae: 0.0035\n",
      "Epoch 812/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3222e-05 - mae: 0.0035 - val_loss: 3.3714e-05 - val_mae: 0.0035\n",
      "Epoch 813/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2825e-05 - mae: 0.0034 - val_loss: 3.1861e-05 - val_mae: 0.0034\n",
      "Epoch 814/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2726e-05 - mae: 0.0034 - val_loss: 3.3223e-05 - val_mae: 0.0035\n",
      "Epoch 815/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4532e-05 - mae: 0.0035 - val_loss: 3.2068e-05 - val_mae: 0.0035\n",
      "Epoch 816/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4442e-05 - mae: 0.0035 - val_loss: 3.2245e-05 - val_mae: 0.0035\n",
      "Epoch 817/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5305e-05 - mae: 0.0035 - val_loss: 3.1474e-05 - val_mae: 0.0034\n",
      "Epoch 818/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4003e-05 - mae: 0.0034 - val_loss: 2.9949e-05 - val_mae: 0.0033\n",
      "Epoch 819/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2154e-05 - mae: 0.0033 - val_loss: 2.9507e-05 - val_mae: 0.0033\n",
      "Epoch 820/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3053e-05 - mae: 0.0034 - val_loss: 3.2167e-05 - val_mae: 0.0035\n",
      "Epoch 821/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2182e-05 - mae: 0.0034 - val_loss: 3.4864e-05 - val_mae: 0.0037\n",
      "Epoch 822/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3356e-05 - mae: 0.0035 - val_loss: 3.2424e-05 - val_mae: 0.0035\n",
      "Epoch 823/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2523e-05 - mae: 0.0034 - val_loss: 3.2017e-05 - val_mae: 0.0034\n",
      "Epoch 824/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0870e-05 - mae: 0.0033 - val_loss: 3.2620e-05 - val_mae: 0.0034\n",
      "Epoch 825/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0695e-05 - mae: 0.0033 - val_loss: 3.3153e-05 - val_mae: 0.0035\n",
      "Epoch 826/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2202e-05 - mae: 0.0034 - val_loss: 3.2081e-05 - val_mae: 0.0034\n",
      "Epoch 827/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2083e-05 - mae: 0.0033 - val_loss: 3.1995e-05 - val_mae: 0.0034\n",
      "Epoch 828/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2283e-05 - mae: 0.0033 - val_loss: 3.4375e-05 - val_mae: 0.0035\n",
      "Epoch 829/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2052e-05 - mae: 0.0034 - val_loss: 3.2135e-05 - val_mae: 0.0033\n",
      "Epoch 830/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4429e-05 - mae: 0.0034 - val_loss: 3.4289e-05 - val_mae: 0.0034\n",
      "Epoch 831/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3199e-05 - mae: 0.0034 - val_loss: 3.3753e-05 - val_mae: 0.0035\n",
      "Epoch 832/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2657e-05 - mae: 0.0034 - val_loss: 3.3834e-05 - val_mae: 0.0035\n",
      "Epoch 833/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0275e-05 - mae: 0.0033 - val_loss: 3.2673e-05 - val_mae: 0.0034\n",
      "Epoch 834/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3354e-05 - mae: 0.0034 - val_loss: 3.2085e-05 - val_mae: 0.0034\n",
      "Epoch 835/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6351e-05 - mae: 0.0035 - val_loss: 3.1819e-05 - val_mae: 0.0034\n",
      "Epoch 836/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3874e-05 - mae: 0.0034 - val_loss: 3.1463e-05 - val_mae: 0.0034\n",
      "Epoch 837/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1957e-05 - mae: 0.0033 - val_loss: 3.1391e-05 - val_mae: 0.0033\n",
      "Epoch 838/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2876e-05 - mae: 0.0034 - val_loss: 3.3804e-05 - val_mae: 0.0035\n",
      "Epoch 839/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4607e-05 - mae: 0.0035 - val_loss: 3.2219e-05 - val_mae: 0.0034\n",
      "Epoch 840/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2233e-05 - mae: 0.0033 - val_loss: 3.2057e-05 - val_mae: 0.0034\n",
      "Epoch 841/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2599e-05 - mae: 0.0034 - val_loss: 3.3075e-05 - val_mae: 0.0035\n",
      "Epoch 842/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.7954e-05 - mae: 0.0032 - val_loss: 3.2495e-05 - val_mae: 0.0035\n",
      "Epoch 843/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0236e-05 - mae: 0.0033 - val_loss: 3.3417e-05 - val_mae: 0.0035\n",
      "Epoch 844/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5700e-05 - mae: 0.0035 - val_loss: 3.2322e-05 - val_mae: 0.0034\n",
      "Epoch 845/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4175e-05 - mae: 0.0034 - val_loss: 3.2246e-05 - val_mae: 0.0033\n",
      "Epoch 846/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1116e-05 - mae: 0.0033 - val_loss: 3.2779e-05 - val_mae: 0.0035\n",
      "Epoch 847/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2876e-05 - mae: 0.0034 - val_loss: 3.2340e-05 - val_mae: 0.0034\n",
      "Epoch 848/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3419e-05 - mae: 0.0034 - val_loss: 3.1514e-05 - val_mae: 0.0034\n",
      "Epoch 849/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1499e-05 - mae: 0.0033 - val_loss: 3.1069e-05 - val_mae: 0.0034\n",
      "Epoch 850/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0656e-05 - mae: 0.0032 - val_loss: 3.1740e-05 - val_mae: 0.0034\n",
      "Epoch 851/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1528e-05 - mae: 0.0033 - val_loss: 3.2001e-05 - val_mae: 0.0034\n",
      "Epoch 852/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4531e-05 - mae: 0.0035 - val_loss: 3.2001e-05 - val_mae: 0.0034\n",
      "Epoch 853/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2584e-05 - mae: 0.0034 - val_loss: 3.2598e-05 - val_mae: 0.0033\n",
      "Epoch 854/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2999e-05 - mae: 0.0034 - val_loss: 3.2489e-05 - val_mae: 0.0033\n",
      "Epoch 855/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2670e-05 - mae: 0.0034 - val_loss: 3.2701e-05 - val_mae: 0.0034\n",
      "Epoch 856/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.8894e-05 - mae: 0.0032 - val_loss: 3.2369e-05 - val_mae: 0.0034\n",
      "Epoch 857/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3647e-05 - mae: 0.0034 - val_loss: 3.3782e-05 - val_mae: 0.0036\n",
      "Epoch 858/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0890e-05 - mae: 0.0033 - val_loss: 3.2868e-05 - val_mae: 0.0035\n",
      "Epoch 859/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2463e-05 - mae: 0.0034 - val_loss: 3.2329e-05 - val_mae: 0.0034\n",
      "Epoch 860/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1709e-05 - mae: 0.0034 - val_loss: 3.2937e-05 - val_mae: 0.0035\n",
      "Epoch 861/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3251e-05 - mae: 0.0034 - val_loss: 3.3101e-05 - val_mae: 0.0034\n",
      "Epoch 862/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2587e-05 - mae: 0.0034 - val_loss: 3.3237e-05 - val_mae: 0.0035\n",
      "Epoch 863/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2602e-05 - mae: 0.0034 - val_loss: 3.3290e-05 - val_mae: 0.0034\n",
      "Epoch 864/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0204e-05 - mae: 0.0033 - val_loss: 3.1760e-05 - val_mae: 0.0034\n",
      "Epoch 865/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3521e-05 - mae: 0.0034 - val_loss: 3.4865e-05 - val_mae: 0.0036\n",
      "Epoch 866/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2760e-05 - mae: 0.0034 - val_loss: 3.3566e-05 - val_mae: 0.0035\n",
      "Epoch 867/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5952e-05 - mae: 0.0035 - val_loss: 3.2381e-05 - val_mae: 0.0034\n",
      "Epoch 868/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0186e-05 - mae: 0.0033 - val_loss: 3.2943e-05 - val_mae: 0.0035\n",
      "Epoch 869/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.8293e-05 - mae: 0.0032 - val_loss: 3.3361e-05 - val_mae: 0.0035\n",
      "Epoch 870/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2375e-05 - mae: 0.0034 - val_loss: 3.2264e-05 - val_mae: 0.0034\n",
      "Epoch 871/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1970e-05 - mae: 0.0033 - val_loss: 3.2653e-05 - val_mae: 0.0034\n",
      "Epoch 872/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2522e-05 - mae: 0.0034 - val_loss: 3.3455e-05 - val_mae: 0.0035\n",
      "Epoch 873/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1368e-05 - mae: 0.0034 - val_loss: 3.2603e-05 - val_mae: 0.0034\n",
      "Epoch 874/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7574e-05 - mae: 0.0035 - val_loss: 3.2251e-05 - val_mae: 0.0034\n",
      "Epoch 875/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1631e-05 - mae: 0.0033 - val_loss: 3.2799e-05 - val_mae: 0.0034\n",
      "Epoch 876/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5607e-05 - mae: 0.0035 - val_loss: 3.3806e-05 - val_mae: 0.0034\n",
      "Epoch 877/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2957e-05 - mae: 0.0034 - val_loss: 3.3187e-05 - val_mae: 0.0034\n",
      "Epoch 878/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2364e-05 - mae: 0.0033 - val_loss: 3.2333e-05 - val_mae: 0.0034\n",
      "Epoch 879/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1457e-05 - mae: 0.0033 - val_loss: 3.1354e-05 - val_mae: 0.0034\n",
      "Epoch 880/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2038e-05 - mae: 0.0033 - val_loss: 3.1732e-05 - val_mae: 0.0034\n",
      "Epoch 881/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3378e-05 - mae: 0.0034 - val_loss: 3.2572e-05 - val_mae: 0.0035\n",
      "Epoch 882/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4449e-05 - mae: 0.0035 - val_loss: 3.2972e-05 - val_mae: 0.0035\n",
      "Epoch 883/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3609e-05 - mae: 0.0034 - val_loss: 3.1381e-05 - val_mae: 0.0033\n",
      "Epoch 884/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1982e-05 - mae: 0.0033 - val_loss: 3.2181e-05 - val_mae: 0.0034\n",
      "Epoch 885/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4783e-05 - mae: 0.0034 - val_loss: 3.3021e-05 - val_mae: 0.0034\n",
      "Epoch 886/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3465e-05 - mae: 0.0034 - val_loss: 3.3129e-05 - val_mae: 0.0035\n",
      "Epoch 887/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0780e-05 - mae: 0.0034 - val_loss: 3.2967e-05 - val_mae: 0.0034\n",
      "Epoch 888/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0870e-05 - mae: 0.0033 - val_loss: 3.1541e-05 - val_mae: 0.0034\n",
      "Epoch 889/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2397e-05 - mae: 0.0033 - val_loss: 3.0598e-05 - val_mae: 0.0034\n",
      "Epoch 890/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1857e-05 - mae: 0.0033 - val_loss: 3.1675e-05 - val_mae: 0.0034\n",
      "Epoch 891/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.9265e-05 - mae: 0.0032 - val_loss: 3.3458e-05 - val_mae: 0.0035\n",
      "Epoch 892/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1376e-05 - mae: 0.0034 - val_loss: 3.4275e-05 - val_mae: 0.0034\n",
      "Epoch 893/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5252e-05 - mae: 0.0034 - val_loss: 3.4110e-05 - val_mae: 0.0035\n",
      "Epoch 894/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4351e-05 - mae: 0.0034 - val_loss: 3.2511e-05 - val_mae: 0.0034\n",
      "Epoch 895/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3217e-05 - mae: 0.0034 - val_loss: 3.1150e-05 - val_mae: 0.0034\n",
      "Epoch 896/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2883e-05 - mae: 0.0034 - val_loss: 3.1335e-05 - val_mae: 0.0035\n",
      "Epoch 897/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2732e-05 - mae: 0.0035 - val_loss: 3.2540e-05 - val_mae: 0.0035\n",
      "Epoch 898/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4541e-05 - mae: 0.0036 - val_loss: 3.2454e-05 - val_mae: 0.0034\n",
      "Epoch 899/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2022e-05 - mae: 0.0034 - val_loss: 3.2297e-05 - val_mae: 0.0034\n",
      "Epoch 900/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.9522e-05 - mae: 0.0032 - val_loss: 3.1958e-05 - val_mae: 0.0034\n",
      "Epoch 901/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2701e-05 - mae: 0.0033 - val_loss: 3.2883e-05 - val_mae: 0.0035\n",
      "Epoch 902/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3361e-05 - mae: 0.0034 - val_loss: 3.2391e-05 - val_mae: 0.0035\n",
      "Epoch 903/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3432e-05 - mae: 0.0034 - val_loss: 3.1804e-05 - val_mae: 0.0034\n",
      "Epoch 904/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2542e-05 - mae: 0.0033 - val_loss: 3.2681e-05 - val_mae: 0.0034\n",
      "Epoch 905/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3051e-05 - mae: 0.0033 - val_loss: 3.3596e-05 - val_mae: 0.0035\n",
      "Epoch 906/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5069e-05 - mae: 0.0035 - val_loss: 3.3905e-05 - val_mae: 0.0035\n",
      "Epoch 907/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4710e-05 - mae: 0.0035 - val_loss: 3.3739e-05 - val_mae: 0.0035\n",
      "Epoch 908/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2792e-05 - mae: 0.0034 - val_loss: 3.1659e-05 - val_mae: 0.0034\n",
      "Epoch 909/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2175e-05 - mae: 0.0033 - val_loss: 3.1207e-05 - val_mae: 0.0033\n",
      "Epoch 910/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.9579e-05 - mae: 0.0032 - val_loss: 3.1966e-05 - val_mae: 0.0034\n",
      "Epoch 911/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4170e-05 - mae: 0.0034 - val_loss: 3.2516e-05 - val_mae: 0.0034\n",
      "Epoch 912/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2746e-05 - mae: 0.0034 - val_loss: 3.3497e-05 - val_mae: 0.0034\n",
      "Epoch 913/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5966e-05 - mae: 0.0034 - val_loss: 3.3394e-05 - val_mae: 0.0034\n",
      "Epoch 914/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2203e-05 - mae: 0.0033 - val_loss: 3.2350e-05 - val_mae: 0.0034\n",
      "Epoch 915/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3475e-05 - mae: 0.0034 - val_loss: 3.2050e-05 - val_mae: 0.0034\n",
      "Epoch 916/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2394e-05 - mae: 0.0034 - val_loss: 3.1272e-05 - val_mae: 0.0034\n",
      "Epoch 917/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1546e-05 - mae: 0.0033 - val_loss: 3.1243e-05 - val_mae: 0.0034\n",
      "Epoch 918/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4694e-05 - mae: 0.0034 - val_loss: 3.2394e-05 - val_mae: 0.0034\n",
      "Epoch 919/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2149e-05 - mae: 0.0033 - val_loss: 3.2921e-05 - val_mae: 0.0034\n",
      "Epoch 920/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3922e-05 - mae: 0.0034 - val_loss: 3.3588e-05 - val_mae: 0.0034\n",
      "Epoch 921/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4371e-05 - mae: 0.0034 - val_loss: 3.3934e-05 - val_mae: 0.0034\n",
      "Epoch 922/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3949e-05 - mae: 0.0034 - val_loss: 3.4162e-05 - val_mae: 0.0035\n",
      "Epoch 923/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1575e-05 - mae: 0.0034 - val_loss: 3.2813e-05 - val_mae: 0.0035\n",
      "Epoch 924/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3235e-05 - mae: 0.0034 - val_loss: 3.1833e-05 - val_mae: 0.0034\n",
      "Epoch 925/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.8582e-05 - mae: 0.0032 - val_loss: 3.1819e-05 - val_mae: 0.0034\n",
      "Epoch 926/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3663e-05 - mae: 0.0034 - val_loss: 3.1903e-05 - val_mae: 0.0034\n",
      "Epoch 927/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1029e-05 - mae: 0.0033 - val_loss: 3.3100e-05 - val_mae: 0.0035\n",
      "Epoch 928/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0276e-05 - mae: 0.0033 - val_loss: 3.2877e-05 - val_mae: 0.0034\n",
      "Epoch 929/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0353e-05 - mae: 0.0033 - val_loss: 3.2629e-05 - val_mae: 0.0034\n",
      "Epoch 930/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3121e-05 - mae: 0.0034 - val_loss: 3.3883e-05 - val_mae: 0.0035\n",
      "Epoch 931/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2897e-05 - mae: 0.0034 - val_loss: 3.2365e-05 - val_mae: 0.0034\n",
      "Epoch 932/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1512e-05 - mae: 0.0033 - val_loss: 3.1863e-05 - val_mae: 0.0034\n",
      "Epoch 933/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.8678e-05 - mae: 0.0032 - val_loss: 3.2426e-05 - val_mae: 0.0034\n",
      "Epoch 934/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4829e-05 - mae: 0.0034 - val_loss: 3.2348e-05 - val_mae: 0.0034\n",
      "Epoch 935/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2350e-05 - mae: 0.0033 - val_loss: 3.2181e-05 - val_mae: 0.0034\n",
      "Epoch 936/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.8321e-05 - mae: 0.0032 - val_loss: 3.3432e-05 - val_mae: 0.0035\n",
      "Epoch 937/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4304e-05 - mae: 0.0034 - val_loss: 3.3979e-05 - val_mae: 0.0035\n",
      "Epoch 938/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3472e-05 - mae: 0.0034 - val_loss: 3.2645e-05 - val_mae: 0.0034\n",
      "Epoch 939/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0453e-05 - mae: 0.0033 - val_loss: 3.2236e-05 - val_mae: 0.0034\n",
      "Epoch 940/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.9174e-05 - mae: 0.0032 - val_loss: 3.1474e-05 - val_mae: 0.0033\n",
      "Epoch 941/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1777e-05 - mae: 0.0033 - val_loss: 3.1799e-05 - val_mae: 0.0034\n",
      "Epoch 942/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2756e-05 - mae: 0.0034 - val_loss: 3.2140e-05 - val_mae: 0.0034\n",
      "Epoch 943/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3124e-05 - mae: 0.0034 - val_loss: 3.1267e-05 - val_mae: 0.0034\n",
      "Epoch 944/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2771e-05 - mae: 0.0034 - val_loss: 3.3335e-05 - val_mae: 0.0035\n",
      "Epoch 945/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4500e-05 - mae: 0.0034 - val_loss: 3.1838e-05 - val_mae: 0.0033\n",
      "Epoch 946/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2602e-05 - mae: 0.0033 - val_loss: 3.5069e-05 - val_mae: 0.0034\n",
      "Epoch 947/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3306e-05 - mae: 0.0034 - val_loss: 3.3657e-05 - val_mae: 0.0034\n",
      "Epoch 948/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2263e-05 - mae: 0.0034 - val_loss: 3.6059e-05 - val_mae: 0.0035\n",
      "Epoch 949/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4110e-05 - mae: 0.0035 - val_loss: 3.2966e-05 - val_mae: 0.0034\n",
      "Epoch 950/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0650e-05 - mae: 0.0033 - val_loss: 3.2029e-05 - val_mae: 0.0034\n",
      "Epoch 951/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2175e-05 - mae: 0.0033 - val_loss: 3.2835e-05 - val_mae: 0.0035\n",
      "Epoch 952/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2141e-05 - mae: 0.0034 - val_loss: 3.3956e-05 - val_mae: 0.0035\n",
      "Epoch 953/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2360e-05 - mae: 0.0034 - val_loss: 3.2553e-05 - val_mae: 0.0034\n",
      "Epoch 954/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4739e-05 - mae: 0.0034 - val_loss: 3.1889e-05 - val_mae: 0.0034\n",
      "Epoch 955/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2389e-05 - mae: 0.0033 - val_loss: 3.2033e-05 - val_mae: 0.0034\n",
      "Epoch 956/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3640e-05 - mae: 0.0034 - val_loss: 3.2696e-05 - val_mae: 0.0035\n",
      "Epoch 957/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3603e-05 - mae: 0.0034 - val_loss: 3.2867e-05 - val_mae: 0.0035\n",
      "Epoch 958/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2401e-05 - mae: 0.0033 - val_loss: 3.3364e-05 - val_mae: 0.0035\n",
      "Epoch 959/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0503e-05 - mae: 0.0033 - val_loss: 3.2635e-05 - val_mae: 0.0034\n",
      "Epoch 960/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4063e-05 - mae: 0.0034 - val_loss: 3.1817e-05 - val_mae: 0.0034\n",
      "Epoch 961/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3125e-05 - mae: 0.0034 - val_loss: 3.2598e-05 - val_mae: 0.0034\n",
      "Epoch 962/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2341e-05 - mae: 0.0034 - val_loss: 3.3283e-05 - val_mae: 0.0035\n",
      "Epoch 963/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3445e-05 - mae: 0.0034 - val_loss: 3.2040e-05 - val_mae: 0.0034\n",
      "Epoch 964/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2304e-05 - mae: 0.0034 - val_loss: 3.2735e-05 - val_mae: 0.0034\n",
      "Epoch 965/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1557e-05 - mae: 0.0033 - val_loss: 3.3199e-05 - val_mae: 0.0035\n",
      "Epoch 966/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6535e-05 - mae: 0.0036 - val_loss: 3.1843e-05 - val_mae: 0.0033\n",
      "Epoch 967/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4293e-05 - mae: 0.0035 - val_loss: 3.1698e-05 - val_mae: 0.0033\n",
      "Epoch 968/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0315e-05 - mae: 0.0033 - val_loss: 3.4625e-05 - val_mae: 0.0035\n",
      "Epoch 969/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5087e-05 - mae: 0.0035 - val_loss: 3.2325e-05 - val_mae: 0.0034\n",
      "Epoch 970/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3503e-05 - mae: 0.0034 - val_loss: 3.2337e-05 - val_mae: 0.0034\n",
      "Epoch 971/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0305e-05 - mae: 0.0032 - val_loss: 3.2334e-05 - val_mae: 0.0034\n",
      "Epoch 972/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3121e-05 - mae: 0.0034 - val_loss: 3.2509e-05 - val_mae: 0.0035\n",
      "Epoch 973/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4224e-05 - mae: 0.0034 - val_loss: 3.1773e-05 - val_mae: 0.0034\n",
      "Epoch 974/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.8942e-05 - mae: 0.0032 - val_loss: 3.1709e-05 - val_mae: 0.0034\n",
      "Epoch 975/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.9749e-05 - mae: 0.0032 - val_loss: 3.3676e-05 - val_mae: 0.0035\n",
      "Epoch 976/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3947e-05 - mae: 0.0034 - val_loss: 3.2096e-05 - val_mae: 0.0034\n",
      "Epoch 977/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1221e-05 - mae: 0.0033 - val_loss: 3.1795e-05 - val_mae: 0.0034\n",
      "Epoch 978/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.9037e-05 - mae: 0.0032 - val_loss: 3.2358e-05 - val_mae: 0.0035\n",
      "Epoch 979/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4023e-05 - mae: 0.0034 - val_loss: 3.2236e-05 - val_mae: 0.0035\n",
      "Epoch 980/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0478e-05 - mae: 0.0033 - val_loss: 3.1506e-05 - val_mae: 0.0034\n",
      "Epoch 981/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1169e-05 - mae: 0.0032 - val_loss: 3.1857e-05 - val_mae: 0.0034\n",
      "Epoch 982/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.0042e-05 - mae: 0.0032 - val_loss: 3.1789e-05 - val_mae: 0.0034\n",
      "Epoch 983/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0220e-05 - mae: 0.0032 - val_loss: 3.2307e-05 - val_mae: 0.0034\n",
      "Epoch 984/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.9460e-05 - mae: 0.0032 - val_loss: 3.1544e-05 - val_mae: 0.0034\n",
      "Epoch 985/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.8917e-05 - mae: 0.0032 - val_loss: 3.1784e-05 - val_mae: 0.0034\n",
      "Epoch 986/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0909e-05 - mae: 0.0033 - val_loss: 3.2267e-05 - val_mae: 0.0034\n",
      "Epoch 987/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3393e-05 - mae: 0.0034 - val_loss: 3.1992e-05 - val_mae: 0.0034\n",
      "Epoch 988/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0427e-05 - mae: 0.0033 - val_loss: 3.2891e-05 - val_mae: 0.0034\n",
      "Epoch 989/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3080e-05 - mae: 0.0034 - val_loss: 3.2877e-05 - val_mae: 0.0034\n",
      "Epoch 990/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.9416e-05 - mae: 0.0032 - val_loss: 3.2470e-05 - val_mae: 0.0035\n",
      "Epoch 991/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4100e-05 - mae: 0.0034 - val_loss: 3.2110e-05 - val_mae: 0.0034\n",
      "Epoch 992/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5390e-05 - mae: 0.0035 - val_loss: 3.2761e-05 - val_mae: 0.0034\n",
      "Epoch 993/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0712e-05 - mae: 0.0033 - val_loss: 3.2103e-05 - val_mae: 0.0034\n",
      "Epoch 994/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1781e-05 - mae: 0.0033 - val_loss: 3.2396e-05 - val_mae: 0.0034\n",
      "Epoch 995/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0194e-05 - mae: 0.0033 - val_loss: 3.2009e-05 - val_mae: 0.0034\n",
      "Epoch 996/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2924e-05 - mae: 0.0033 - val_loss: 3.2179e-05 - val_mae: 0.0034\n",
      "Epoch 997/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2635e-05 - mae: 0.0033 - val_loss: 3.1835e-05 - val_mae: 0.0034\n",
      "Epoch 998/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1560e-05 - mae: 0.0033 - val_loss: 3.2146e-05 - val_mae: 0.0034\n",
      "Epoch 999/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.9282e-05 - mae: 0.0032 - val_loss: 3.2386e-05 - val_mae: 0.0034\n",
      "Epoch 1000/1000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2929e-05 - mae: 0.0033 - val_loss: 3.2350e-05 - val_mae: 0.0034\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model1.fit(X_train, y_train, epochs=1000, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bd1081-e9f3-4151-87e4-ac672bc29136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.8345e-05 - mae: 0.0031\n",
      "Model Loss: 2.7921600121771917e-05, Model MAE: 0.0031059342436492443\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, mae = model1.evaluate(X_train, y_train)\n",
    "print(f'Model Loss: {loss}, Model MAE: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d5a3db-b803-425b-947b-705f9a58c052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation MAE values\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('Model Mean Absolute Error')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd8d663-5fa6-4a89-8509-739dc0d6222e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TF-Metal/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a more complex model\n",
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(2048, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(y_train.shape[1], activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model with a different optimizer and a learning rate scheduler\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "\n",
    "model2.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a57b4b-623a-4ee3-bf4a-20844f994572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 1.6547e-04 - mae: 0.0082 - val_loss: 1.0389e-04 - val_mae: 0.0061 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.4157e-05 - mae: 0.0062 - val_loss: 5.6686e-05 - val_mae: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.1562e-05 - mae: 0.0053 - val_loss: 4.5124e-05 - val_mae: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.1203e-05 - mae: 0.0047 - val_loss: 4.3329e-05 - val_mae: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.9072e-05 - mae: 0.0045 - val_loss: 4.1830e-05 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7777e-05 - mae: 0.0043 - val_loss: 4.1722e-05 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.9007e-05 - mae: 0.0044 - val_loss: 4.1515e-05 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.9217e-05 - mae: 0.0044 - val_loss: 4.1345e-05 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9105e-05 - mae: 0.0044 - val_loss: 4.1155e-05 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7195e-05 - mae: 0.0043 - val_loss: 4.1191e-05 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6689e-05 - mae: 0.0043\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6796e-05 - mae: 0.0043 - val_loss: 4.1246e-05 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7211e-05 - mae: 0.0043 - val_loss: 4.1363e-05 - val_mae: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 13/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6400e-05 - mae: 0.0043 - val_loss: 4.1221e-05 - val_mae: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 14/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.7264e-05 - mae: 0.0043 - val_loss: 4.1147e-05 - val_mae: 0.0041 - learning_rate: 5.0000e-04\n",
      "Epoch 15/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8290e-05 - mae: 0.0043 - val_loss: 4.1122e-05 - val_mae: 0.0041 - learning_rate: 5.0000e-04\n",
      "Epoch 16/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9052e-05 - mae: 0.0043 - val_loss: 4.1051e-05 - val_mae: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 17/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7330e-05 - mae: 0.0043 - val_loss: 4.1050e-05 - val_mae: 0.0041 - learning_rate: 5.0000e-04\n",
      "Epoch 18/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8143e-05 - mae: 0.0043 - val_loss: 4.0965e-05 - val_mae: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 19/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4847e-05 - mae: 0.0042 - val_loss: 4.1202e-05 - val_mae: 0.0041 - learning_rate: 5.0000e-04\n",
      "Epoch 20/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6381e-05 - mae: 0.0043 - val_loss: 4.0861e-05 - val_mae: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 21/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7611e-05 - mae: 0.0043\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7588e-05 - mae: 0.0043 - val_loss: 4.0940e-05 - val_mae: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 22/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6671e-05 - mae: 0.0043 - val_loss: 4.0891e-05 - val_mae: 0.0040 - learning_rate: 2.5000e-04\n",
      "Epoch 23/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4279e-05 - mae: 0.0041 - val_loss: 4.0859e-05 - val_mae: 0.0040 - learning_rate: 2.5000e-04\n",
      "Epoch 24/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6727e-05 - mae: 0.0042 - val_loss: 4.0930e-05 - val_mae: 0.0040 - learning_rate: 2.5000e-04\n",
      "Epoch 25/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9011e-05 - mae: 0.0043 - val_loss: 4.0875e-05 - val_mae: 0.0040 - learning_rate: 2.5000e-04\n",
      "Epoch 26/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.0045e-05 - mae: 0.0044 - val_loss: 4.0797e-05 - val_mae: 0.0040 - learning_rate: 2.5000e-04\n",
      "Epoch 27/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5223e-05 - mae: 0.0042 - val_loss: 4.0813e-05 - val_mae: 0.0040 - learning_rate: 2.5000e-04\n",
      "Epoch 28/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7431e-05 - mae: 0.0043 - val_loss: 4.0774e-05 - val_mae: 0.0040 - learning_rate: 2.5000e-04\n",
      "Epoch 29/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0626e-05 - mae: 0.0044 - val_loss: 4.0785e-05 - val_mae: 0.0040 - learning_rate: 2.5000e-04\n",
      "Epoch 30/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6762e-05 - mae: 0.0042 - val_loss: 4.0765e-05 - val_mae: 0.0040 - learning_rate: 2.5000e-04\n",
      "Epoch 31/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6605e-05 - mae: 0.0042\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6633e-05 - mae: 0.0042 - val_loss: 4.0708e-05 - val_mae: 0.0040 - learning_rate: 2.5000e-04\n",
      "Epoch 32/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6124e-05 - mae: 0.0042 - val_loss: 4.0692e-05 - val_mae: 0.0040 - learning_rate: 1.2500e-04\n",
      "Epoch 33/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7976e-05 - mae: 0.0043 - val_loss: 4.0669e-05 - val_mae: 0.0040 - learning_rate: 1.2500e-04\n",
      "Epoch 34/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6381e-05 - mae: 0.0042 - val_loss: 4.0674e-05 - val_mae: 0.0040 - learning_rate: 1.2500e-04\n",
      "Epoch 35/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8251e-05 - mae: 0.0043 - val_loss: 4.0703e-05 - val_mae: 0.0040 - learning_rate: 1.2500e-04\n",
      "Epoch 36/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8604e-05 - mae: 0.0043 - val_loss: 4.0621e-05 - val_mae: 0.0040 - learning_rate: 1.2500e-04\n",
      "Epoch 37/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8870e-05 - mae: 0.0043 - val_loss: 4.0597e-05 - val_mae: 0.0040 - learning_rate: 1.2500e-04\n",
      "Epoch 38/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6138e-05 - mae: 0.0042 - val_loss: 4.0612e-05 - val_mae: 0.0040 - learning_rate: 1.2500e-04\n",
      "Epoch 39/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7200e-05 - mae: 0.0042 - val_loss: 4.0592e-05 - val_mae: 0.0040 - learning_rate: 1.2500e-04\n",
      "Epoch 40/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8062e-05 - mae: 0.0043 - val_loss: 4.0596e-05 - val_mae: 0.0040 - learning_rate: 1.2500e-04\n",
      "Epoch 41/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4750e-05 - mae: 0.0042\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4938e-05 - mae: 0.0042 - val_loss: 4.0544e-05 - val_mae: 0.0040 - learning_rate: 1.2500e-04\n",
      "Epoch 42/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4534e-05 - mae: 0.0042 - val_loss: 4.0563e-05 - val_mae: 0.0040 - learning_rate: 6.2500e-05\n",
      "Epoch 43/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.1525e-05 - mae: 0.0044 - val_loss: 4.0581e-05 - val_mae: 0.0040 - learning_rate: 6.2500e-05\n",
      "Epoch 44/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6710e-05 - mae: 0.0042 - val_loss: 4.0547e-05 - val_mae: 0.0040 - learning_rate: 6.2500e-05\n",
      "Epoch 45/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8005e-05 - mae: 0.0043 - val_loss: 4.0544e-05 - val_mae: 0.0040 - learning_rate: 6.2500e-05\n",
      "Epoch 46/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.5379e-05 - mae: 0.0042 - val_loss: 4.0558e-05 - val_mae: 0.0040 - learning_rate: 6.2500e-05\n",
      "Epoch 47/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.8034e-05 - mae: 0.0043 - val_loss: 4.0557e-05 - val_mae: 0.0040 - learning_rate: 6.2500e-05\n",
      "Epoch 48/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5836e-05 - mae: 0.0042 - val_loss: 4.0534e-05 - val_mae: 0.0040 - learning_rate: 6.2500e-05\n",
      "Epoch 49/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7149e-05 - mae: 0.0043 - val_loss: 4.0552e-05 - val_mae: 0.0040 - learning_rate: 6.2500e-05\n",
      "Epoch 50/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6955e-05 - mae: 0.0043 - val_loss: 4.0541e-05 - val_mae: 0.0040 - learning_rate: 6.2500e-05\n",
      "Epoch 51/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8074e-05 - mae: 0.0043\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7904e-05 - mae: 0.0043 - val_loss: 4.0503e-05 - val_mae: 0.0040 - learning_rate: 6.2500e-05\n",
      "Epoch 52/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7516e-05 - mae: 0.0043 - val_loss: 4.0509e-05 - val_mae: 0.0040 - learning_rate: 3.1250e-05\n",
      "Epoch 53/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4734e-05 - mae: 0.0042 - val_loss: 4.0503e-05 - val_mae: 0.0040 - learning_rate: 3.1250e-05\n",
      "Epoch 54/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.5132e-05 - mae: 0.0042 - val_loss: 4.0509e-05 - val_mae: 0.0040 - learning_rate: 3.1250e-05\n",
      "Epoch 55/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3771e-05 - mae: 0.0041 - val_loss: 4.0508e-05 - val_mae: 0.0040 - learning_rate: 3.1250e-05\n",
      "Epoch 56/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4310e-05 - mae: 0.0041 - val_loss: 4.0505e-05 - val_mae: 0.0040 - learning_rate: 3.1250e-05\n",
      "Epoch 57/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5678e-05 - mae: 0.0042 - val_loss: 4.0523e-05 - val_mae: 0.0040 - learning_rate: 3.1250e-05\n",
      "Epoch 58/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7396e-05 - mae: 0.0043 - val_loss: 4.0529e-05 - val_mae: 0.0040 - learning_rate: 3.1250e-05\n",
      "Epoch 59/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.7779e-05 - mae: 0.0043 - val_loss: 4.0529e-05 - val_mae: 0.0040 - learning_rate: 3.1250e-05\n",
      "Epoch 60/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2727e-05 - mae: 0.0041 - val_loss: 4.0520e-05 - val_mae: 0.0040 - learning_rate: 3.1250e-05\n",
      "Epoch 61/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9110e-05 - mae: 0.0043\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8854e-05 - mae: 0.0043 - val_loss: 4.0536e-05 - val_mae: 0.0040 - learning_rate: 3.1250e-05\n",
      "Epoch 62/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5626e-05 - mae: 0.0042 - val_loss: 4.0544e-05 - val_mae: 0.0040 - learning_rate: 1.5625e-05\n",
      "Epoch 63/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5699e-05 - mae: 0.0042 - val_loss: 4.0548e-05 - val_mae: 0.0040 - learning_rate: 1.5625e-05\n",
      "Epoch 64/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4898e-05 - mae: 0.0042 - val_loss: 4.0551e-05 - val_mae: 0.0040 - learning_rate: 1.5625e-05\n",
      "Epoch 65/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5890e-05 - mae: 0.0042 - val_loss: 4.0557e-05 - val_mae: 0.0040 - learning_rate: 1.5625e-05\n",
      "Epoch 66/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6425e-05 - mae: 0.0043 - val_loss: 4.0560e-05 - val_mae: 0.0040 - learning_rate: 1.5625e-05\n",
      "Epoch 67/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5914e-05 - mae: 0.0042 - val_loss: 4.0553e-05 - val_mae: 0.0040 - learning_rate: 1.5625e-05\n",
      "Epoch 68/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5388e-05 - mae: 0.0042 - val_loss: 4.0545e-05 - val_mae: 0.0040 - learning_rate: 1.5625e-05\n",
      "Epoch 69/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7606e-05 - mae: 0.0043 - val_loss: 4.0542e-05 - val_mae: 0.0040 - learning_rate: 1.5625e-05\n",
      "Epoch 70/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.2338e-05 - mae: 0.0041 - val_loss: 4.0544e-05 - val_mae: 0.0040 - learning_rate: 1.5625e-05\n",
      "Epoch 71/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1999e-05 - mae: 0.0041\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2437e-05 - mae: 0.0041 - val_loss: 4.0550e-05 - val_mae: 0.0040 - learning_rate: 1.5625e-05\n",
      "Epoch 72/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4553e-05 - mae: 0.0042 - val_loss: 4.0549e-05 - val_mae: 0.0040 - learning_rate: 7.8125e-06\n",
      "Epoch 73/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7407e-05 - mae: 0.0043 - val_loss: 4.0550e-05 - val_mae: 0.0040 - learning_rate: 7.8125e-06\n",
      "Epoch 74/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5030e-05 - mae: 0.0042 - val_loss: 4.0545e-05 - val_mae: 0.0040 - learning_rate: 7.8125e-06\n",
      "Epoch 75/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5326e-05 - mae: 0.0042 - val_loss: 4.0538e-05 - val_mae: 0.0040 - learning_rate: 7.8125e-06\n",
      "Epoch 76/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5949e-05 - mae: 0.0043 - val_loss: 4.0536e-05 - val_mae: 0.0040 - learning_rate: 7.8125e-06\n",
      "Epoch 77/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8037e-05 - mae: 0.0043 - val_loss: 4.0537e-05 - val_mae: 0.0040 - learning_rate: 7.8125e-06\n",
      "Epoch 78/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.5556e-05 - mae: 0.0042 - val_loss: 4.0535e-05 - val_mae: 0.0040 - learning_rate: 7.8125e-06\n",
      "Epoch 79/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6874e-05 - mae: 0.0042 - val_loss: 4.0532e-05 - val_mae: 0.0040 - learning_rate: 7.8125e-06\n",
      "Epoch 80/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7886e-05 - mae: 0.0043 - val_loss: 4.0525e-05 - val_mae: 0.0040 - learning_rate: 7.8125e-06\n",
      "Epoch 81/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5426e-05 - mae: 0.0042\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5507e-05 - mae: 0.0042 - val_loss: 4.0520e-05 - val_mae: 0.0040 - learning_rate: 7.8125e-06\n",
      "Epoch 82/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8187e-05 - mae: 0.0043 - val_loss: 4.0519e-05 - val_mae: 0.0040 - learning_rate: 3.9063e-06\n",
      "Epoch 83/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7802e-05 - mae: 0.0043 - val_loss: 4.0519e-05 - val_mae: 0.0040 - learning_rate: 3.9063e-06\n",
      "Epoch 84/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5876e-05 - mae: 0.0042 - val_loss: 4.0519e-05 - val_mae: 0.0040 - learning_rate: 3.9063e-06\n",
      "Epoch 85/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7457e-05 - mae: 0.0043 - val_loss: 4.0518e-05 - val_mae: 0.0040 - learning_rate: 3.9063e-06\n",
      "Epoch 86/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6706e-05 - mae: 0.0043 - val_loss: 4.0515e-05 - val_mae: 0.0040 - learning_rate: 3.9063e-06\n",
      "Epoch 87/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9738e-05 - mae: 0.0044 - val_loss: 4.0517e-05 - val_mae: 0.0040 - learning_rate: 3.9063e-06\n",
      "Epoch 88/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7262e-05 - mae: 0.0043 - val_loss: 4.0517e-05 - val_mae: 0.0040 - learning_rate: 3.9063e-06\n",
      "Epoch 89/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6528e-05 - mae: 0.0043 - val_loss: 4.0516e-05 - val_mae: 0.0040 - learning_rate: 3.9063e-06\n",
      "Epoch 90/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6016e-05 - mae: 0.0042 - val_loss: 4.0516e-05 - val_mae: 0.0040 - learning_rate: 3.9063e-06\n",
      "Epoch 91/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6958e-05 - mae: 0.0043\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6899e-05 - mae: 0.0043 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 3.9063e-06\n",
      "Epoch 92/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9660e-05 - mae: 0.0044 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 1.9531e-06\n",
      "Epoch 93/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3972e-05 - mae: 0.0041 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 1.9531e-06\n",
      "Epoch 94/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6081e-05 - mae: 0.0042 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 1.9531e-06\n",
      "Epoch 95/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4745e-05 - mae: 0.0042 - val_loss: 4.0515e-05 - val_mae: 0.0040 - learning_rate: 1.9531e-06\n",
      "Epoch 96/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6948e-05 - mae: 0.0043 - val_loss: 4.0517e-05 - val_mae: 0.0040 - learning_rate: 1.9531e-06\n",
      "Epoch 97/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3375e-05 - mae: 0.0042 - val_loss: 4.0516e-05 - val_mae: 0.0040 - learning_rate: 1.9531e-06\n",
      "Epoch 98/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6161e-05 - mae: 0.0042 - val_loss: 4.0515e-05 - val_mae: 0.0040 - learning_rate: 1.9531e-06\n",
      "Epoch 99/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5721e-05 - mae: 0.0042 - val_loss: 4.0516e-05 - val_mae: 0.0040 - learning_rate: 1.9531e-06\n",
      "Epoch 100/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5359e-05 - mae: 0.0042 - val_loss: 4.0515e-05 - val_mae: 0.0040 - learning_rate: 1.9531e-06\n",
      "Epoch 101/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6976e-05 - mae: 0.0043\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6910e-05 - mae: 0.0043 - val_loss: 4.0516e-05 - val_mae: 0.0040 - learning_rate: 1.9531e-06\n",
      "Epoch 102/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.2510e-05 - mae: 0.0041 - val_loss: 4.0515e-05 - val_mae: 0.0040 - learning_rate: 9.7656e-07\n",
      "Epoch 103/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.5355e-05 - mae: 0.0042 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 9.7656e-07\n",
      "Epoch 104/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2878e-05 - mae: 0.0041 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 9.7656e-07\n",
      "Epoch 105/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9343e-05 - mae: 0.0043 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 9.7656e-07\n",
      "Epoch 106/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8986e-05 - mae: 0.0043 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 9.7656e-07\n",
      "Epoch 107/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4976e-05 - mae: 0.0042 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 9.7656e-07\n",
      "Epoch 108/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8609e-05 - mae: 0.0043 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 9.7656e-07\n",
      "Epoch 109/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6241e-05 - mae: 0.0042 - val_loss: 4.0515e-05 - val_mae: 0.0040 - learning_rate: 9.7656e-07\n",
      "Epoch 110/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.7757e-05 - mae: 0.0043 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 9.7656e-07\n",
      "Epoch 111/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7557e-05 - mae: 0.0043\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7442e-05 - mae: 0.0043 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 9.7656e-07\n",
      "Epoch 112/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7796e-05 - mae: 0.0043 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 4.8828e-07\n",
      "Epoch 113/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5708e-05 - mae: 0.0042 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 4.8828e-07\n",
      "Epoch 114/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7969e-05 - mae: 0.0043 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 4.8828e-07\n",
      "Epoch 115/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6161e-05 - mae: 0.0043 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 4.8828e-07\n",
      "Epoch 116/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5942e-05 - mae: 0.0042 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 4.8828e-07\n",
      "Epoch 117/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3693e-05 - mae: 0.0041 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 4.8828e-07\n",
      "Epoch 118/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9340e-05 - mae: 0.0044 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 4.8828e-07\n",
      "Epoch 119/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6319e-05 - mae: 0.0042 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 4.8828e-07\n",
      "Epoch 120/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6667e-05 - mae: 0.0043 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 4.8828e-07\n",
      "Epoch 121/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4466e-05 - mae: 0.0042\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4665e-05 - mae: 0.0042 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 4.8828e-07\n",
      "Epoch 122/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8102e-05 - mae: 0.0043 - val_loss: 4.0514e-05 - val_mae: 0.0040 - learning_rate: 2.4414e-07\n",
      "Epoch 123/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.8060e-05 - mae: 0.0043 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 2.4414e-07\n",
      "Epoch 124/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6836e-05 - mae: 0.0042 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 2.4414e-07\n",
      "Epoch 125/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7999e-05 - mae: 0.0043 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 2.4414e-07\n",
      "Epoch 126/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5889e-05 - mae: 0.0042 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 2.4414e-07\n",
      "Epoch 127/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4645e-05 - mae: 0.0042 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 2.4414e-07\n",
      "Epoch 128/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5028e-05 - mae: 0.0042 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 2.4414e-07\n",
      "Epoch 129/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2612e-05 - mae: 0.0041 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 2.4414e-07\n",
      "Epoch 130/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6462e-05 - mae: 0.0042 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 2.4414e-07\n",
      "Epoch 131/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4731e-05 - mae: 0.0042\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4905e-05 - mae: 0.0042 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 2.4414e-07\n",
      "Epoch 132/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8179e-05 - mae: 0.0043 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 1.2207e-07\n",
      "Epoch 133/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9041e-05 - mae: 0.0044 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 1.2207e-07\n",
      "Epoch 134/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5217e-05 - mae: 0.0042 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 1.2207e-07\n",
      "Epoch 135/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.5111e-05 - mae: 0.0042 - val_loss: 4.0513e-05 - val_mae: 0.0040 - learning_rate: 1.2207e-07\n",
      "Epoch 136/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8140e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2207e-07\n",
      "Epoch 137/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6950e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2207e-07\n",
      "Epoch 138/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.7607e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2207e-07\n",
      "Epoch 139/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8608e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2207e-07\n",
      "Epoch 140/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.2182e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2207e-07\n",
      "Epoch 141/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5860e-05 - mae: 0.0042\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5903e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2207e-07\n",
      "Epoch 142/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8277e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.1035e-08\n",
      "Epoch 143/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9249e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.1035e-08\n",
      "Epoch 144/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.3768e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.1035e-08\n",
      "Epoch 145/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5186e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.1035e-08\n",
      "Epoch 146/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6472e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.1035e-08\n",
      "Epoch 147/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8344e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.1035e-08\n",
      "Epoch 148/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3408e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.1035e-08\n",
      "Epoch 149/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4373e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.1035e-08\n",
      "Epoch 150/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6701e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.1035e-08\n",
      "Epoch 151/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5363e-05 - mae: 0.0042\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5463e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.1035e-08\n",
      "Epoch 152/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7421e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.0518e-08\n",
      "Epoch 153/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6780e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.0518e-08\n",
      "Epoch 154/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5630e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.0518e-08\n",
      "Epoch 155/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5064e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.0518e-08\n",
      "Epoch 156/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6350e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.0518e-08\n",
      "Epoch 157/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.5301e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.0518e-08\n",
      "Epoch 158/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6827e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.0518e-08\n",
      "Epoch 159/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6332e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.0518e-08\n",
      "Epoch 160/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.8257e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.0518e-08\n",
      "Epoch 161/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6910e-05 - mae: 0.0042\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6855e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.0518e-08\n",
      "Epoch 162/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5967e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5259e-08\n",
      "Epoch 163/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.9282e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5259e-08\n",
      "Epoch 164/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7564e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5259e-08\n",
      "Epoch 165/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5397e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5259e-08\n",
      "Epoch 166/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6192e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5259e-08\n",
      "Epoch 167/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.5716e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5259e-08\n",
      "Epoch 168/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5913e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5259e-08\n",
      "Epoch 169/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7833e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5259e-08\n",
      "Epoch 170/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6286e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5259e-08\n",
      "Epoch 171/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6488e-05 - mae: 0.0042\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6473e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5259e-08\n",
      "Epoch 172/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.4530e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.6294e-09\n",
      "Epoch 173/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.3977e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.6294e-09\n",
      "Epoch 174/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5678e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.6294e-09\n",
      "Epoch 175/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9680e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.6294e-09\n",
      "Epoch 176/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6009e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.6294e-09\n",
      "Epoch 177/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0419e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.6294e-09\n",
      "Epoch 178/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9087e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.6294e-09\n",
      "Epoch 179/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4704e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.6294e-09\n",
      "Epoch 180/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3552e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.6294e-09\n",
      "Epoch 181/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9219e-05 - mae: 0.0044\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8925e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.6294e-09\n",
      "Epoch 182/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6204e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.8147e-09\n",
      "Epoch 183/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4001e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.8147e-09\n",
      "Epoch 184/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.3785e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.8147e-09\n",
      "Epoch 185/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5824e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.8147e-09\n",
      "Epoch 186/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6248e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.8147e-09\n",
      "Epoch 187/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4775e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.8147e-09\n",
      "Epoch 188/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.4814e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.8147e-09\n",
      "Epoch 189/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.4909e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.8147e-09\n",
      "Epoch 190/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7951e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.8147e-09\n",
      "Epoch 191/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7873e-05 - mae: 0.0043\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7719e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.8147e-09\n",
      "Epoch 192/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7048e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.9073e-09\n",
      "Epoch 193/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5587e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.9073e-09\n",
      "Epoch 194/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6826e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.9073e-09\n",
      "Epoch 195/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3547e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.9073e-09\n",
      "Epoch 196/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.5792e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.9073e-09\n",
      "Epoch 197/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5633e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.9073e-09\n",
      "Epoch 198/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6566e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.9073e-09\n",
      "Epoch 199/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3795e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.9073e-09\n",
      "Epoch 200/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5221e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.9073e-09\n",
      "Epoch 201/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6723e-05 - mae: 0.0043\n",
      "Epoch 201: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6668e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.9073e-09\n",
      "Epoch 202/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6695e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.5367e-10\n",
      "Epoch 203/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6983e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.5367e-10\n",
      "Epoch 204/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5127e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.5367e-10\n",
      "Epoch 205/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.8712e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.5367e-10\n",
      "Epoch 206/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7517e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.5367e-10\n",
      "Epoch 207/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2104e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.5367e-10\n",
      "Epoch 208/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4198e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.5367e-10\n",
      "Epoch 209/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8563e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.5367e-10\n",
      "Epoch 210/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5419e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.5367e-10\n",
      "Epoch 211/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4427e-05 - mae: 0.0042\n",
      "Epoch 211: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4628e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.5367e-10\n",
      "Epoch 212/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3266e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.7684e-10\n",
      "Epoch 213/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.7576e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.7684e-10\n",
      "Epoch 214/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7086e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.7684e-10\n",
      "Epoch 215/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5662e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.7684e-10\n",
      "Epoch 216/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3312e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.7684e-10\n",
      "Epoch 217/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5266e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.7684e-10\n",
      "Epoch 218/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9160e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.7684e-10\n",
      "Epoch 219/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8705e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.7684e-10\n",
      "Epoch 220/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6444e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.7684e-10\n",
      "Epoch 221/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7518e-05 - mae: 0.0042\n",
      "Epoch 221: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7398e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.7684e-10\n",
      "Epoch 222/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.4141e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3842e-10\n",
      "Epoch 223/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9233e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3842e-10\n",
      "Epoch 224/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7017e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3842e-10\n",
      "Epoch 225/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2969e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3842e-10\n",
      "Epoch 226/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6272e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3842e-10\n",
      "Epoch 227/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5173e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3842e-10\n",
      "Epoch 228/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2090e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3842e-10\n",
      "Epoch 229/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3101e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3842e-10\n",
      "Epoch 230/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6285e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3842e-10\n",
      "Epoch 231/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9705e-05 - mae: 0.0044\n",
      "Epoch 231: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9376e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3842e-10\n",
      "Epoch 232/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5974e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1921e-10\n",
      "Epoch 233/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7407e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1921e-10\n",
      "Epoch 234/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6746e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1921e-10\n",
      "Epoch 235/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9811e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1921e-10\n",
      "Epoch 236/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5712e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1921e-10\n",
      "Epoch 237/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6281e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1921e-10\n",
      "Epoch 238/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8446e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1921e-10\n",
      "Epoch 239/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4915e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1921e-10\n",
      "Epoch 240/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9266e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1921e-10\n",
      "Epoch 241/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5119e-05 - mae: 0.0042\n",
      "Epoch 241: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5227e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1921e-10\n",
      "Epoch 242/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8842e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.9605e-11\n",
      "Epoch 243/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7884e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.9605e-11\n",
      "Epoch 244/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7005e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.9605e-11\n",
      "Epoch 245/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7785e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.9605e-11\n",
      "Epoch 246/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.8612e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.9605e-11\n",
      "Epoch 247/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6485e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.9605e-11\n",
      "Epoch 248/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8531e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.9605e-11\n",
      "Epoch 249/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4693e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.9605e-11\n",
      "Epoch 250/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4434e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.9605e-11\n",
      "Epoch 251/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6153e-05 - mae: 0.0042\n",
      "Epoch 251: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6174e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.9605e-11\n",
      "Epoch 252/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.4002e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9802e-11\n",
      "Epoch 253/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6614e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9802e-11\n",
      "Epoch 254/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5687e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9802e-11\n",
      "Epoch 255/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5568e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9802e-11\n",
      "Epoch 256/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8342e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9802e-11\n",
      "Epoch 257/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8063e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9802e-11\n",
      "Epoch 258/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6576e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9802e-11\n",
      "Epoch 259/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7445e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9802e-11\n",
      "Epoch 260/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7121e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9802e-11\n",
      "Epoch 261/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9099e-05 - mae: 0.0044\n",
      "Epoch 261: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8835e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9802e-11\n",
      "Epoch 262/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5255e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4901e-11\n",
      "Epoch 263/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3718e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4901e-11\n",
      "Epoch 264/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5448e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4901e-11\n",
      "Epoch 265/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5990e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4901e-11\n",
      "Epoch 266/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8922e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4901e-11\n",
      "Epoch 267/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4711e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4901e-11\n",
      "Epoch 268/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4588e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4901e-11\n",
      "Epoch 269/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7935e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4901e-11\n",
      "Epoch 270/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6102e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4901e-11\n",
      "Epoch 271/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.3202e-05 - mae: 0.0041\n",
      "Epoch 271: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3515e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4901e-11\n",
      "Epoch 272/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7728e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.4506e-12\n",
      "Epoch 273/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5449e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.4506e-12\n",
      "Epoch 274/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6544e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.4506e-12\n",
      "Epoch 275/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2375e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.4506e-12\n",
      "Epoch 276/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7186e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.4506e-12\n",
      "Epoch 277/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9375e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.4506e-12\n",
      "Epoch 278/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6453e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.4506e-12\n",
      "Epoch 279/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4960e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.4506e-12\n",
      "Epoch 280/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5879e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.4506e-12\n",
      "Epoch 281/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5460e-05 - mae: 0.0042\n",
      "Epoch 281: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5538e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.4506e-12\n",
      "Epoch 282/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4523e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.7253e-12\n",
      "Epoch 283/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6293e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.7253e-12\n",
      "Epoch 284/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7229e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.7253e-12\n",
      "Epoch 285/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.8908e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.7253e-12\n",
      "Epoch 286/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6321e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.7253e-12\n",
      "Epoch 287/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5857e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.7253e-12\n",
      "Epoch 288/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2906e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.7253e-12\n",
      "Epoch 289/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5459e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.7253e-12\n",
      "Epoch 290/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7227e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.7253e-12\n",
      "Epoch 291/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.2335e-05 - mae: 0.0045\n",
      "Epoch 291: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.1731e-05 - mae: 0.0045 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.7253e-12\n",
      "Epoch 292/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5101e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8626e-12\n",
      "Epoch 293/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8255e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8626e-12\n",
      "Epoch 294/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6590e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8626e-12\n",
      "Epoch 295/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.9049e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8626e-12\n",
      "Epoch 296/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5987e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8626e-12\n",
      "Epoch 297/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5875e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8626e-12\n",
      "Epoch 298/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8413e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8626e-12\n",
      "Epoch 299/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4731e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8626e-12\n",
      "Epoch 300/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8474e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8626e-12\n",
      "Epoch 301/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9754e-05 - mae: 0.0044\n",
      "Epoch 301: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9407e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8626e-12\n",
      "Epoch 302/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3310e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.3132e-13\n",
      "Epoch 303/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7667e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.3132e-13\n",
      "Epoch 304/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6017e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.3132e-13\n",
      "Epoch 305/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7629e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.3132e-13\n",
      "Epoch 306/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5150e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.3132e-13\n",
      "Epoch 307/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5381e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.3132e-13\n",
      "Epoch 308/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6274e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.3132e-13\n",
      "Epoch 309/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9082e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.3132e-13\n",
      "Epoch 310/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6852e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.3132e-13\n",
      "Epoch 311/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6350e-05 - mae: 0.0042\n",
      "Epoch 311: ReduceLROnPlateau reducing learning rate to 4.656613094254636e-13.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6365e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.3132e-13\n",
      "Epoch 312/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7735e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.6566e-13\n",
      "Epoch 313/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6423e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.6566e-13\n",
      "Epoch 314/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7606e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.6566e-13\n",
      "Epoch 315/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4540e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.6566e-13\n",
      "Epoch 316/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4545e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.6566e-13\n",
      "Epoch 317/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7192e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.6566e-13\n",
      "Epoch 318/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9595e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.6566e-13\n",
      "Epoch 319/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7020e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.6566e-13\n",
      "Epoch 320/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4761e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.6566e-13\n",
      "Epoch 321/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4214e-05 - mae: 0.0042\n",
      "Epoch 321: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4428e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.6566e-13\n",
      "Epoch 322/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9409e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3283e-13\n",
      "Epoch 323/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3679e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3283e-13\n",
      "Epoch 324/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5194e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3283e-13\n",
      "Epoch 325/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7069e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3283e-13\n",
      "Epoch 326/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5285e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3283e-13\n",
      "Epoch 327/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4873e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3283e-13\n",
      "Epoch 328/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8427e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3283e-13\n",
      "Epoch 329/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8798e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3283e-13\n",
      "Epoch 330/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7350e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3283e-13\n",
      "Epoch 331/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4479e-05 - mae: 0.0042\n",
      "Epoch 331: ReduceLROnPlateau reducing learning rate to 1.164153273563659e-13.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4669e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.3283e-13\n",
      "Epoch 332/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.9120e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1642e-13\n",
      "Epoch 333/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7180e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1642e-13\n",
      "Epoch 334/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5843e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1642e-13\n",
      "Epoch 335/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8315e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1642e-13\n",
      "Epoch 336/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5955e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1642e-13\n",
      "Epoch 337/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6364e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1642e-13\n",
      "Epoch 338/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7612e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1642e-13\n",
      "Epoch 339/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5106e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1642e-13\n",
      "Epoch 340/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8445e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1642e-13\n",
      "Epoch 341/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5369e-05 - mae: 0.0042\n",
      "Epoch 341: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5478e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1642e-13\n",
      "Epoch 342/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6599e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.8208e-14\n",
      "Epoch 343/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7188e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.8208e-14\n",
      "Epoch 344/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5357e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.8208e-14\n",
      "Epoch 345/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5805e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.8208e-14\n",
      "Epoch 346/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2445e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.8208e-14\n",
      "Epoch 347/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5863e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.8208e-14\n",
      "Epoch 348/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3896e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.8208e-14\n",
      "Epoch 349/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4293e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.8208e-14\n",
      "Epoch 350/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6286e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.8208e-14\n",
      "Epoch 351/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8162e-05 - mae: 0.0043\n",
      "Epoch 351: ReduceLROnPlateau reducing learning rate to 2.9103831839091474e-14.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7985e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.8208e-14\n",
      "Epoch 352/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7072e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9104e-14\n",
      "Epoch 353/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8632e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9104e-14\n",
      "Epoch 354/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5402e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9104e-14\n",
      "Epoch 355/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4680e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9104e-14\n",
      "Epoch 356/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4935e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9104e-14\n",
      "Epoch 357/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5219e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9104e-14\n",
      "Epoch 358/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4401e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9104e-14\n",
      "Epoch 359/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3864e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9104e-14\n",
      "Epoch 360/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0078e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9104e-14\n",
      "Epoch 361/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4110e-05 - mae: 0.0042\n",
      "Epoch 361: ReduceLROnPlateau reducing learning rate to 1.4551915919545737e-14.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4337e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.9104e-14\n",
      "Epoch 362/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7857e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4552e-14\n",
      "Epoch 363/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4396e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4552e-14\n",
      "Epoch 364/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3694e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4552e-14\n",
      "Epoch 365/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7525e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4552e-14\n",
      "Epoch 366/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4689e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4552e-14\n",
      "Epoch 367/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6793e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4552e-14\n",
      "Epoch 368/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6730e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4552e-14\n",
      "Epoch 369/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7366e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4552e-14\n",
      "Epoch 370/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7634e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4552e-14\n",
      "Epoch 371/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7389e-05 - mae: 0.0043\n",
      "Epoch 371: ReduceLROnPlateau reducing learning rate to 7.275957959772868e-15.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7272e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4552e-14\n",
      "Epoch 372/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7606e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.2760e-15\n",
      "Epoch 373/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8283e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.2760e-15\n",
      "Epoch 374/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7336e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.2760e-15\n",
      "Epoch 375/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5447e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.2760e-15\n",
      "Epoch 376/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7508e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.2760e-15\n",
      "Epoch 377/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5949e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.2760e-15\n",
      "Epoch 378/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3989e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.2760e-15\n",
      "Epoch 379/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4368e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.2760e-15\n",
      "Epoch 380/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4518e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.2760e-15\n",
      "Epoch 381/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4976e-05 - mae: 0.0042\n",
      "Epoch 381: ReduceLROnPlateau reducing learning rate to 3.637978979886434e-15.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5114e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.2760e-15\n",
      "Epoch 382/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4838e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.6380e-15\n",
      "Epoch 383/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4649e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.6380e-15\n",
      "Epoch 384/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6286e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.6380e-15\n",
      "Epoch 385/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6802e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.6380e-15\n",
      "Epoch 386/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4404e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.6380e-15\n",
      "Epoch 387/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4129e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.6380e-15\n",
      "Epoch 388/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4308e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.6380e-15\n",
      "Epoch 389/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6938e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.6380e-15\n",
      "Epoch 390/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7342e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.6380e-15\n",
      "Epoch 391/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6510e-05 - mae: 0.0042\n",
      "Epoch 391: ReduceLROnPlateau reducing learning rate to 1.818989489943217e-15.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6484e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.6380e-15\n",
      "Epoch 392/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.3442e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8190e-15\n",
      "Epoch 393/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6782e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8190e-15\n",
      "Epoch 394/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6460e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8190e-15\n",
      "Epoch 395/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.0604e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8190e-15\n",
      "Epoch 396/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6390e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8190e-15\n",
      "Epoch 397/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7392e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8190e-15\n",
      "Epoch 398/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5786e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8190e-15\n",
      "Epoch 399/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4702e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8190e-15\n",
      "Epoch 400/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5741e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8190e-15\n",
      "Epoch 401/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5122e-05 - mae: 0.0042\n",
      "Epoch 401: ReduceLROnPlateau reducing learning rate to 9.094947449716085e-16.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5243e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.8190e-15\n",
      "Epoch 402/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5144e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.0949e-16\n",
      "Epoch 403/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4745e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.0949e-16\n",
      "Epoch 404/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5477e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.0949e-16\n",
      "Epoch 405/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6162e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.0949e-16\n",
      "Epoch 406/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6750e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.0949e-16\n",
      "Epoch 407/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8558e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.0949e-16\n",
      "Epoch 408/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7358e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.0949e-16\n",
      "Epoch 409/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7764e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.0949e-16\n",
      "Epoch 410/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3704e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.0949e-16\n",
      "Epoch 411/1000\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4623e-05 - mae: 0.0042\n",
      "Epoch 411: ReduceLROnPlateau reducing learning rate to 4.547473724858043e-16.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.5190e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 9.0949e-16\n",
      "Epoch 412/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5021e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.5475e-16\n",
      "Epoch 413/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8147e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.5475e-16\n",
      "Epoch 414/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5762e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.5475e-16\n",
      "Epoch 415/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5604e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.5475e-16\n",
      "Epoch 416/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5008e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.5475e-16\n",
      "Epoch 417/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9117e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.5475e-16\n",
      "Epoch 418/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9873e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.5475e-16\n",
      "Epoch 419/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5408e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.5475e-16\n",
      "Epoch 420/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7508e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.5475e-16\n",
      "Epoch 421/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6199e-05 - mae: 0.0042\n",
      "Epoch 421: ReduceLROnPlateau reducing learning rate to 2.2737368624290214e-16.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6218e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.5475e-16\n",
      "Epoch 422/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.1004e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2737e-16\n",
      "Epoch 423/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8425e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2737e-16\n",
      "Epoch 424/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4303e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2737e-16\n",
      "Epoch 425/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5595e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2737e-16\n",
      "Epoch 426/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3850e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2737e-16\n",
      "Epoch 427/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5354e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2737e-16\n",
      "Epoch 428/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5320e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2737e-16\n",
      "Epoch 429/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4537e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2737e-16\n",
      "Epoch 430/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5417e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2737e-16\n",
      "Epoch 431/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.3062e-05 - mae: 0.0041\n",
      "Epoch 431: ReduceLROnPlateau reducing learning rate to 1.1368684312145107e-16.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3396e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2737e-16\n",
      "Epoch 432/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7077e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1369e-16\n",
      "Epoch 433/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5283e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1369e-16\n",
      "Epoch 434/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6078e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1369e-16\n",
      "Epoch 435/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6140e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1369e-16\n",
      "Epoch 436/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9387e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1369e-16\n",
      "Epoch 437/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8126e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1369e-16\n",
      "Epoch 438/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6203e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1369e-16\n",
      "Epoch 439/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8486e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1369e-16\n",
      "Epoch 440/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4190e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1369e-16\n",
      "Epoch 441/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6671e-05 - mae: 0.0043\n",
      "Epoch 441: ReduceLROnPlateau reducing learning rate to 5.684342156072553e-17.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6639e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1369e-16\n",
      "Epoch 442/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7213e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.6843e-17\n",
      "Epoch 443/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5883e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.6843e-17\n",
      "Epoch 444/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5115e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.6843e-17\n",
      "Epoch 445/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7429e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.6843e-17\n",
      "Epoch 446/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.8490e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.6843e-17\n",
      "Epoch 447/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9406e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.6843e-17\n",
      "Epoch 448/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5470e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.6843e-17\n",
      "Epoch 449/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7646e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.6843e-17\n",
      "Epoch 450/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4190e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.6843e-17\n",
      "Epoch 451/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9556e-05 - mae: 0.0043\n",
      "Epoch 451: ReduceLROnPlateau reducing learning rate to 2.842171078036277e-17.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9226e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.6843e-17\n",
      "Epoch 452/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6595e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.8422e-17\n",
      "Epoch 453/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6631e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.8422e-17\n",
      "Epoch 454/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7499e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.8422e-17\n",
      "Epoch 455/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4550e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.8422e-17\n",
      "Epoch 456/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7707e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.8422e-17\n",
      "Epoch 457/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4966e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.8422e-17\n",
      "Epoch 458/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3390e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.8422e-17\n",
      "Epoch 459/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5255e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.8422e-17\n",
      "Epoch 460/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7121e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.8422e-17\n",
      "Epoch 461/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6804e-05 - mae: 0.0043\n",
      "Epoch 461: ReduceLROnPlateau reducing learning rate to 1.4210855390181384e-17.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6741e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.8422e-17\n",
      "Epoch 462/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3527e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4211e-17\n",
      "Epoch 463/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8131e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4211e-17\n",
      "Epoch 464/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.8302e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4211e-17\n",
      "Epoch 465/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9128e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4211e-17\n",
      "Epoch 466/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3802e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4211e-17\n",
      "Epoch 467/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3734e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4211e-17\n",
      "Epoch 468/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7212e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4211e-17\n",
      "Epoch 469/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3730e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4211e-17\n",
      "Epoch 470/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4605e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4211e-17\n",
      "Epoch 471/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4348e-05 - mae: 0.0041\n",
      "Epoch 471: ReduceLROnPlateau reducing learning rate to 7.105427695090692e-18.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4539e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.4211e-17\n",
      "Epoch 472/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7979e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.1054e-18\n",
      "Epoch 473/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7482e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.1054e-18\n",
      "Epoch 474/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6523e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.1054e-18\n",
      "Epoch 475/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8134e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.1054e-18\n",
      "Epoch 476/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2550e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.1054e-18\n",
      "Epoch 477/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7880e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.1054e-18\n",
      "Epoch 478/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8082e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.1054e-18\n",
      "Epoch 479/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4956e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.1054e-18\n",
      "Epoch 480/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.1276e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.1054e-18\n",
      "Epoch 481/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7785e-05 - mae: 0.0043\n",
      "Epoch 481: ReduceLROnPlateau reducing learning rate to 3.552713847545346e-18.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7639e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 7.1054e-18\n",
      "Epoch 482/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5606e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.5527e-18\n",
      "Epoch 483/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8627e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.5527e-18\n",
      "Epoch 484/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6986e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.5527e-18\n",
      "Epoch 485/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7220e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.5527e-18\n",
      "Epoch 486/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4471e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.5527e-18\n",
      "Epoch 487/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8610e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.5527e-18\n",
      "Epoch 488/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4715e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.5527e-18\n",
      "Epoch 489/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6800e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.5527e-18\n",
      "Epoch 490/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7593e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.5527e-18\n",
      "Epoch 491/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6190e-05 - mae: 0.0042\n",
      "Epoch 491: ReduceLROnPlateau reducing learning rate to 1.776356923772673e-18.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6196e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.5527e-18\n",
      "Epoch 492/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8558e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7764e-18\n",
      "Epoch 493/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6098e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7764e-18\n",
      "Epoch 494/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7108e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7764e-18\n",
      "Epoch 495/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6972e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7764e-18\n",
      "Epoch 496/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7841e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7764e-18\n",
      "Epoch 497/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4518e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7764e-18\n",
      "Epoch 498/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6559e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7764e-18\n",
      "Epoch 499/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7995e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7764e-18\n",
      "Epoch 500/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9640e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7764e-18\n",
      "Epoch 501/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7508e-05 - mae: 0.0043\n",
      "Epoch 501: ReduceLROnPlateau reducing learning rate to 8.881784618863365e-19.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7396e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7764e-18\n",
      "Epoch 502/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6280e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.8818e-19\n",
      "Epoch 503/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5512e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.8818e-19\n",
      "Epoch 504/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7256e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.8818e-19\n",
      "Epoch 505/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4397e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.8818e-19\n",
      "Epoch 506/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6792e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.8818e-19\n",
      "Epoch 507/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4624e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.8818e-19\n",
      "Epoch 508/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5153e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.8818e-19\n",
      "Epoch 509/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5749e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.8818e-19\n",
      "Epoch 510/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0083e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.8818e-19\n",
      "Epoch 511/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9153e-05 - mae: 0.0043\n",
      "Epoch 511: ReduceLROnPlateau reducing learning rate to 4.440892309431682e-19.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8858e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.8818e-19\n",
      "Epoch 512/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5510e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.4409e-19\n",
      "Epoch 513/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8455e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.4409e-19\n",
      "Epoch 514/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6584e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.4409e-19\n",
      "Epoch 515/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6429e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.4409e-19\n",
      "Epoch 516/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8723e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.4409e-19\n",
      "Epoch 517/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4312e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.4409e-19\n",
      "Epoch 518/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6631e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.4409e-19\n",
      "Epoch 519/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3736e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.4409e-19\n",
      "Epoch 520/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0196e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.4409e-19\n",
      "Epoch 521/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.4541e-05 - mae: 0.0042\n",
      "Epoch 521: ReduceLROnPlateau reducing learning rate to 2.220446154715841e-19.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4724e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.4409e-19\n",
      "Epoch 522/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.2595e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2204e-19\n",
      "Epoch 523/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5566e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2204e-19\n",
      "Epoch 524/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6681e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2204e-19\n",
      "Epoch 525/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.2015e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2204e-19\n",
      "Epoch 526/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4742e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2204e-19\n",
      "Epoch 527/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5230e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2204e-19\n",
      "Epoch 528/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7587e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2204e-19\n",
      "Epoch 529/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5142e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2204e-19\n",
      "Epoch 530/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0632e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2204e-19\n",
      "Epoch 531/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5845e-05 - mae: 0.0042\n",
      "Epoch 531: ReduceLROnPlateau reducing learning rate to 1.1102230773579206e-19.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5890e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.2204e-19\n",
      "Epoch 532/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4848e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1102e-19\n",
      "Epoch 533/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.9468e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1102e-19\n",
      "Epoch 534/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4322e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1102e-19\n",
      "Epoch 535/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.9070e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1102e-19\n",
      "Epoch 536/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3999e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1102e-19\n",
      "Epoch 537/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5046e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1102e-19\n",
      "Epoch 538/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6572e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1102e-19\n",
      "Epoch 539/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4856e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1102e-19\n",
      "Epoch 540/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7805e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1102e-19\n",
      "Epoch 541/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4689e-05 - mae: 0.0042\n",
      "Epoch 541: ReduceLROnPlateau reducing learning rate to 5.551115386789603e-20.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4836e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.1102e-19\n",
      "Epoch 542/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5169e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.5511e-20\n",
      "Epoch 543/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5769e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.5511e-20\n",
      "Epoch 544/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8598e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.5511e-20\n",
      "Epoch 545/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0406e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.5511e-20\n",
      "Epoch 546/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7351e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.5511e-20\n",
      "Epoch 547/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7024e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.5511e-20\n",
      "Epoch 548/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5283e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.5511e-20\n",
      "Epoch 549/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6366e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.5511e-20\n",
      "Epoch 550/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7366e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.5511e-20\n",
      "Epoch 551/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7167e-05 - mae: 0.0043\n",
      "Epoch 551: ReduceLROnPlateau reducing learning rate to 2.7755576933948015e-20.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7071e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.5511e-20\n",
      "Epoch 552/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4543e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7756e-20\n",
      "Epoch 553/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6250e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7756e-20\n",
      "Epoch 554/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.2154e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7756e-20\n",
      "Epoch 555/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2290e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7756e-20\n",
      "Epoch 556/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2433e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7756e-20\n",
      "Epoch 557/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5774e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7756e-20\n",
      "Epoch 558/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5137e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7756e-20\n",
      "Epoch 559/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6196e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7756e-20\n",
      "Epoch 560/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0741e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7756e-20\n",
      "Epoch 561/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4004e-05 - mae: 0.0042\n",
      "Epoch 561: ReduceLROnPlateau reducing learning rate to 1.3877788466974007e-20.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4245e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7756e-20\n",
      "Epoch 562/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7315e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3878e-20\n",
      "Epoch 563/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8433e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3878e-20\n",
      "Epoch 564/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8364e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3878e-20\n",
      "Epoch 565/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8678e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3878e-20\n",
      "Epoch 566/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4679e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3878e-20\n",
      "Epoch 567/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0603e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3878e-20\n",
      "Epoch 568/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6066e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3878e-20\n",
      "Epoch 569/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7152e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3878e-20\n",
      "Epoch 570/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6276e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3878e-20\n",
      "Epoch 571/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8497e-05 - mae: 0.0043\n",
      "Epoch 571: ReduceLROnPlateau reducing learning rate to 6.938894233487004e-21.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8273e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3878e-20\n",
      "Epoch 572/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7030e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.9389e-21\n",
      "Epoch 573/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6493e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.9389e-21\n",
      "Epoch 574/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4739e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.9389e-21\n",
      "Epoch 575/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6682e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.9389e-21\n",
      "Epoch 576/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5210e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.9389e-21\n",
      "Epoch 577/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5508e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.9389e-21\n",
      "Epoch 578/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5715e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.9389e-21\n",
      "Epoch 579/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4462e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.9389e-21\n",
      "Epoch 580/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5612e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.9389e-21\n",
      "Epoch 581/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6849e-05 - mae: 0.0043\n",
      "Epoch 581: ReduceLROnPlateau reducing learning rate to 3.469447116743502e-21.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6807e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.9389e-21\n",
      "Epoch 582/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8424e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.4694e-21\n",
      "Epoch 583/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5838e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.4694e-21\n",
      "Epoch 584/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7866e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.4694e-21\n",
      "Epoch 585/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7447e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.4694e-21\n",
      "Epoch 586/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8270e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.4694e-21\n",
      "Epoch 587/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6324e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.4694e-21\n",
      "Epoch 588/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.0515e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.4694e-21\n",
      "Epoch 589/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9335e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.4694e-21\n",
      "Epoch 590/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5134e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.4694e-21\n",
      "Epoch 591/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6585e-05 - mae: 0.0042\n",
      "Epoch 591: ReduceLROnPlateau reducing learning rate to 1.734723558371751e-21.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6557e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.4694e-21\n",
      "Epoch 592/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7454e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7347e-21\n",
      "Epoch 593/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9202e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7347e-21\n",
      "Epoch 594/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6947e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7347e-21\n",
      "Epoch 595/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3041e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7347e-21\n",
      "Epoch 596/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5910e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7347e-21\n",
      "Epoch 597/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7879e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7347e-21\n",
      "Epoch 598/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6509e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7347e-21\n",
      "Epoch 599/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8980e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7347e-21\n",
      "Epoch 600/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7786e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7347e-21\n",
      "Epoch 601/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6714e-05 - mae: 0.0043\n",
      "Epoch 601: ReduceLROnPlateau reducing learning rate to 8.673617791858755e-22.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6672e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.7347e-21\n",
      "Epoch 602/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7655e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.6736e-22\n",
      "Epoch 603/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5411e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.6736e-22\n",
      "Epoch 604/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4765e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.6736e-22\n",
      "Epoch 605/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8593e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.6736e-22\n",
      "Epoch 606/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7176e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.6736e-22\n",
      "Epoch 607/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5714e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.6736e-22\n",
      "Epoch 608/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8592e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.6736e-22\n",
      "Epoch 609/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7185e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.6736e-22\n",
      "Epoch 610/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8302e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.6736e-22\n",
      "Epoch 611/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7891e-05 - mae: 0.0043\n",
      "Epoch 611: ReduceLROnPlateau reducing learning rate to 4.336808895929377e-22.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7728e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.6736e-22\n",
      "Epoch 612/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6119e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.3368e-22\n",
      "Epoch 613/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6916e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.3368e-22\n",
      "Epoch 614/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9208e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.3368e-22\n",
      "Epoch 615/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5830e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.3368e-22\n",
      "Epoch 616/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9282e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.3368e-22\n",
      "Epoch 617/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.1768e-05 - mae: 0.0045 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.3368e-22\n",
      "Epoch 618/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5479e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.3368e-22\n",
      "Epoch 619/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9269e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.3368e-22\n",
      "Epoch 620/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6370e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.3368e-22\n",
      "Epoch 621/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6681e-05 - mae: 0.0042\n",
      "Epoch 621: ReduceLROnPlateau reducing learning rate to 2.1684044479646887e-22.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6648e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.3368e-22\n",
      "Epoch 622/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4141e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1684e-22\n",
      "Epoch 623/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7547e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1684e-22\n",
      "Epoch 624/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8745e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1684e-22\n",
      "Epoch 625/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6658e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1684e-22\n",
      "Epoch 626/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6175e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1684e-22\n",
      "Epoch 627/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3548e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1684e-22\n",
      "Epoch 628/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8253e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1684e-22\n",
      "Epoch 629/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5486e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1684e-22\n",
      "Epoch 630/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8342e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1684e-22\n",
      "Epoch 631/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.8384e-05 - mae: 0.0043\n",
      "Epoch 631: ReduceLROnPlateau reducing learning rate to 1.0842022239823443e-22.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8178e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1684e-22\n",
      "Epoch 632/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6355e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0842e-22\n",
      "Epoch 633/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4041e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0842e-22\n",
      "Epoch 634/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4958e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0842e-22\n",
      "Epoch 635/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8246e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0842e-22\n",
      "Epoch 636/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6662e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0842e-22\n",
      "Epoch 637/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4778e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0842e-22\n",
      "Epoch 638/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9081e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0842e-22\n",
      "Epoch 639/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4302e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0842e-22\n",
      "Epoch 640/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6569e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0842e-22\n",
      "Epoch 641/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5001e-05 - mae: 0.0042\n",
      "Epoch 641: ReduceLROnPlateau reducing learning rate to 5.421011119911722e-23.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5132e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0842e-22\n",
      "Epoch 642/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6618e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.4210e-23\n",
      "Epoch 643/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9226e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.4210e-23\n",
      "Epoch 644/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8484e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.4210e-23\n",
      "Epoch 645/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4515e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.4210e-23\n",
      "Epoch 646/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4622e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.4210e-23\n",
      "Epoch 647/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7103e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.4210e-23\n",
      "Epoch 648/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.8185e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.4210e-23\n",
      "Epoch 649/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4692e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.4210e-23\n",
      "Epoch 650/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.0605e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.4210e-23\n",
      "Epoch 651/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4131e-05 - mae: 0.0042\n",
      "Epoch 651: ReduceLROnPlateau reducing learning rate to 2.710505559955861e-23.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4349e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.4210e-23\n",
      "Epoch 652/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6645e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7105e-23\n",
      "Epoch 653/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5576e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7105e-23\n",
      "Epoch 654/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8161e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7105e-23\n",
      "Epoch 655/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4335e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7105e-23\n",
      "Epoch 656/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8851e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7105e-23\n",
      "Epoch 657/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6736e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7105e-23\n",
      "Epoch 658/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6043e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7105e-23\n",
      "Epoch 659/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6834e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7105e-23\n",
      "Epoch 660/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6214e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7105e-23\n",
      "Epoch 661/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4633e-05 - mae: 0.0042\n",
      "Epoch 661: ReduceLROnPlateau reducing learning rate to 1.3552527799779304e-23.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4801e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.7105e-23\n",
      "Epoch 662/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7754e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3553e-23\n",
      "Epoch 663/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4466e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3553e-23\n",
      "Epoch 664/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6463e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3553e-23\n",
      "Epoch 665/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9850e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3553e-23\n",
      "Epoch 666/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3717e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3553e-23\n",
      "Epoch 667/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8885e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3553e-23\n",
      "Epoch 668/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9421e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3553e-23\n",
      "Epoch 669/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9649e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3553e-23\n",
      "Epoch 670/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5113e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3553e-23\n",
      "Epoch 671/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8873e-05 - mae: 0.0043\n",
      "Epoch 671: ReduceLROnPlateau reducing learning rate to 6.776263899889652e-24.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8612e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3553e-23\n",
      "Epoch 672/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4621e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.7763e-24\n",
      "Epoch 673/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4518e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.7763e-24\n",
      "Epoch 674/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6895e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.7763e-24\n",
      "Epoch 675/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7052e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.7763e-24\n",
      "Epoch 676/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8758e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.7763e-24\n",
      "Epoch 677/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4032e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.7763e-24\n",
      "Epoch 678/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2288e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.7763e-24\n",
      "Epoch 679/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7747e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.7763e-24\n",
      "Epoch 680/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6813e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.7763e-24\n",
      "Epoch 681/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6086e-05 - mae: 0.0042\n",
      "Epoch 681: ReduceLROnPlateau reducing learning rate to 3.388131949944826e-24.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6124e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.7763e-24\n",
      "Epoch 682/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4122e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3881e-24\n",
      "Epoch 683/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4514e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3881e-24\n",
      "Epoch 684/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6909e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3881e-24\n",
      "Epoch 685/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4863e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3881e-24\n",
      "Epoch 686/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6555e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3881e-24\n",
      "Epoch 687/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9196e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3881e-24\n",
      "Epoch 688/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7972e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3881e-24\n",
      "Epoch 689/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5083e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3881e-24\n",
      "Epoch 690/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7815e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3881e-24\n",
      "Epoch 691/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7535e-05 - mae: 0.0043\n",
      "Epoch 691: ReduceLROnPlateau reducing learning rate to 1.694065974972413e-24.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7422e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3881e-24\n",
      "Epoch 692/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5953e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6941e-24\n",
      "Epoch 693/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6303e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6941e-24\n",
      "Epoch 694/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4726e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6941e-24\n",
      "Epoch 695/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5323e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6941e-24\n",
      "Epoch 696/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5632e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6941e-24\n",
      "Epoch 697/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4135e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6941e-24\n",
      "Epoch 698/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6788e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6941e-24\n",
      "Epoch 699/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5640e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6941e-24\n",
      "Epoch 700/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8496e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6941e-24\n",
      "Epoch 701/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8634e-05 - mae: 0.0043\n",
      "Epoch 701: ReduceLROnPlateau reducing learning rate to 8.470329874862065e-25.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8408e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6941e-24\n",
      "Epoch 702/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3940e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.4703e-25\n",
      "Epoch 703/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7762e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.4703e-25\n",
      "Epoch 704/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7469e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.4703e-25\n",
      "Epoch 705/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3485e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.4703e-25\n",
      "Epoch 706/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8606e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.4703e-25\n",
      "Epoch 707/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7493e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.4703e-25\n",
      "Epoch 708/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8442e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.4703e-25\n",
      "Epoch 709/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6440e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.4703e-25\n",
      "Epoch 710/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5894e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.4703e-25\n",
      "Epoch 711/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.3687e-05 - mae: 0.0042\n",
      "Epoch 711: ReduceLROnPlateau reducing learning rate to 4.2351649374310325e-25.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3954e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.4703e-25\n",
      "Epoch 712/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6972e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.2352e-25\n",
      "Epoch 713/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8469e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.2352e-25\n",
      "Epoch 714/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0170e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.2352e-25\n",
      "Epoch 715/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7432e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.2352e-25\n",
      "Epoch 716/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5738e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.2352e-25\n",
      "Epoch 717/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6334e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.2352e-25\n",
      "Epoch 718/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4381e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.2352e-25\n",
      "Epoch 719/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6795e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.2352e-25\n",
      "Epoch 720/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4528e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.2352e-25\n",
      "Epoch 721/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4601e-05 - mae: 0.0042\n",
      "Epoch 721: ReduceLROnPlateau reducing learning rate to 2.1175824687155163e-25.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4763e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.2352e-25\n",
      "Epoch 722/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6211e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1176e-25\n",
      "Epoch 723/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6196e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1176e-25\n",
      "Epoch 724/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4293e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1176e-25\n",
      "Epoch 725/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5769e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1176e-25\n",
      "Epoch 726/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5539e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1176e-25\n",
      "Epoch 727/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6138e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1176e-25\n",
      "Epoch 728/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.5412e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1176e-25\n",
      "Epoch 729/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7208e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1176e-25\n",
      "Epoch 730/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7589e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1176e-25\n",
      "Epoch 731/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6954e-05 - mae: 0.0042\n",
      "Epoch 731: ReduceLROnPlateau reducing learning rate to 1.0587912343577581e-25.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6900e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.1176e-25\n",
      "Epoch 732/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6183e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0588e-25\n",
      "Epoch 733/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4490e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0588e-25\n",
      "Epoch 734/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5939e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0588e-25\n",
      "Epoch 735/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7307e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0588e-25\n",
      "Epoch 736/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5883e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0588e-25\n",
      "Epoch 737/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2276e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0588e-25\n",
      "Epoch 738/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7204e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0588e-25\n",
      "Epoch 739/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7160e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0588e-25\n",
      "Epoch 740/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7168e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0588e-25\n",
      "Epoch 741/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.3135e-05 - mae: 0.0041\n",
      "Epoch 741: ReduceLROnPlateau reducing learning rate to 5.293956171788791e-26.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3432e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0588e-25\n",
      "Epoch 742/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7662e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.2940e-26\n",
      "Epoch 743/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4025e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.2940e-26\n",
      "Epoch 744/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9817e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.2940e-26\n",
      "Epoch 745/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4373e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.2940e-26\n",
      "Epoch 746/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4279e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.2940e-26\n",
      "Epoch 747/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.5582e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.2940e-26\n",
      "Epoch 748/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2740e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.2940e-26\n",
      "Epoch 749/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4622e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.2940e-26\n",
      "Epoch 750/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5367e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.2940e-26\n",
      "Epoch 751/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7918e-05 - mae: 0.0043\n",
      "Epoch 751: ReduceLROnPlateau reducing learning rate to 2.6469780858943953e-26.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7762e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.2940e-26\n",
      "Epoch 752/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7891e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.6470e-26\n",
      "Epoch 753/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6332e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.6470e-26\n",
      "Epoch 754/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5811e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.6470e-26\n",
      "Epoch 755/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7730e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.6470e-26\n",
      "Epoch 756/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5997e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.6470e-26\n",
      "Epoch 757/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5713e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.6470e-26\n",
      "Epoch 758/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.8851e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.6470e-26\n",
      "Epoch 759/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3941e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.6470e-26\n",
      "Epoch 760/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2708e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.6470e-26\n",
      "Epoch 761/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.8614e-05 - mae: 0.0044\n",
      "Epoch 761: ReduceLROnPlateau reducing learning rate to 1.3234890429471977e-26.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8384e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.6470e-26\n",
      "Epoch 762/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6716e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3235e-26\n",
      "Epoch 763/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7643e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3235e-26\n",
      "Epoch 764/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9943e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3235e-26\n",
      "Epoch 765/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6733e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3235e-26\n",
      "Epoch 766/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2590e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3235e-26\n",
      "Epoch 767/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7841e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3235e-26\n",
      "Epoch 768/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8523e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3235e-26\n",
      "Epoch 769/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5210e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3235e-26\n",
      "Epoch 770/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4449e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3235e-26\n",
      "Epoch 771/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7490e-05 - mae: 0.0043\n",
      "Epoch 771: ReduceLROnPlateau reducing learning rate to 6.617445214735988e-27.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7375e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.3235e-26\n",
      "Epoch 772/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5238e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.6174e-27\n",
      "Epoch 773/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8785e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.6174e-27\n",
      "Epoch 774/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5100e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.6174e-27\n",
      "Epoch 775/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7898e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.6174e-27\n",
      "Epoch 776/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.9538e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.6174e-27\n",
      "Epoch 777/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9500e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.6174e-27\n",
      "Epoch 778/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6302e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.6174e-27\n",
      "Epoch 779/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7168e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.6174e-27\n",
      "Epoch 780/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5750e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.6174e-27\n",
      "Epoch 781/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8700e-05 - mae: 0.0043\n",
      "Epoch 781: ReduceLROnPlateau reducing learning rate to 3.308722607367994e-27.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8453e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.6174e-27\n",
      "Epoch 782/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5287e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3087e-27\n",
      "Epoch 783/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9377e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3087e-27\n",
      "Epoch 784/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.9002e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3087e-27\n",
      "Epoch 785/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6656e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3087e-27\n",
      "Epoch 786/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4811e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3087e-27\n",
      "Epoch 787/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7936e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3087e-27\n",
      "Epoch 788/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5420e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3087e-27\n",
      "Epoch 789/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8082e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3087e-27\n",
      "Epoch 790/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5330e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3087e-27\n",
      "Epoch 791/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7752e-05 - mae: 0.0043\n",
      "Epoch 791: ReduceLROnPlateau reducing learning rate to 1.654361303683997e-27.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7606e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.3087e-27\n",
      "Epoch 792/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6277e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6544e-27\n",
      "Epoch 793/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7663e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6544e-27\n",
      "Epoch 794/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4835e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6544e-27\n",
      "Epoch 795/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8929e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6544e-27\n",
      "Epoch 796/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4650e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6544e-27\n",
      "Epoch 797/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7931e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6544e-27\n",
      "Epoch 798/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9330e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6544e-27\n",
      "Epoch 799/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8840e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6544e-27\n",
      "Epoch 800/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3015e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6544e-27\n",
      "Epoch 801/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.2629e-05 - mae: 0.0041\n",
      "Epoch 801: ReduceLROnPlateau reducing learning rate to 8.271806518419985e-28.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2989e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6544e-27\n",
      "Epoch 802/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5905e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.2718e-28\n",
      "Epoch 803/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7356e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.2718e-28\n",
      "Epoch 804/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7241e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.2718e-28\n",
      "Epoch 805/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3735e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.2718e-28\n",
      "Epoch 806/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4366e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.2718e-28\n",
      "Epoch 807/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5703e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.2718e-28\n",
      "Epoch 808/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5932e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.2718e-28\n",
      "Epoch 809/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2490e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.2718e-28\n",
      "Epoch 810/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6524e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.2718e-28\n",
      "Epoch 811/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7971e-05 - mae: 0.0043\n",
      "Epoch 811: ReduceLROnPlateau reducing learning rate to 4.135903259209993e-28.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7809e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.2718e-28\n",
      "Epoch 812/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5553e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.1359e-28\n",
      "Epoch 813/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7985e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.1359e-28\n",
      "Epoch 814/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.5624e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.1359e-28\n",
      "Epoch 815/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6759e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.1359e-28\n",
      "Epoch 816/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3626e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.1359e-28\n",
      "Epoch 817/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8307e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.1359e-28\n",
      "Epoch 818/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6011e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.1359e-28\n",
      "Epoch 819/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9498e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.1359e-28\n",
      "Epoch 820/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5400e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.1359e-28\n",
      "Epoch 821/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4637e-05 - mae: 0.0042\n",
      "Epoch 821: ReduceLROnPlateau reducing learning rate to 2.0679516296049964e-28.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4809e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.1359e-28\n",
      "Epoch 822/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7663e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0680e-28\n",
      "Epoch 823/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4967e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0680e-28\n",
      "Epoch 824/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5460e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0680e-28\n",
      "Epoch 825/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3854e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0680e-28\n",
      "Epoch 826/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4437e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0680e-28\n",
      "Epoch 827/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5726e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0680e-28\n",
      "Epoch 828/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7316e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0680e-28\n",
      "Epoch 829/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6277e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0680e-28\n",
      "Epoch 830/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8723e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0680e-28\n",
      "Epoch 831/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.8458e-05 - mae: 0.0043\n",
      "Epoch 831: ReduceLROnPlateau reducing learning rate to 1.0339758148024982e-28.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8251e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0680e-28\n",
      "Epoch 832/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4446e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0340e-28\n",
      "Epoch 833/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6247e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0340e-28\n",
      "Epoch 834/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5431e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0340e-28\n",
      "Epoch 835/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5691e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0340e-28\n",
      "Epoch 836/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7216e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0340e-28\n",
      "Epoch 837/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6736e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0340e-28\n",
      "Epoch 838/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5976e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0340e-28\n",
      "Epoch 839/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7111e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0340e-28\n",
      "Epoch 840/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7065e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0340e-28\n",
      "Epoch 841/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.7423e-05 - mae: 0.0043\n",
      "Epoch 841: ReduceLROnPlateau reducing learning rate to 5.169879074012491e-29.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7318e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0340e-28\n",
      "Epoch 842/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7579e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.1699e-29\n",
      "Epoch 843/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4440e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.1699e-29\n",
      "Epoch 844/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8508e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.1699e-29\n",
      "Epoch 845/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8964e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.1699e-29\n",
      "Epoch 846/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4798e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.1699e-29\n",
      "Epoch 847/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3217e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.1699e-29\n",
      "Epoch 848/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8354e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.1699e-29\n",
      "Epoch 849/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6867e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.1699e-29\n",
      "Epoch 850/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.5900e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.1699e-29\n",
      "Epoch 851/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.5266e-05 - mae: 0.0042\n",
      "Epoch 851: ReduceLROnPlateau reducing learning rate to 2.5849395370062454e-29.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5371e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.1699e-29\n",
      "Epoch 852/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7142e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5849e-29\n",
      "Epoch 853/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4688e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5849e-29\n",
      "Epoch 854/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.2255e-05 - mae: 0.0045 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5849e-29\n",
      "Epoch 855/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7494e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5849e-29\n",
      "Epoch 856/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8374e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5849e-29\n",
      "Epoch 857/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7972e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5849e-29\n",
      "Epoch 858/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8958e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5849e-29\n",
      "Epoch 859/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.5059e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5849e-29\n",
      "Epoch 860/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4449e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5849e-29\n",
      "Epoch 861/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7947e-05 - mae: 0.0043\n",
      "Epoch 861: ReduceLROnPlateau reducing learning rate to 1.2924697685031227e-29.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7782e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5849e-29\n",
      "Epoch 862/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4331e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2925e-29\n",
      "Epoch 863/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5271e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2925e-29\n",
      "Epoch 864/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5950e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2925e-29\n",
      "Epoch 865/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5387e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2925e-29\n",
      "Epoch 866/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6486e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2925e-29\n",
      "Epoch 867/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5793e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2925e-29\n",
      "Epoch 868/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.5900e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2925e-29\n",
      "Epoch 869/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.0219e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2925e-29\n",
      "Epoch 870/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5458e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2925e-29\n",
      "Epoch 871/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.5245e-05 - mae: 0.0042\n",
      "Epoch 871: ReduceLROnPlateau reducing learning rate to 6.462348842515614e-30.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5353e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2925e-29\n",
      "Epoch 872/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8044e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.4623e-30\n",
      "Epoch 873/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9654e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.4623e-30\n",
      "Epoch 874/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8012e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.4623e-30\n",
      "Epoch 875/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4776e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.4623e-30\n",
      "Epoch 876/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6736e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.4623e-30\n",
      "Epoch 877/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6042e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.4623e-30\n",
      "Epoch 878/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7679e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.4623e-30\n",
      "Epoch 879/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4158e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.4623e-30\n",
      "Epoch 880/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5418e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.4623e-30\n",
      "Epoch 881/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6954e-05 - mae: 0.0043\n",
      "Epoch 881: ReduceLROnPlateau reducing learning rate to 3.231174421257807e-30.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6872e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.4623e-30\n",
      "Epoch 882/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4372e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.2312e-30\n",
      "Epoch 883/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5083e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.2312e-30\n",
      "Epoch 884/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8032e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.2312e-30\n",
      "Epoch 885/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5197e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.2312e-30\n",
      "Epoch 886/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6294e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.2312e-30\n",
      "Epoch 887/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5913e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.2312e-30\n",
      "Epoch 888/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5638e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.2312e-30\n",
      "Epoch 889/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6971e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.2312e-30\n",
      "Epoch 890/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6624e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.2312e-30\n",
      "Epoch 891/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6769e-05 - mae: 0.0043\n",
      "Epoch 891: ReduceLROnPlateau reducing learning rate to 1.6155872106289034e-30.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6732e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.2312e-30\n",
      "Epoch 892/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8351e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6156e-30\n",
      "Epoch 893/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8592e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6156e-30\n",
      "Epoch 894/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7989e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6156e-30\n",
      "Epoch 895/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6023e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6156e-30\n",
      "Epoch 896/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7399e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6156e-30\n",
      "Epoch 897/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8171e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6156e-30\n",
      "Epoch 898/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8262e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6156e-30\n",
      "Epoch 899/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4970e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6156e-30\n",
      "Epoch 900/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3727e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6156e-30\n",
      "Epoch 901/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.3836e-05 - mae: 0.0042\n",
      "Epoch 901: ReduceLROnPlateau reducing learning rate to 8.077936053144517e-31.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4074e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.6156e-30\n",
      "Epoch 902/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6315e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.0779e-31\n",
      "Epoch 903/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6278e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.0779e-31\n",
      "Epoch 904/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7327e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.0779e-31\n",
      "Epoch 905/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5483e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.0779e-31\n",
      "Epoch 906/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5726e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.0779e-31\n",
      "Epoch 907/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8336e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.0779e-31\n",
      "Epoch 908/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7731e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.0779e-31\n",
      "Epoch 909/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3582e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.0779e-31\n",
      "Epoch 910/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5350e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.0779e-31\n",
      "Epoch 911/1000\n",
      "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.3514e-05 - mae: 0.0041\n",
      "Epoch 911: ReduceLROnPlateau reducing learning rate to 4.0389680265722585e-31.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.4102e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 8.0779e-31\n",
      "Epoch 912/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5133e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.0390e-31\n",
      "Epoch 913/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6733e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.0390e-31\n",
      "Epoch 914/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5333e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.0390e-31\n",
      "Epoch 915/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7154e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.0390e-31\n",
      "Epoch 916/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7320e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.0390e-31\n",
      "Epoch 917/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6689e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.0390e-31\n",
      "Epoch 918/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7441e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.0390e-31\n",
      "Epoch 919/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7344e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.0390e-31\n",
      "Epoch 920/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6339e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.0390e-31\n",
      "Epoch 921/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6753e-05 - mae: 0.0043\n",
      "Epoch 921: ReduceLROnPlateau reducing learning rate to 2.0194840132861292e-31.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6710e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 4.0390e-31\n",
      "Epoch 922/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7065e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0195e-31\n",
      "Epoch 923/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5202e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0195e-31\n",
      "Epoch 924/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6244e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0195e-31\n",
      "Epoch 925/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3463e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0195e-31\n",
      "Epoch 926/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7070e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0195e-31\n",
      "Epoch 927/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3909e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0195e-31\n",
      "Epoch 928/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.2544e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0195e-31\n",
      "Epoch 929/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8786e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0195e-31\n",
      "Epoch 930/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9809e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0195e-31\n",
      "Epoch 931/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6638e-05 - mae: 0.0042\n",
      "Epoch 931: ReduceLROnPlateau reducing learning rate to 1.0097420066430646e-31.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6599e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.0195e-31\n",
      "Epoch 932/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7687e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0097e-31\n",
      "Epoch 933/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6120e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0097e-31\n",
      "Epoch 934/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5662e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0097e-31\n",
      "Epoch 935/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4200e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0097e-31\n",
      "Epoch 936/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3310e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0097e-31\n",
      "Epoch 937/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5152e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0097e-31\n",
      "Epoch 938/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5683e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0097e-31\n",
      "Epoch 939/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0674e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0097e-31\n",
      "Epoch 940/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6786e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0097e-31\n",
      "Epoch 941/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9005e-05 - mae: 0.0043\n",
      "Epoch 941: ReduceLROnPlateau reducing learning rate to 5.048710033215323e-32.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8740e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.0097e-31\n",
      "Epoch 942/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5449e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.0487e-32\n",
      "Epoch 943/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6294e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.0487e-32\n",
      "Epoch 944/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2684e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.0487e-32\n",
      "Epoch 945/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7187e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.0487e-32\n",
      "Epoch 946/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7334e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.0487e-32\n",
      "Epoch 947/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6800e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.0487e-32\n",
      "Epoch 948/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4717e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.0487e-32\n",
      "Epoch 949/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9440e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.0487e-32\n",
      "Epoch 950/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.1809e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.0487e-32\n",
      "Epoch 951/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.7925e-05 - mae: 0.0043\n",
      "Epoch 951: ReduceLROnPlateau reducing learning rate to 2.5243550166076616e-32.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7767e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 5.0487e-32\n",
      "Epoch 952/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6386e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5244e-32\n",
      "Epoch 953/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4475e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5244e-32\n",
      "Epoch 954/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7079e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5244e-32\n",
      "Epoch 955/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9277e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5244e-32\n",
      "Epoch 956/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3967e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5244e-32\n",
      "Epoch 957/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8617e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5244e-32\n",
      "Epoch 958/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.9454e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5244e-32\n",
      "Epoch 959/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6355e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5244e-32\n",
      "Epoch 960/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5097e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5244e-32\n",
      "Epoch 961/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5315e-05 - mae: 0.0042\n",
      "Epoch 961: ReduceLROnPlateau reducing learning rate to 1.2621775083038308e-32.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5421e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 2.5244e-32\n",
      "Epoch 962/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6561e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2622e-32\n",
      "Epoch 963/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5933e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2622e-32\n",
      "Epoch 964/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6236e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2622e-32\n",
      "Epoch 965/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5261e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2622e-32\n",
      "Epoch 966/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.5700e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2622e-32\n",
      "Epoch 967/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5484e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2622e-32\n",
      "Epoch 968/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9645e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2622e-32\n",
      "Epoch 969/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9444e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2622e-32\n",
      "Epoch 970/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8527e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2622e-32\n",
      "Epoch 971/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.1259e-05 - mae: 0.0040\n",
      "Epoch 971: ReduceLROnPlateau reducing learning rate to 6.310887541519154e-33.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.1770e-05 - mae: 0.0041 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.2622e-32\n",
      "Epoch 972/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8946e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.3109e-33\n",
      "Epoch 973/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7238e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.3109e-33\n",
      "Epoch 974/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7759e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.3109e-33\n",
      "Epoch 975/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8512e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.3109e-33\n",
      "Epoch 976/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4507e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.3109e-33\n",
      "Epoch 977/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4514e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.3109e-33\n",
      "Epoch 978/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3998e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.3109e-33\n",
      "Epoch 979/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3654e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.3109e-33\n",
      "Epoch 980/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.3512e-05 - mae: 0.0045 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.3109e-33\n",
      "Epoch 981/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.0607e-05 - mae: 0.0044\n",
      "Epoch 981: ReduceLROnPlateau reducing learning rate to 3.155443770759577e-33.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.0180e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 6.3109e-33\n",
      "Epoch 982/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8249e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.1554e-33\n",
      "Epoch 983/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5760e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.1554e-33\n",
      "Epoch 984/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6154e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.1554e-33\n",
      "Epoch 985/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9369e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.1554e-33\n",
      "Epoch 986/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6796e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.1554e-33\n",
      "Epoch 987/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9387e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.1554e-33\n",
      "Epoch 988/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4693e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.1554e-33\n",
      "Epoch 989/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5226e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.1554e-33\n",
      "Epoch 990/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9383e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.1554e-33\n",
      "Epoch 991/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.5218e-05 - mae: 0.0042\n",
      "Epoch 991: ReduceLROnPlateau reducing learning rate to 1.5777218853797885e-33.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5322e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 3.1554e-33\n",
      "Epoch 992/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6863e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5777e-33\n",
      "Epoch 993/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.1709e-05 - mae: 0.0044 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5777e-33\n",
      "Epoch 994/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.8061e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5777e-33\n",
      "Epoch 995/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5482e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5777e-33\n",
      "Epoch 996/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7749e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5777e-33\n",
      "Epoch 997/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8956e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5777e-33\n",
      "Epoch 998/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6107e-05 - mae: 0.0042 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5777e-33\n",
      "Epoch 999/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7751e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5777e-33\n",
      "Epoch 1000/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6675e-05 - mae: 0.0043 - val_loss: 4.0512e-05 - val_mae: 0.0040 - learning_rate: 1.5777e-33\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.6364e-05 - mae: 0.0042 \n",
      "Model Loss: 4.463968798518181e-05, Model MAE: 0.00414000079035759\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model with a learning rate scheduler\n",
    "history = model2.fit(X_train, y_train, epochs=1000, batch_size=16, validation_split=0.2, callbacks=[lr_scheduler])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model2.evaluate(X_train, y_train)\n",
    "print(f'Model Loss: {loss}, Model MAE: {mae}')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation MAE values\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('Model Mean Absolute Error')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab38f9-dbed-4abe-bb1f-bd3e61f2c609",
   "metadata": {},
   "source": [
    "# approach 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca944cfb-4dc0-4ded-b2f7-4dd57de91b90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a more complex model with batch normalization and L2 regularization\n",
    "model3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(2048, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(y_train.shape[1], activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "197c609b-e932-4148-b24f-60f251c97525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - loss: 2.5075 - mae: 0.1957 - val_loss: 2.0923 - val_mae: 0.0098 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.0330 - mae: 0.1674 - val_loss: 1.6580 - val_mae: 0.0106 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.5955 - mae: 0.1366 - val_loss: 1.2805 - val_mae: 0.0112 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2216 - mae: 0.1086 - val_loss: 0.9673 - val_mae: 0.0104 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.9182 - mae: 0.0893 - val_loss: 0.7206 - val_mae: 0.0098 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.6826 - mae: 0.0739 - val_loss: 0.5346 - val_mae: 0.0088 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.5064 - mae: 0.0640 - val_loss: 0.3951 - val_mae: 0.0084 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.3745 - mae: 0.0563 - val_loss: 0.2918 - val_mae: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2773 - mae: 0.0514 - val_loss: 0.2162 - val_mae: 0.0079 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2061 - mae: 0.0462 - val_loss: 0.1610 - val_mae: 0.0074 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1542 - mae: 0.0429 - val_loss: 0.1209 - val_mae: 0.0069 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1166 - mae: 0.0399 - val_loss: 0.0918 - val_mae: 0.0071 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0889 - mae: 0.0357 - val_loss: 0.0705 - val_mae: 0.0067 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0684 - mae: 0.0318 - val_loss: 0.0543 - val_mae: 0.0067 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0530 - mae: 0.0293 - val_loss: 0.0421 - val_mae: 0.0061 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0413 - mae: 0.0268 - val_loss: 0.0332 - val_mae: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0328 - mae: 0.0249 - val_loss: 0.0264 - val_mae: 0.0056 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0262 - mae: 0.0232 - val_loss: 0.0213 - val_mae: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0212 - mae: 0.0210 - val_loss: 0.0175 - val_mae: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0174 - mae: 0.0191 - val_loss: 0.0145 - val_mae: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0146 - mae: 0.0176 - val_loss: 0.0122 - val_mae: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0122 - mae: 0.0163 - val_loss: 0.0103 - val_mae: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0104 - mae: 0.0144 - val_loss: 0.0089 - val_mae: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0090 - mae: 0.0139 - val_loss: 0.0078 - val_mae: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0079 - mae: 0.0124 - val_loss: 0.0069 - val_mae: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0070 - mae: 0.0115 - val_loss: 0.0062 - val_mae: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0063 - mae: 0.0100 - val_loss: 0.0056 - val_mae: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0055 - mae: 0.0091 - val_loss: 0.0050 - val_mae: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0049 - mae: 0.0067 - val_loss: 0.0045 - val_mae: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0044 - mae: 0.0047 - val_loss: 0.0041 - val_mae: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0040 - mae: 0.0042 - val_loss: 0.0037 - val_mae: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0036 - mae: 0.0039 - val_loss: 0.0034 - val_mae: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0033 - mae: 0.0037 - val_loss: 0.0031 - val_mae: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0030 - mae: 0.0039 - val_loss: 0.0028 - val_mae: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0028 - mae: 0.0038 - val_loss: 0.0026 - val_mae: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0025 - mae: 0.0036 - val_loss: 0.0024 - val_mae: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0024 - mae: 0.0037 - val_loss: 0.0022 - val_mae: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0022 - mae: 0.0037 - val_loss: 0.0021 - val_mae: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0020 - mae: 0.0036 - val_loss: 0.0019 - val_mae: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0019 - mae: 0.0037 - val_loss: 0.0018 - val_mae: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0018 - mae: 0.0037 - val_loss: 0.0017 - val_mae: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0016 - mae: 0.0036 - val_loss: 0.0016 - val_mae: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0015 - mae: 0.0036 - val_loss: 0.0014 - val_mae: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0014 - mae: 0.0036 - val_loss: 0.0014 - val_mae: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0013 - mae: 0.0036 - val_loss: 0.0013 - val_mae: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0012 - mae: 0.0037 - val_loss: 0.0012 - val_mae: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 47/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0011 - mae: 0.0037 - val_loss: 0.0011 - val_mae: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 48/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0011 - mae: 0.0035 - val_loss: 0.0010 - val_mae: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 49/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 9.9202e-04 - mae: 0.0035 - val_loss: 9.5382e-04 - val_mae: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 50/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 9.2573e-04 - mae: 0.0035 - val_loss: 8.9072e-04 - val_mae: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 51/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.6551e-04 - mae: 0.0036 - val_loss: 8.3239e-04 - val_mae: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 52/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 8.1184e-04 - mae: 0.0037 - val_loss: 7.7732e-04 - val_mae: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 53/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.5457e-04 - mae: 0.0036 - val_loss: 7.2618e-04 - val_mae: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 54/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0477e-04 - mae: 0.0035 - val_loss: 6.7886e-04 - val_mae: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 55/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6228e-04 - mae: 0.0037 - val_loss: 6.3471e-04 - val_mae: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 56/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.1705e-04 - mae: 0.0037 - val_loss: 5.9300e-04 - val_mae: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 57/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 5.7796e-04 - mae: 0.0037 - val_loss: 5.5480e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 58/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 5.3578e-04 - mae: 0.0035 - val_loss: 5.1907e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 59/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 5.0130e-04 - mae: 0.0035 - val_loss: 4.8623e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 60/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 4.7504e-04 - mae: 0.0037 - val_loss: 4.5616e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 61/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 4.4451e-04 - mae: 0.0037 - val_loss: 4.2733e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 62/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 4.2000e-04 - mae: 0.0038 - val_loss: 4.0034e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 63/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.9312e-04 - mae: 0.0038 - val_loss: 3.7502e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 64/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.6671e-04 - mae: 0.0038 - val_loss: 3.5227e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 65/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.4337e-04 - mae: 0.0037 - val_loss: 3.3092e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 66/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.1911e-04 - mae: 0.0036 - val_loss: 3.1060e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 67/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.0353e-04 - mae: 0.0037 - val_loss: 2.9182e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 68/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.8306e-04 - mae: 0.0037 - val_loss: 2.7512e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 69/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.6953e-04 - mae: 0.0038 - val_loss: 2.5875e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 70/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.5111e-04 - mae: 0.0037 - val_loss: 2.4363e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 71/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.3717e-04 - mae: 0.0038 - val_loss: 2.2981e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 72/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.2102e-04 - mae: 0.0037 - val_loss: 2.1732e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 73/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.1382e-04 - mae: 0.0039 - val_loss: 2.0505e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 74/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.0243e-04 - mae: 0.0039 - val_loss: 1.9305e-04 - val_mae: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 75/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.8332e-04 - mae: 0.0036 - val_loss: 1.8189e-04 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 76/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.7758e-04 - mae: 0.0038 - val_loss: 1.7240e-04 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 77/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6680e-04 - mae: 0.0037 - val_loss: 1.6303e-04 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 78/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.5846e-04 - mae: 0.0037 - val_loss: 1.5489e-04 - val_mae: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 79/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.5068e-04 - mae: 0.0037 - val_loss: 1.4709e-04 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 80/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.4610e-04 - mae: 0.0038 - val_loss: 1.3977e-04 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 81/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.4207e-04 - mae: 0.0040 - val_loss: 1.3315e-04 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 82/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.2995e-04 - mae: 0.0037 - val_loss: 1.2678e-04 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 83/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.2815e-04 - mae: 0.0039 - val_loss: 1.2134e-04 - val_mae: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 84/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.1896e-04 - mae: 0.0037 - val_loss: 1.1588e-04 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 85/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.1433e-04 - mae: 0.0038 - val_loss: 1.1029e-04 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 86/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.0664e-04 - mae: 0.0037 - val_loss: 1.0625e-04 - val_mae: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 87/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.0748e-04 - mae: 0.0039 - val_loss: 1.0101e-04 - val_mae: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 88/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 9.7663e-05 - mae: 0.0037\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 9.7794e-05 - mae: 0.0037 - val_loss: 9.6852e-05 - val_mae: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 89/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 9.8278e-05 - mae: 0.0038 - val_loss: 9.5013e-05 - val_mae: 0.0038 - learning_rate: 5.0000e-04\n",
      "Epoch 90/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.9526e-05 - mae: 0.0035 - val_loss: 9.2945e-05 - val_mae: 0.0038 - learning_rate: 5.0000e-04\n",
      "Epoch 91/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 9.2621e-05 - mae: 0.0037 - val_loss: 9.1050e-05 - val_mae: 0.0038 - learning_rate: 5.0000e-04\n",
      "Epoch 92/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 9.0414e-05 - mae: 0.0037 - val_loss: 8.9326e-05 - val_mae: 0.0038 - learning_rate: 5.0000e-04\n",
      "Epoch 93/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.6562e-05 - mae: 0.0036 - val_loss: 8.7393e-05 - val_mae: 0.0038 - learning_rate: 5.0000e-04\n",
      "Epoch 94/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.8369e-05 - mae: 0.0038 - val_loss: 8.6012e-05 - val_mae: 0.0038 - learning_rate: 5.0000e-04\n",
      "Epoch 95/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.3399e-05 - mae: 0.0037 - val_loss: 8.4424e-05 - val_mae: 0.0038 - learning_rate: 5.0000e-04\n",
      "Epoch 96/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.1017e-05 - mae: 0.0036 - val_loss: 8.3035e-05 - val_mae: 0.0038 - learning_rate: 5.0000e-04\n",
      "Epoch 97/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.3610e-05 - mae: 0.0037 - val_loss: 8.1428e-05 - val_mae: 0.0038 - learning_rate: 5.0000e-04\n",
      "Epoch 98/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 8.4131e-05 - mae: 0.0039\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.3944e-05 - mae: 0.0039 - val_loss: 8.0405e-05 - val_mae: 0.0038 - learning_rate: 5.0000e-04\n",
      "Epoch 99/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.7449e-05 - mae: 0.0036 - val_loss: 7.9549e-05 - val_mae: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 100/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.9470e-05 - mae: 0.0037 - val_loss: 7.8540e-05 - val_mae: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 101/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.8488e-05 - mae: 0.0038 - val_loss: 7.7834e-05 - val_mae: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 102/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.9791e-05 - mae: 0.0038 - val_loss: 7.7324e-05 - val_mae: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 103/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.1053e-05 - mae: 0.0039 - val_loss: 7.6180e-05 - val_mae: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 104/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.6322e-05 - mae: 0.0038 - val_loss: 7.5926e-05 - val_mae: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 105/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.8104e-05 - mae: 0.0038 - val_loss: 7.5373e-05 - val_mae: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 106/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.9755e-05 - mae: 0.0039 - val_loss: 7.5420e-05 - val_mae: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 107/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.2502e-05 - mae: 0.0036 - val_loss: 7.4032e-05 - val_mae: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 108/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 7.1446e-05 - mae: 0.0036\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.1687e-05 - mae: 0.0036 - val_loss: 7.3309e-05 - val_mae: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 109/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9912e-05 - mae: 0.0036 - val_loss: 7.3138e-05 - val_mae: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 110/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.1621e-05 - mae: 0.0036 - val_loss: 7.3160e-05 - val_mae: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 111/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.2720e-05 - mae: 0.0037 - val_loss: 7.2988e-05 - val_mae: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 112/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9419e-05 - mae: 0.0036 - val_loss: 7.2799e-05 - val_mae: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 113/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.0546e-05 - mae: 0.0037 - val_loss: 7.2563e-05 - val_mae: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 114/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.2026e-05 - mae: 0.0037 - val_loss: 7.2217e-05 - val_mae: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 115/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.9515e-05 - mae: 0.0036 - val_loss: 7.1761e-05 - val_mae: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 116/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0606e-05 - mae: 0.0037 - val_loss: 7.1597e-05 - val_mae: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 117/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1134e-05 - mae: 0.0037 - val_loss: 7.1758e-05 - val_mae: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 118/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.8300e-05 - mae: 0.0036\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.8564e-05 - mae: 0.0036 - val_loss: 7.1686e-05 - val_mae: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 119/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1939e-05 - mae: 0.0038 - val_loss: 7.1291e-05 - val_mae: 0.0038 - learning_rate: 6.2500e-05\n",
      "Epoch 120/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0805e-05 - mae: 0.0037 - val_loss: 7.0711e-05 - val_mae: 0.0038 - learning_rate: 6.2500e-05\n",
      "Epoch 121/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1874e-05 - mae: 0.0038 - val_loss: 7.0653e-05 - val_mae: 0.0038 - learning_rate: 6.2500e-05\n",
      "Epoch 122/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.9626e-05 - mae: 0.0037 - val_loss: 7.0653e-05 - val_mae: 0.0038 - learning_rate: 6.2500e-05\n",
      "Epoch 123/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9831e-05 - mae: 0.0037 - val_loss: 7.0920e-05 - val_mae: 0.0038 - learning_rate: 6.2500e-05\n",
      "Epoch 124/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.3588e-05 - mae: 0.0039 - val_loss: 7.0257e-05 - val_mae: 0.0038 - learning_rate: 6.2500e-05\n",
      "Epoch 125/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6887e-05 - mae: 0.0036 - val_loss: 7.0258e-05 - val_mae: 0.0038 - learning_rate: 6.2500e-05\n",
      "Epoch 126/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0607e-05 - mae: 0.0038 - val_loss: 6.9981e-05 - val_mae: 0.0038 - learning_rate: 6.2500e-05\n",
      "Epoch 127/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8483e-05 - mae: 0.0037 - val_loss: 7.0563e-05 - val_mae: 0.0038 - learning_rate: 6.2500e-05\n",
      "Epoch 128/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 7.1302e-05 - mae: 0.0038\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1148e-05 - mae: 0.0038 - val_loss: 7.0117e-05 - val_mae: 0.0038 - learning_rate: 6.2500e-05\n",
      "Epoch 129/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1614e-05 - mae: 0.0038 - val_loss: 6.9886e-05 - val_mae: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 130/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1416e-05 - mae: 0.0038 - val_loss: 7.0036e-05 - val_mae: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 131/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6742e-05 - mae: 0.0036 - val_loss: 7.0036e-05 - val_mae: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 132/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8187e-05 - mae: 0.0037 - val_loss: 6.9888e-05 - val_mae: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 133/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.0124e-05 - mae: 0.0037 - val_loss: 6.9883e-05 - val_mae: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 134/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9380e-05 - mae: 0.0037 - val_loss: 6.9407e-05 - val_mae: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 135/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0579e-05 - mae: 0.0038 - val_loss: 6.9308e-05 - val_mae: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 136/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7457e-05 - mae: 0.0037 - val_loss: 6.9117e-05 - val_mae: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 137/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.8259e-05 - mae: 0.0037 - val_loss: 6.9469e-05 - val_mae: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 138/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.7547e-05 - mae: 0.0037\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7665e-05 - mae: 0.0037 - val_loss: 6.9589e-05 - val_mae: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 139/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5512e-05 - mae: 0.0037 - val_loss: 6.9432e-05 - val_mae: 0.0038 - learning_rate: 1.5625e-05\n",
      "Epoch 140/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6758e-05 - mae: 0.0037 - val_loss: 6.9476e-05 - val_mae: 0.0038 - learning_rate: 1.5625e-05\n",
      "Epoch 141/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1554e-05 - mae: 0.0038 - val_loss: 6.9522e-05 - val_mae: 0.0038 - learning_rate: 1.5625e-05\n",
      "Epoch 142/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.3654e-05 - mae: 0.0040 - val_loss: 6.9673e-05 - val_mae: 0.0038 - learning_rate: 1.5625e-05\n",
      "Epoch 143/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.7003e-05 - mae: 0.0037 - val_loss: 6.9792e-05 - val_mae: 0.0039 - learning_rate: 1.5625e-05\n",
      "Epoch 144/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9424e-05 - mae: 0.0037 - val_loss: 6.9679e-05 - val_mae: 0.0039 - learning_rate: 1.5625e-05\n",
      "Epoch 145/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1406e-05 - mae: 0.0038 - val_loss: 6.9452e-05 - val_mae: 0.0038 - learning_rate: 1.5625e-05\n",
      "Epoch 146/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1242e-05 - mae: 0.0038 - val_loss: 6.9304e-05 - val_mae: 0.0038 - learning_rate: 1.5625e-05\n",
      "Epoch 147/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5225e-05 - mae: 0.0036 - val_loss: 6.9395e-05 - val_mae: 0.0038 - learning_rate: 1.5625e-05\n",
      "Epoch 148/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.4858e-05 - mae: 0.0036\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5144e-05 - mae: 0.0036 - val_loss: 6.9481e-05 - val_mae: 0.0039 - learning_rate: 1.5625e-05\n",
      "Epoch 149/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.4526e-05 - mae: 0.0036 - val_loss: 6.9477e-05 - val_mae: 0.0039 - learning_rate: 7.8125e-06\n",
      "Epoch 150/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9650e-05 - mae: 0.0038 - val_loss: 6.9473e-05 - val_mae: 0.0039 - learning_rate: 7.8125e-06\n",
      "Epoch 151/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8442e-05 - mae: 0.0037 - val_loss: 6.9406e-05 - val_mae: 0.0039 - learning_rate: 7.8125e-06\n",
      "Epoch 152/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7594e-05 - mae: 0.0037 - val_loss: 6.9411e-05 - val_mae: 0.0039 - learning_rate: 7.8125e-06\n",
      "Epoch 153/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9153e-05 - mae: 0.0037 - val_loss: 6.9386e-05 - val_mae: 0.0039 - learning_rate: 7.8125e-06\n",
      "Epoch 154/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7306e-05 - mae: 0.0037 - val_loss: 6.9271e-05 - val_mae: 0.0039 - learning_rate: 7.8125e-06\n",
      "Epoch 155/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8875e-05 - mae: 0.0038 - val_loss: 6.9139e-05 - val_mae: 0.0038 - learning_rate: 7.8125e-06\n",
      "Epoch 156/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8143e-05 - mae: 0.0037 - val_loss: 6.9179e-05 - val_mae: 0.0039 - learning_rate: 7.8125e-06\n",
      "Epoch 157/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5904e-05 - mae: 0.0036 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 7.8125e-06\n",
      "Epoch 158/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.7504e-05 - mae: 0.0037\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7601e-05 - mae: 0.0037 - val_loss: 6.9210e-05 - val_mae: 0.0039 - learning_rate: 7.8125e-06\n",
      "Epoch 159/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8966e-05 - mae: 0.0038 - val_loss: 6.9178e-05 - val_mae: 0.0039 - learning_rate: 3.9063e-06\n",
      "Epoch 160/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7669e-05 - mae: 0.0038 - val_loss: 6.9154e-05 - val_mae: 0.0039 - learning_rate: 3.9063e-06\n",
      "Epoch 161/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.4166e-05 - mae: 0.0039 - val_loss: 6.9119e-05 - val_mae: 0.0039 - learning_rate: 3.9063e-06\n",
      "Epoch 162/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1085e-05 - mae: 0.0039 - val_loss: 6.9148e-05 - val_mae: 0.0039 - learning_rate: 3.9063e-06\n",
      "Epoch 163/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5433e-05 - mae: 0.0036 - val_loss: 6.9183e-05 - val_mae: 0.0039 - learning_rate: 3.9063e-06\n",
      "Epoch 164/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7997e-05 - mae: 0.0038 - val_loss: 6.9167e-05 - val_mae: 0.0039 - learning_rate: 3.9063e-06\n",
      "Epoch 165/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.7509e-05 - mae: 0.0037 - val_loss: 6.9132e-05 - val_mae: 0.0039 - learning_rate: 3.9063e-06\n",
      "Epoch 166/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7013e-05 - mae: 0.0037 - val_loss: 6.9145e-05 - val_mae: 0.0039 - learning_rate: 3.9063e-06\n",
      "Epoch 167/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0620e-05 - mae: 0.0038 - val_loss: 6.9193e-05 - val_mae: 0.0039 - learning_rate: 3.9063e-06\n",
      "Epoch 168/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.9410e-05 - mae: 0.0038\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9356e-05 - mae: 0.0038 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 3.9063e-06\n",
      "Epoch 169/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5018e-05 - mae: 0.0036 - val_loss: 6.9211e-05 - val_mae: 0.0039 - learning_rate: 1.9531e-06\n",
      "Epoch 170/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5959e-05 - mae: 0.0037 - val_loss: 6.9233e-05 - val_mae: 0.0039 - learning_rate: 1.9531e-06\n",
      "Epoch 171/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.4215e-05 - mae: 0.0036 - val_loss: 6.9220e-05 - val_mae: 0.0039 - learning_rate: 1.9531e-06\n",
      "Epoch 172/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9506e-05 - mae: 0.0038 - val_loss: 6.9205e-05 - val_mae: 0.0039 - learning_rate: 1.9531e-06\n",
      "Epoch 173/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.6415e-05 - mae: 0.0037 - val_loss: 6.9233e-05 - val_mae: 0.0039 - learning_rate: 1.9531e-06\n",
      "Epoch 174/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.7043e-05 - mae: 0.0037 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 1.9531e-06\n",
      "Epoch 175/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.9715e-05 - mae: 0.0037 - val_loss: 6.9199e-05 - val_mae: 0.0039 - learning_rate: 1.9531e-06\n",
      "Epoch 176/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1477e-05 - mae: 0.0039 - val_loss: 6.9203e-05 - val_mae: 0.0039 - learning_rate: 1.9531e-06\n",
      "Epoch 177/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7211e-05 - mae: 0.0037 - val_loss: 6.9200e-05 - val_mae: 0.0039 - learning_rate: 1.9531e-06\n",
      "Epoch 178/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.6502e-05 - mae: 0.0037\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6590e-05 - mae: 0.0037 - val_loss: 6.9237e-05 - val_mae: 0.0039 - learning_rate: 1.9531e-06\n",
      "Epoch 179/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.4240e-05 - mae: 0.0036 - val_loss: 6.9229e-05 - val_mae: 0.0039 - learning_rate: 9.7656e-07\n",
      "Epoch 180/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7815e-05 - mae: 0.0037 - val_loss: 6.9244e-05 - val_mae: 0.0039 - learning_rate: 9.7656e-07\n",
      "Epoch 181/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0059e-05 - mae: 0.0038 - val_loss: 6.9234e-05 - val_mae: 0.0039 - learning_rate: 9.7656e-07\n",
      "Epoch 182/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1120e-05 - mae: 0.0039 - val_loss: 6.9248e-05 - val_mae: 0.0039 - learning_rate: 9.7656e-07\n",
      "Epoch 183/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8284e-05 - mae: 0.0038 - val_loss: 6.9224e-05 - val_mae: 0.0039 - learning_rate: 9.7656e-07\n",
      "Epoch 184/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7255e-05 - mae: 0.0037 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 9.7656e-07\n",
      "Epoch 185/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7081e-05 - mae: 0.0037 - val_loss: 6.9211e-05 - val_mae: 0.0039 - learning_rate: 9.7656e-07\n",
      "Epoch 186/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5930e-05 - mae: 0.0037 - val_loss: 6.9180e-05 - val_mae: 0.0039 - learning_rate: 9.7656e-07\n",
      "Epoch 187/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1250e-05 - mae: 0.0039 - val_loss: 6.9150e-05 - val_mae: 0.0039 - learning_rate: 9.7656e-07\n",
      "Epoch 188/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.6512e-05 - mae: 0.0037\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6568e-05 - mae: 0.0037 - val_loss: 6.9161e-05 - val_mae: 0.0039 - learning_rate: 9.7656e-07\n",
      "Epoch 189/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7986e-05 - mae: 0.0037 - val_loss: 6.9203e-05 - val_mae: 0.0039 - learning_rate: 4.8828e-07\n",
      "Epoch 190/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8213e-05 - mae: 0.0037 - val_loss: 6.9204e-05 - val_mae: 0.0039 - learning_rate: 4.8828e-07\n",
      "Epoch 191/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7821e-05 - mae: 0.0037 - val_loss: 6.9201e-05 - val_mae: 0.0039 - learning_rate: 4.8828e-07\n",
      "Epoch 192/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0451e-05 - mae: 0.0038 - val_loss: 6.9203e-05 - val_mae: 0.0039 - learning_rate: 4.8828e-07\n",
      "Epoch 193/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.4184e-05 - mae: 0.0036 - val_loss: 6.9198e-05 - val_mae: 0.0039 - learning_rate: 4.8828e-07\n",
      "Epoch 194/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8601e-05 - mae: 0.0037 - val_loss: 6.9187e-05 - val_mae: 0.0039 - learning_rate: 4.8828e-07\n",
      "Epoch 195/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.4905e-05 - mae: 0.0036 - val_loss: 6.9164e-05 - val_mae: 0.0039 - learning_rate: 4.8828e-07\n",
      "Epoch 196/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1167e-05 - mae: 0.0039 - val_loss: 6.9161e-05 - val_mae: 0.0039 - learning_rate: 4.8828e-07\n",
      "Epoch 197/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1102e-05 - mae: 0.0039 - val_loss: 6.9177e-05 - val_mae: 0.0039 - learning_rate: 4.8828e-07\n",
      "Epoch 198/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.9192e-05 - mae: 0.0038\n",
      "Epoch 198: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9132e-05 - mae: 0.0038 - val_loss: 6.9156e-05 - val_mae: 0.0039 - learning_rate: 4.8828e-07\n",
      "Epoch 199/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7379e-05 - mae: 0.0037 - val_loss: 6.9147e-05 - val_mae: 0.0039 - learning_rate: 2.4414e-07\n",
      "Epoch 200/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0184e-05 - mae: 0.0038 - val_loss: 6.9130e-05 - val_mae: 0.0039 - learning_rate: 2.4414e-07\n",
      "Epoch 201/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9977e-05 - mae: 0.0038 - val_loss: 6.9127e-05 - val_mae: 0.0039 - learning_rate: 2.4414e-07\n",
      "Epoch 202/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8112e-05 - mae: 0.0038 - val_loss: 6.9109e-05 - val_mae: 0.0039 - learning_rate: 2.4414e-07\n",
      "Epoch 203/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6093e-05 - mae: 0.0037 - val_loss: 6.9121e-05 - val_mae: 0.0039 - learning_rate: 2.4414e-07\n",
      "Epoch 204/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7097e-05 - mae: 0.0037 - val_loss: 6.9128e-05 - val_mae: 0.0039 - learning_rate: 2.4414e-07\n",
      "Epoch 205/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.4094e-05 - mae: 0.0036 - val_loss: 6.9144e-05 - val_mae: 0.0039 - learning_rate: 2.4414e-07\n",
      "Epoch 206/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9385e-05 - mae: 0.0038 - val_loss: 6.9145e-05 - val_mae: 0.0039 - learning_rate: 2.4414e-07\n",
      "Epoch 207/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9017e-05 - mae: 0.0038 - val_loss: 6.9147e-05 - val_mae: 0.0039 - learning_rate: 2.4414e-07\n",
      "Epoch 208/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.9371e-05 - mae: 0.0038\n",
      "Epoch 208: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9272e-05 - mae: 0.0038 - val_loss: 6.9140e-05 - val_mae: 0.0039 - learning_rate: 2.4414e-07\n",
      "Epoch 209/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7201e-05 - mae: 0.0037 - val_loss: 6.9148e-05 - val_mae: 0.0039 - learning_rate: 1.2207e-07\n",
      "Epoch 210/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6342e-05 - mae: 0.0037 - val_loss: 6.9152e-05 - val_mae: 0.0039 - learning_rate: 1.2207e-07\n",
      "Epoch 211/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6176e-05 - mae: 0.0037 - val_loss: 6.9144e-05 - val_mae: 0.0039 - learning_rate: 1.2207e-07\n",
      "Epoch 212/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5300e-05 - mae: 0.0036 - val_loss: 6.9165e-05 - val_mae: 0.0039 - learning_rate: 1.2207e-07\n",
      "Epoch 213/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5048e-05 - mae: 0.0037 - val_loss: 6.9172e-05 - val_mae: 0.0039 - learning_rate: 1.2207e-07\n",
      "Epoch 214/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7061e-05 - mae: 0.0037 - val_loss: 6.9164e-05 - val_mae: 0.0039 - learning_rate: 1.2207e-07\n",
      "Epoch 215/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5611e-05 - mae: 0.0037 - val_loss: 6.9183e-05 - val_mae: 0.0039 - learning_rate: 1.2207e-07\n",
      "Epoch 216/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7327e-05 - mae: 0.0037 - val_loss: 6.9199e-05 - val_mae: 0.0039 - learning_rate: 1.2207e-07\n",
      "Epoch 217/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5223e-05 - mae: 0.0036 - val_loss: 6.9187e-05 - val_mae: 0.0039 - learning_rate: 1.2207e-07\n",
      "Epoch 218/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.6118e-05 - mae: 0.0037\n",
      "Epoch 218: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6234e-05 - mae: 0.0037 - val_loss: 6.9186e-05 - val_mae: 0.0039 - learning_rate: 1.2207e-07\n",
      "Epoch 219/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5975e-05 - mae: 0.0037 - val_loss: 6.9187e-05 - val_mae: 0.0039 - learning_rate: 6.1035e-08\n",
      "Epoch 220/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0021e-05 - mae: 0.0038 - val_loss: 6.9203e-05 - val_mae: 0.0039 - learning_rate: 6.1035e-08\n",
      "Epoch 221/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8136e-05 - mae: 0.0037 - val_loss: 6.9189e-05 - val_mae: 0.0039 - learning_rate: 6.1035e-08\n",
      "Epoch 222/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.2103e-05 - mae: 0.0039 - val_loss: 6.9173e-05 - val_mae: 0.0039 - learning_rate: 6.1035e-08\n",
      "Epoch 223/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7879e-05 - mae: 0.0037 - val_loss: 6.9174e-05 - val_mae: 0.0039 - learning_rate: 6.1035e-08\n",
      "Epoch 224/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8349e-05 - mae: 0.0038 - val_loss: 6.9162e-05 - val_mae: 0.0039 - learning_rate: 6.1035e-08\n",
      "Epoch 225/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6624e-05 - mae: 0.0037 - val_loss: 6.9156e-05 - val_mae: 0.0039 - learning_rate: 6.1035e-08\n",
      "Epoch 226/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7322e-05 - mae: 0.0037 - val_loss: 6.9164e-05 - val_mae: 0.0039 - learning_rate: 6.1035e-08\n",
      "Epoch 227/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6909e-05 - mae: 0.0037 - val_loss: 6.9158e-05 - val_mae: 0.0039 - learning_rate: 6.1035e-08\n",
      "Epoch 228/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 7.1873e-05 - mae: 0.0038\n",
      "Epoch 228: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1421e-05 - mae: 0.0038 - val_loss: 6.9168e-05 - val_mae: 0.0039 - learning_rate: 6.1035e-08\n",
      "Epoch 229/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7211e-05 - mae: 0.0037 - val_loss: 6.9135e-05 - val_mae: 0.0039 - learning_rate: 3.0518e-08\n",
      "Epoch 230/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5461e-05 - mae: 0.0037 - val_loss: 6.9151e-05 - val_mae: 0.0039 - learning_rate: 3.0518e-08\n",
      "Epoch 231/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6231e-05 - mae: 0.0037 - val_loss: 6.9162e-05 - val_mae: 0.0039 - learning_rate: 3.0518e-08\n",
      "Epoch 232/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6637e-05 - mae: 0.0037 - val_loss: 6.9164e-05 - val_mae: 0.0039 - learning_rate: 3.0518e-08\n",
      "Epoch 233/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0735e-05 - mae: 0.0038 - val_loss: 6.9193e-05 - val_mae: 0.0039 - learning_rate: 3.0518e-08\n",
      "Epoch 234/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7059e-05 - mae: 0.0037 - val_loss: 6.9186e-05 - val_mae: 0.0039 - learning_rate: 3.0518e-08\n",
      "Epoch 235/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6445e-05 - mae: 0.0036 - val_loss: 6.9181e-05 - val_mae: 0.0039 - learning_rate: 3.0518e-08\n",
      "Epoch 236/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6870e-05 - mae: 0.0037 - val_loss: 6.9179e-05 - val_mae: 0.0039 - learning_rate: 3.0518e-08\n",
      "Epoch 237/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7853e-05 - mae: 0.0037 - val_loss: 6.9193e-05 - val_mae: 0.0039 - learning_rate: 3.0518e-08\n",
      "Epoch 238/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.5906e-05 - mae: 0.0036\n",
      "Epoch 238: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6054e-05 - mae: 0.0036 - val_loss: 6.9171e-05 - val_mae: 0.0039 - learning_rate: 3.0518e-08\n",
      "Epoch 239/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7608e-05 - mae: 0.0037 - val_loss: 6.9187e-05 - val_mae: 0.0039 - learning_rate: 1.5259e-08\n",
      "Epoch 240/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5871e-05 - mae: 0.0037 - val_loss: 6.9154e-05 - val_mae: 0.0039 - learning_rate: 1.5259e-08\n",
      "Epoch 241/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6328e-05 - mae: 0.0037 - val_loss: 6.9163e-05 - val_mae: 0.0039 - learning_rate: 1.5259e-08\n",
      "Epoch 242/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7058e-05 - mae: 0.0037 - val_loss: 6.9180e-05 - val_mae: 0.0039 - learning_rate: 1.5259e-08\n",
      "Epoch 243/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.3737e-05 - mae: 0.0039 - val_loss: 6.9199e-05 - val_mae: 0.0039 - learning_rate: 1.5259e-08\n",
      "Epoch 244/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.8291e-05 - mae: 0.0037 - val_loss: 6.9186e-05 - val_mae: 0.0039 - learning_rate: 1.5259e-08\n",
      "Epoch 245/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8129e-05 - mae: 0.0038 - val_loss: 6.9158e-05 - val_mae: 0.0039 - learning_rate: 1.5259e-08\n",
      "Epoch 246/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7301e-05 - mae: 0.0037 - val_loss: 6.9179e-05 - val_mae: 0.0039 - learning_rate: 1.5259e-08\n",
      "Epoch 247/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5454e-05 - mae: 0.0037 - val_loss: 6.9197e-05 - val_mae: 0.0039 - learning_rate: 1.5259e-08\n",
      "Epoch 248/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.8841e-05 - mae: 0.0038\n",
      "Epoch 248: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8770e-05 - mae: 0.0038 - val_loss: 6.9199e-05 - val_mae: 0.0039 - learning_rate: 1.5259e-08\n",
      "Epoch 249/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.4367e-05 - mae: 0.0036 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 7.6294e-09\n",
      "Epoch 250/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6961e-05 - mae: 0.0037 - val_loss: 6.9195e-05 - val_mae: 0.0039 - learning_rate: 7.6294e-09\n",
      "Epoch 251/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7333e-05 - mae: 0.0037 - val_loss: 6.9189e-05 - val_mae: 0.0039 - learning_rate: 7.6294e-09\n",
      "Epoch 252/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.3817e-05 - mae: 0.0036 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 7.6294e-09\n",
      "Epoch 253/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9718e-05 - mae: 0.0038 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 7.6294e-09\n",
      "Epoch 254/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8682e-05 - mae: 0.0038 - val_loss: 6.9220e-05 - val_mae: 0.0039 - learning_rate: 7.6294e-09\n",
      "Epoch 255/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9578e-05 - mae: 0.0038 - val_loss: 6.9218e-05 - val_mae: 0.0039 - learning_rate: 7.6294e-09\n",
      "Epoch 256/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1899e-05 - mae: 0.0038 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 7.6294e-09\n",
      "Epoch 257/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8995e-05 - mae: 0.0038 - val_loss: 6.9240e-05 - val_mae: 0.0039 - learning_rate: 7.6294e-09\n",
      "Epoch 258/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.4500e-05 - mae: 0.0037\n",
      "Epoch 258: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4847e-05 - mae: 0.0037 - val_loss: 6.9233e-05 - val_mae: 0.0039 - learning_rate: 7.6294e-09\n",
      "Epoch 259/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5857e-05 - mae: 0.0037 - val_loss: 6.9211e-05 - val_mae: 0.0039 - learning_rate: 3.8147e-09\n",
      "Epoch 260/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6321e-05 - mae: 0.0037 - val_loss: 6.9203e-05 - val_mae: 0.0039 - learning_rate: 3.8147e-09\n",
      "Epoch 261/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8371e-05 - mae: 0.0037 - val_loss: 6.9179e-05 - val_mae: 0.0039 - learning_rate: 3.8147e-09\n",
      "Epoch 262/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6556e-05 - mae: 0.0037 - val_loss: 6.9176e-05 - val_mae: 0.0039 - learning_rate: 3.8147e-09\n",
      "Epoch 263/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9574e-05 - mae: 0.0039 - val_loss: 6.9189e-05 - val_mae: 0.0039 - learning_rate: 3.8147e-09\n",
      "Epoch 264/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5033e-05 - mae: 0.0036 - val_loss: 6.9187e-05 - val_mae: 0.0039 - learning_rate: 3.8147e-09\n",
      "Epoch 265/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7623e-05 - mae: 0.0038 - val_loss: 6.9196e-05 - val_mae: 0.0039 - learning_rate: 3.8147e-09\n",
      "Epoch 266/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5320e-05 - mae: 0.0036 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 3.8147e-09\n",
      "Epoch 267/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7702e-05 - mae: 0.0038 - val_loss: 6.9241e-05 - val_mae: 0.0039 - learning_rate: 3.8147e-09\n",
      "Epoch 268/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.6111e-05 - mae: 0.0037\n",
      "Epoch 268: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6290e-05 - mae: 0.0037 - val_loss: 6.9234e-05 - val_mae: 0.0039 - learning_rate: 3.8147e-09\n",
      "Epoch 269/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5690e-05 - mae: 0.0037 - val_loss: 6.9197e-05 - val_mae: 0.0039 - learning_rate: 1.9073e-09\n",
      "Epoch 270/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7815e-05 - mae: 0.0037 - val_loss: 6.9226e-05 - val_mae: 0.0039 - learning_rate: 1.9073e-09\n",
      "Epoch 271/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6162e-05 - mae: 0.0037 - val_loss: 6.9239e-05 - val_mae: 0.0039 - learning_rate: 1.9073e-09\n",
      "Epoch 272/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9191e-05 - mae: 0.0037 - val_loss: 6.9248e-05 - val_mae: 0.0039 - learning_rate: 1.9073e-09\n",
      "Epoch 273/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9924e-05 - mae: 0.0038 - val_loss: 6.9235e-05 - val_mae: 0.0039 - learning_rate: 1.9073e-09\n",
      "Epoch 274/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7985e-05 - mae: 0.0037 - val_loss: 6.9232e-05 - val_mae: 0.0039 - learning_rate: 1.9073e-09\n",
      "Epoch 275/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7108e-05 - mae: 0.0037 - val_loss: 6.9205e-05 - val_mae: 0.0039 - learning_rate: 1.9073e-09\n",
      "Epoch 276/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.3242e-05 - mae: 0.0036 - val_loss: 6.9172e-05 - val_mae: 0.0039 - learning_rate: 1.9073e-09\n",
      "Epoch 277/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9038e-05 - mae: 0.0038 - val_loss: 6.9148e-05 - val_mae: 0.0039 - learning_rate: 1.9073e-09\n",
      "Epoch 278/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.6409e-05 - mae: 0.0037\n",
      "Epoch 278: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6566e-05 - mae: 0.0037 - val_loss: 6.9160e-05 - val_mae: 0.0039 - learning_rate: 1.9073e-09\n",
      "Epoch 279/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6692e-05 - mae: 0.0037 - val_loss: 6.9175e-05 - val_mae: 0.0039 - learning_rate: 9.5367e-10\n",
      "Epoch 280/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0947e-05 - mae: 0.0038 - val_loss: 6.9170e-05 - val_mae: 0.0039 - learning_rate: 9.5367e-10\n",
      "Epoch 281/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7755e-05 - mae: 0.0037 - val_loss: 6.9178e-05 - val_mae: 0.0039 - learning_rate: 9.5367e-10\n",
      "Epoch 282/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0187e-05 - mae: 0.0038 - val_loss: 6.9172e-05 - val_mae: 0.0039 - learning_rate: 9.5367e-10\n",
      "Epoch 283/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7426e-05 - mae: 0.0037 - val_loss: 6.9163e-05 - val_mae: 0.0039 - learning_rate: 9.5367e-10\n",
      "Epoch 284/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7552e-05 - mae: 0.0037 - val_loss: 6.9173e-05 - val_mae: 0.0039 - learning_rate: 9.5367e-10\n",
      "Epoch 285/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9976e-05 - mae: 0.0038 - val_loss: 6.9167e-05 - val_mae: 0.0039 - learning_rate: 9.5367e-10\n",
      "Epoch 286/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1033e-05 - mae: 0.0038 - val_loss: 6.9189e-05 - val_mae: 0.0039 - learning_rate: 9.5367e-10\n",
      "Epoch 287/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6000e-05 - mae: 0.0037 - val_loss: 6.9199e-05 - val_mae: 0.0039 - learning_rate: 9.5367e-10\n",
      "Epoch 288/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.8970e-05 - mae: 0.0038\n",
      "Epoch 288: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8947e-05 - mae: 0.0038 - val_loss: 6.9188e-05 - val_mae: 0.0039 - learning_rate: 9.5367e-10\n",
      "Epoch 289/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7591e-05 - mae: 0.0038 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 4.7684e-10\n",
      "Epoch 290/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8393e-05 - mae: 0.0037 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 4.7684e-10\n",
      "Epoch 291/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7068e-05 - mae: 0.0037 - val_loss: 6.9176e-05 - val_mae: 0.0039 - learning_rate: 4.7684e-10\n",
      "Epoch 292/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9504e-05 - mae: 0.0038 - val_loss: 6.9165e-05 - val_mae: 0.0039 - learning_rate: 4.7684e-10\n",
      "Epoch 293/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6675e-05 - mae: 0.0037 - val_loss: 6.9177e-05 - val_mae: 0.0039 - learning_rate: 4.7684e-10\n",
      "Epoch 294/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5521e-05 - mae: 0.0037 - val_loss: 6.9167e-05 - val_mae: 0.0039 - learning_rate: 4.7684e-10\n",
      "Epoch 295/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7200e-05 - mae: 0.0037 - val_loss: 6.9169e-05 - val_mae: 0.0039 - learning_rate: 4.7684e-10\n",
      "Epoch 296/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8687e-05 - mae: 0.0038 - val_loss: 6.9175e-05 - val_mae: 0.0039 - learning_rate: 4.7684e-10\n",
      "Epoch 297/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9041e-05 - mae: 0.0038 - val_loss: 6.9180e-05 - val_mae: 0.0039 - learning_rate: 4.7684e-10\n",
      "Epoch 298/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.7707e-05 - mae: 0.0037\n",
      "Epoch 298: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7706e-05 - mae: 0.0037 - val_loss: 6.9173e-05 - val_mae: 0.0039 - learning_rate: 4.7684e-10\n",
      "Epoch 299/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8930e-05 - mae: 0.0038 - val_loss: 6.9144e-05 - val_mae: 0.0039 - learning_rate: 2.3842e-10\n",
      "Epoch 300/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7603e-05 - mae: 0.0037 - val_loss: 6.9137e-05 - val_mae: 0.0039 - learning_rate: 2.3842e-10\n",
      "Epoch 301/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8523e-05 - mae: 0.0038 - val_loss: 6.9157e-05 - val_mae: 0.0039 - learning_rate: 2.3842e-10\n",
      "Epoch 302/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8097e-05 - mae: 0.0037 - val_loss: 6.9144e-05 - val_mae: 0.0039 - learning_rate: 2.3842e-10\n",
      "Epoch 303/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5854e-05 - mae: 0.0037 - val_loss: 6.9145e-05 - val_mae: 0.0039 - learning_rate: 2.3842e-10\n",
      "Epoch 304/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7523e-05 - mae: 0.0037 - val_loss: 6.9172e-05 - val_mae: 0.0039 - learning_rate: 2.3842e-10\n",
      "Epoch 305/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6026e-05 - mae: 0.0036 - val_loss: 6.9183e-05 - val_mae: 0.0039 - learning_rate: 2.3842e-10\n",
      "Epoch 306/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5300e-05 - mae: 0.0036 - val_loss: 6.9175e-05 - val_mae: 0.0039 - learning_rate: 2.3842e-10\n",
      "Epoch 307/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6168e-05 - mae: 0.0037 - val_loss: 6.9181e-05 - val_mae: 0.0039 - learning_rate: 2.3842e-10\n",
      "Epoch 308/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 7.2386e-05 - mae: 0.0039\n",
      "Epoch 308: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.2009e-05 - mae: 0.0039 - val_loss: 6.9186e-05 - val_mae: 0.0039 - learning_rate: 2.3842e-10\n",
      "Epoch 309/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1656e-05 - mae: 0.0039 - val_loss: 6.9187e-05 - val_mae: 0.0039 - learning_rate: 1.1921e-10\n",
      "Epoch 310/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6379e-05 - mae: 0.0037 - val_loss: 6.9210e-05 - val_mae: 0.0039 - learning_rate: 1.1921e-10\n",
      "Epoch 311/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8381e-05 - mae: 0.0038 - val_loss: 6.9207e-05 - val_mae: 0.0039 - learning_rate: 1.1921e-10\n",
      "Epoch 312/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0296e-05 - mae: 0.0038 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 1.1921e-10\n",
      "Epoch 313/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5334e-05 - mae: 0.0037 - val_loss: 6.9234e-05 - val_mae: 0.0039 - learning_rate: 1.1921e-10\n",
      "Epoch 314/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7248e-05 - mae: 0.0037 - val_loss: 6.9197e-05 - val_mae: 0.0039 - learning_rate: 1.1921e-10\n",
      "Epoch 315/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7495e-05 - mae: 0.0037 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 1.1921e-10\n",
      "Epoch 316/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8354e-05 - mae: 0.0037 - val_loss: 6.9231e-05 - val_mae: 0.0039 - learning_rate: 1.1921e-10\n",
      "Epoch 317/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6922e-05 - mae: 0.0037 - val_loss: 6.9240e-05 - val_mae: 0.0039 - learning_rate: 1.1921e-10\n",
      "Epoch 318/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.6049e-05 - mae: 0.0037\n",
      "Epoch 318: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6341e-05 - mae: 0.0037 - val_loss: 6.9260e-05 - val_mae: 0.0039 - learning_rate: 1.1921e-10\n",
      "Epoch 319/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8297e-05 - mae: 0.0038 - val_loss: 6.9236e-05 - val_mae: 0.0039 - learning_rate: 5.9605e-11\n",
      "Epoch 320/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7799e-05 - mae: 0.0038 - val_loss: 6.9208e-05 - val_mae: 0.0039 - learning_rate: 5.9605e-11\n",
      "Epoch 321/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6288e-05 - mae: 0.0037 - val_loss: 6.9181e-05 - val_mae: 0.0039 - learning_rate: 5.9605e-11\n",
      "Epoch 322/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7131e-05 - mae: 0.0037 - val_loss: 6.9160e-05 - val_mae: 0.0039 - learning_rate: 5.9605e-11\n",
      "Epoch 323/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8211e-05 - mae: 0.0038 - val_loss: 6.9135e-05 - val_mae: 0.0039 - learning_rate: 5.9605e-11\n",
      "Epoch 324/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6364e-05 - mae: 0.0037 - val_loss: 6.9143e-05 - val_mae: 0.0039 - learning_rate: 5.9605e-11\n",
      "Epoch 325/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8422e-05 - mae: 0.0037 - val_loss: 6.9174e-05 - val_mae: 0.0039 - learning_rate: 5.9605e-11\n",
      "Epoch 326/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9009e-05 - mae: 0.0038 - val_loss: 6.9153e-05 - val_mae: 0.0039 - learning_rate: 5.9605e-11\n",
      "Epoch 327/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8166e-05 - mae: 0.0038 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 5.9605e-11\n",
      "Epoch 328/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8693e-05 - mae: 0.0038\n",
      "Epoch 328: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8633e-05 - mae: 0.0038 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 5.9605e-11\n",
      "Epoch 329/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.4583e-05 - mae: 0.0036 - val_loss: 6.9231e-05 - val_mae: 0.0039 - learning_rate: 2.9802e-11\n",
      "Epoch 330/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6159e-05 - mae: 0.0037 - val_loss: 6.9241e-05 - val_mae: 0.0039 - learning_rate: 2.9802e-11\n",
      "Epoch 331/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1270e-05 - mae: 0.0039 - val_loss: 6.9205e-05 - val_mae: 0.0039 - learning_rate: 2.9802e-11\n",
      "Epoch 332/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.3654e-05 - mae: 0.0036 - val_loss: 6.9206e-05 - val_mae: 0.0039 - learning_rate: 2.9802e-11\n",
      "Epoch 333/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6580e-05 - mae: 0.0037 - val_loss: 6.9206e-05 - val_mae: 0.0039 - learning_rate: 2.9802e-11\n",
      "Epoch 334/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6532e-05 - mae: 0.0037 - val_loss: 6.9206e-05 - val_mae: 0.0039 - learning_rate: 2.9802e-11\n",
      "Epoch 335/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6516e-05 - mae: 0.0037 - val_loss: 6.9206e-05 - val_mae: 0.0039 - learning_rate: 2.9802e-11\n",
      "Epoch 336/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5004e-05 - mae: 0.0036 - val_loss: 6.9221e-05 - val_mae: 0.0039 - learning_rate: 2.9802e-11\n",
      "Epoch 337/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0468e-05 - mae: 0.0038 - val_loss: 6.9254e-05 - val_mae: 0.0039 - learning_rate: 2.9802e-11\n",
      "Epoch 338/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8473e-05 - mae: 0.0037\n",
      "Epoch 338: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8444e-05 - mae: 0.0037 - val_loss: 6.9248e-05 - val_mae: 0.0039 - learning_rate: 2.9802e-11\n",
      "Epoch 339/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0322e-05 - mae: 0.0038 - val_loss: 6.9229e-05 - val_mae: 0.0039 - learning_rate: 1.4901e-11\n",
      "Epoch 340/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8020e-05 - mae: 0.0038 - val_loss: 6.9248e-05 - val_mae: 0.0039 - learning_rate: 1.4901e-11\n",
      "Epoch 341/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.2891e-05 - mae: 0.0039 - val_loss: 6.9249e-05 - val_mae: 0.0039 - learning_rate: 1.4901e-11\n",
      "Epoch 342/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8184e-05 - mae: 0.0037 - val_loss: 6.9240e-05 - val_mae: 0.0039 - learning_rate: 1.4901e-11\n",
      "Epoch 343/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6009e-05 - mae: 0.0037 - val_loss: 6.9248e-05 - val_mae: 0.0039 - learning_rate: 1.4901e-11\n",
      "Epoch 344/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7361e-05 - mae: 0.0038 - val_loss: 6.9250e-05 - val_mae: 0.0039 - learning_rate: 1.4901e-11\n",
      "Epoch 345/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9877e-05 - mae: 0.0038 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 1.4901e-11\n",
      "Epoch 346/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8465e-05 - mae: 0.0038 - val_loss: 6.9194e-05 - val_mae: 0.0039 - learning_rate: 1.4901e-11\n",
      "Epoch 347/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7657e-05 - mae: 0.0037 - val_loss: 6.9172e-05 - val_mae: 0.0039 - learning_rate: 1.4901e-11\n",
      "Epoch 348/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8871e-05 - mae: 0.0037\n",
      "Epoch 348: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8750e-05 - mae: 0.0037 - val_loss: 6.9154e-05 - val_mae: 0.0039 - learning_rate: 1.4901e-11\n",
      "Epoch 349/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6115e-05 - mae: 0.0037 - val_loss: 6.9132e-05 - val_mae: 0.0039 - learning_rate: 7.4506e-12\n",
      "Epoch 350/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8664e-05 - mae: 0.0038 - val_loss: 6.9125e-05 - val_mae: 0.0039 - learning_rate: 7.4506e-12\n",
      "Epoch 351/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7668e-05 - mae: 0.0037 - val_loss: 6.9119e-05 - val_mae: 0.0039 - learning_rate: 7.4506e-12\n",
      "Epoch 352/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7343e-05 - mae: 0.0037 - val_loss: 6.9136e-05 - val_mae: 0.0039 - learning_rate: 7.4506e-12\n",
      "Epoch 353/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7363e-05 - mae: 0.0037 - val_loss: 6.9149e-05 - val_mae: 0.0039 - learning_rate: 7.4506e-12\n",
      "Epoch 354/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6736e-05 - mae: 0.0037 - val_loss: 6.9157e-05 - val_mae: 0.0039 - learning_rate: 7.4506e-12\n",
      "Epoch 355/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5269e-05 - mae: 0.0037 - val_loss: 6.9128e-05 - val_mae: 0.0039 - learning_rate: 7.4506e-12\n",
      "Epoch 356/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5728e-05 - mae: 0.0036 - val_loss: 6.9123e-05 - val_mae: 0.0039 - learning_rate: 7.4506e-12\n",
      "Epoch 357/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5422e-05 - mae: 0.0037 - val_loss: 6.9129e-05 - val_mae: 0.0039 - learning_rate: 7.4506e-12\n",
      "Epoch 358/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.6602e-05 - mae: 0.0037\n",
      "Epoch 358: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6782e-05 - mae: 0.0037 - val_loss: 6.9143e-05 - val_mae: 0.0039 - learning_rate: 7.4506e-12\n",
      "Epoch 359/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0129e-05 - mae: 0.0038 - val_loss: 6.9154e-05 - val_mae: 0.0039 - learning_rate: 3.7253e-12\n",
      "Epoch 360/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8175e-05 - mae: 0.0038 - val_loss: 6.9178e-05 - val_mae: 0.0039 - learning_rate: 3.7253e-12\n",
      "Epoch 361/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6313e-05 - mae: 0.0037 - val_loss: 6.9169e-05 - val_mae: 0.0039 - learning_rate: 3.7253e-12\n",
      "Epoch 362/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7720e-05 - mae: 0.0037 - val_loss: 6.9169e-05 - val_mae: 0.0039 - learning_rate: 3.7253e-12\n",
      "Epoch 363/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6366e-05 - mae: 0.0037 - val_loss: 6.9171e-05 - val_mae: 0.0039 - learning_rate: 3.7253e-12\n",
      "Epoch 364/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9756e-05 - mae: 0.0038 - val_loss: 6.9185e-05 - val_mae: 0.0039 - learning_rate: 3.7253e-12\n",
      "Epoch 365/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6764e-05 - mae: 0.0037 - val_loss: 6.9194e-05 - val_mae: 0.0039 - learning_rate: 3.7253e-12\n",
      "Epoch 366/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5473e-05 - mae: 0.0037 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 3.7253e-12\n",
      "Epoch 367/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7292e-05 - mae: 0.0037 - val_loss: 6.9222e-05 - val_mae: 0.0039 - learning_rate: 3.7253e-12\n",
      "Epoch 368/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8365e-05 - mae: 0.0038\n",
      "Epoch 368: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8327e-05 - mae: 0.0038 - val_loss: 6.9201e-05 - val_mae: 0.0039 - learning_rate: 3.7253e-12\n",
      "Epoch 369/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6884e-05 - mae: 0.0037 - val_loss: 6.9216e-05 - val_mae: 0.0039 - learning_rate: 1.8626e-12\n",
      "Epoch 370/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9879e-05 - mae: 0.0039 - val_loss: 6.9239e-05 - val_mae: 0.0039 - learning_rate: 1.8626e-12\n",
      "Epoch 371/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6669e-05 - mae: 0.0037 - val_loss: 6.9233e-05 - val_mae: 0.0039 - learning_rate: 1.8626e-12\n",
      "Epoch 372/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5326e-05 - mae: 0.0036 - val_loss: 6.9246e-05 - val_mae: 0.0039 - learning_rate: 1.8626e-12\n",
      "Epoch 373/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8891e-05 - mae: 0.0038 - val_loss: 6.9218e-05 - val_mae: 0.0039 - learning_rate: 1.8626e-12\n",
      "Epoch 374/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9310e-05 - mae: 0.0038 - val_loss: 6.9227e-05 - val_mae: 0.0039 - learning_rate: 1.8626e-12\n",
      "Epoch 375/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7211e-05 - mae: 0.0037 - val_loss: 6.9252e-05 - val_mae: 0.0039 - learning_rate: 1.8626e-12\n",
      "Epoch 376/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6604e-05 - mae: 0.0037 - val_loss: 6.9233e-05 - val_mae: 0.0039 - learning_rate: 1.8626e-12\n",
      "Epoch 377/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7514e-05 - mae: 0.0037 - val_loss: 6.9205e-05 - val_mae: 0.0039 - learning_rate: 1.8626e-12\n",
      "Epoch 378/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.7991e-05 - mae: 0.0038\n",
      "Epoch 378: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7996e-05 - mae: 0.0038 - val_loss: 6.9197e-05 - val_mae: 0.0039 - learning_rate: 1.8626e-12\n",
      "Epoch 379/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.4823e-05 - mae: 0.0036 - val_loss: 6.9211e-05 - val_mae: 0.0039 - learning_rate: 9.3132e-13\n",
      "Epoch 380/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6977e-05 - mae: 0.0037 - val_loss: 6.9204e-05 - val_mae: 0.0039 - learning_rate: 9.3132e-13\n",
      "Epoch 381/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7885e-05 - mae: 0.0037 - val_loss: 6.9221e-05 - val_mae: 0.0039 - learning_rate: 9.3132e-13\n",
      "Epoch 382/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.4591e-05 - mae: 0.0036 - val_loss: 6.9253e-05 - val_mae: 0.0039 - learning_rate: 9.3132e-13\n",
      "Epoch 383/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6315e-05 - mae: 0.0037 - val_loss: 6.9270e-05 - val_mae: 0.0039 - learning_rate: 9.3132e-13\n",
      "Epoch 384/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4912e-05 - mae: 0.0036 - val_loss: 6.9276e-05 - val_mae: 0.0039 - learning_rate: 9.3132e-13\n",
      "Epoch 385/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9567e-05 - mae: 0.0038 - val_loss: 6.9249e-05 - val_mae: 0.0039 - learning_rate: 9.3132e-13\n",
      "Epoch 386/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7757e-05 - mae: 0.0037 - val_loss: 6.9247e-05 - val_mae: 0.0039 - learning_rate: 9.3132e-13\n",
      "Epoch 387/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6463e-05 - mae: 0.0037 - val_loss: 6.9265e-05 - val_mae: 0.0039 - learning_rate: 9.3132e-13\n",
      "Epoch 388/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8164e-05 - mae: 0.0037\n",
      "Epoch 388: ReduceLROnPlateau reducing learning rate to 4.656613094254636e-13.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8085e-05 - mae: 0.0037 - val_loss: 6.9251e-05 - val_mae: 0.0039 - learning_rate: 9.3132e-13\n",
      "Epoch 389/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8364e-05 - mae: 0.0038 - val_loss: 6.9239e-05 - val_mae: 0.0039 - learning_rate: 4.6566e-13\n",
      "Epoch 390/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9546e-05 - mae: 0.0038 - val_loss: 6.9228e-05 - val_mae: 0.0039 - learning_rate: 4.6566e-13\n",
      "Epoch 391/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7268e-05 - mae: 0.0037 - val_loss: 6.9226e-05 - val_mae: 0.0039 - learning_rate: 4.6566e-13\n",
      "Epoch 392/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9493e-05 - mae: 0.0038 - val_loss: 6.9224e-05 - val_mae: 0.0039 - learning_rate: 4.6566e-13\n",
      "Epoch 393/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5493e-05 - mae: 0.0036 - val_loss: 6.9192e-05 - val_mae: 0.0039 - learning_rate: 4.6566e-13\n",
      "Epoch 394/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1331e-05 - mae: 0.0039 - val_loss: 6.9216e-05 - val_mae: 0.0039 - learning_rate: 4.6566e-13\n",
      "Epoch 395/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6943e-05 - mae: 0.0037 - val_loss: 6.9208e-05 - val_mae: 0.0039 - learning_rate: 4.6566e-13\n",
      "Epoch 396/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9624e-05 - mae: 0.0038 - val_loss: 6.9234e-05 - val_mae: 0.0039 - learning_rate: 4.6566e-13\n",
      "Epoch 397/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5783e-05 - mae: 0.0037 - val_loss: 6.9231e-05 - val_mae: 0.0039 - learning_rate: 4.6566e-13\n",
      "Epoch 398/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.6719e-05 - mae: 0.0037\n",
      "Epoch 398: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6796e-05 - mae: 0.0037 - val_loss: 6.9205e-05 - val_mae: 0.0039 - learning_rate: 4.6566e-13\n",
      "Epoch 399/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5588e-05 - mae: 0.0037 - val_loss: 6.9186e-05 - val_mae: 0.0039 - learning_rate: 2.3283e-13\n",
      "Epoch 400/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9896e-05 - mae: 0.0038 - val_loss: 6.9183e-05 - val_mae: 0.0039 - learning_rate: 2.3283e-13\n",
      "Epoch 401/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7647e-05 - mae: 0.0037 - val_loss: 6.9178e-05 - val_mae: 0.0039 - learning_rate: 2.3283e-13\n",
      "Epoch 402/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7100e-05 - mae: 0.0037 - val_loss: 6.9176e-05 - val_mae: 0.0039 - learning_rate: 2.3283e-13\n",
      "Epoch 403/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0352e-05 - mae: 0.0039 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 2.3283e-13\n",
      "Epoch 404/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9208e-05 - mae: 0.0038 - val_loss: 6.9165e-05 - val_mae: 0.0039 - learning_rate: 2.3283e-13\n",
      "Epoch 405/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8151e-05 - mae: 0.0038 - val_loss: 6.9185e-05 - val_mae: 0.0039 - learning_rate: 2.3283e-13\n",
      "Epoch 406/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8903e-05 - mae: 0.0038 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 2.3283e-13\n",
      "Epoch 407/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0168e-05 - mae: 0.0038 - val_loss: 6.9186e-05 - val_mae: 0.0039 - learning_rate: 2.3283e-13\n",
      "Epoch 408/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.5055e-05 - mae: 0.0037\n",
      "Epoch 408: ReduceLROnPlateau reducing learning rate to 1.164153273563659e-13.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5336e-05 - mae: 0.0037 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 2.3283e-13\n",
      "Epoch 409/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4548e-05 - mae: 0.0036 - val_loss: 6.9182e-05 - val_mae: 0.0039 - learning_rate: 1.1642e-13\n",
      "Epoch 410/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7307e-05 - mae: 0.0037 - val_loss: 6.9197e-05 - val_mae: 0.0039 - learning_rate: 1.1642e-13\n",
      "Epoch 411/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1897e-05 - mae: 0.0039 - val_loss: 6.9201e-05 - val_mae: 0.0039 - learning_rate: 1.1642e-13\n",
      "Epoch 412/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7016e-05 - mae: 0.0037 - val_loss: 6.9159e-05 - val_mae: 0.0039 - learning_rate: 1.1642e-13\n",
      "Epoch 413/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1464e-05 - mae: 0.0039 - val_loss: 6.9152e-05 - val_mae: 0.0039 - learning_rate: 1.1642e-13\n",
      "Epoch 414/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9850e-05 - mae: 0.0038 - val_loss: 6.9152e-05 - val_mae: 0.0039 - learning_rate: 1.1642e-13\n",
      "Epoch 415/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8005e-05 - mae: 0.0037 - val_loss: 6.9147e-05 - val_mae: 0.0039 - learning_rate: 1.1642e-13\n",
      "Epoch 416/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5750e-05 - mae: 0.0036 - val_loss: 6.9160e-05 - val_mae: 0.0039 - learning_rate: 1.1642e-13\n",
      "Epoch 417/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5637e-05 - mae: 0.0036 - val_loss: 6.9155e-05 - val_mae: 0.0039 - learning_rate: 1.1642e-13\n",
      "Epoch 418/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.3542e-05 - mae: 0.0039\n",
      "Epoch 418: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.3028e-05 - mae: 0.0039 - val_loss: 6.9165e-05 - val_mae: 0.0039 - learning_rate: 1.1642e-13\n",
      "Epoch 419/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4910e-05 - mae: 0.0036 - val_loss: 6.9163e-05 - val_mae: 0.0039 - learning_rate: 5.8208e-14\n",
      "Epoch 420/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9020e-05 - mae: 0.0038 - val_loss: 6.9173e-05 - val_mae: 0.0039 - learning_rate: 5.8208e-14\n",
      "Epoch 421/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.3980e-05 - mae: 0.0036 - val_loss: 6.9182e-05 - val_mae: 0.0039 - learning_rate: 5.8208e-14\n",
      "Epoch 422/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0869e-05 - mae: 0.0038 - val_loss: 6.9197e-05 - val_mae: 0.0039 - learning_rate: 5.8208e-14\n",
      "Epoch 423/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9899e-05 - mae: 0.0038 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 5.8208e-14\n",
      "Epoch 424/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6963e-05 - mae: 0.0037 - val_loss: 6.9215e-05 - val_mae: 0.0039 - learning_rate: 5.8208e-14\n",
      "Epoch 425/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6292e-05 - mae: 0.0036 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 5.8208e-14\n",
      "Epoch 426/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5635e-05 - mae: 0.0036 - val_loss: 6.9235e-05 - val_mae: 0.0039 - learning_rate: 5.8208e-14\n",
      "Epoch 427/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7573e-05 - mae: 0.0038 - val_loss: 6.9203e-05 - val_mae: 0.0039 - learning_rate: 5.8208e-14\n",
      "Epoch 428/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.6984e-05 - mae: 0.0037\n",
      "Epoch 428: ReduceLROnPlateau reducing learning rate to 2.9103831839091474e-14.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7035e-05 - mae: 0.0037 - val_loss: 6.9193e-05 - val_mae: 0.0039 - learning_rate: 5.8208e-14\n",
      "Epoch 429/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5043e-05 - mae: 0.0037 - val_loss: 6.9206e-05 - val_mae: 0.0039 - learning_rate: 2.9104e-14\n",
      "Epoch 430/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7114e-05 - mae: 0.0036 - val_loss: 6.9196e-05 - val_mae: 0.0039 - learning_rate: 2.9104e-14\n",
      "Epoch 431/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8566e-05 - mae: 0.0038 - val_loss: 6.9190e-05 - val_mae: 0.0039 - learning_rate: 2.9104e-14\n",
      "Epoch 432/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6170e-05 - mae: 0.0037 - val_loss: 6.9195e-05 - val_mae: 0.0039 - learning_rate: 2.9104e-14\n",
      "Epoch 433/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7838e-05 - mae: 0.0037 - val_loss: 6.9204e-05 - val_mae: 0.0039 - learning_rate: 2.9104e-14\n",
      "Epoch 434/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.5746e-05 - mae: 0.0037 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 2.9104e-14\n",
      "Epoch 435/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9176e-05 - mae: 0.0038 - val_loss: 6.9203e-05 - val_mae: 0.0039 - learning_rate: 2.9104e-14\n",
      "Epoch 436/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8588e-05 - mae: 0.0037 - val_loss: 6.9223e-05 - val_mae: 0.0039 - learning_rate: 2.9104e-14\n",
      "Epoch 437/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8289e-05 - mae: 0.0037 - val_loss: 6.9228e-05 - val_mae: 0.0039 - learning_rate: 2.9104e-14\n",
      "Epoch 438/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.4226e-05 - mae: 0.0036\n",
      "Epoch 438: ReduceLROnPlateau reducing learning rate to 1.4551915919545737e-14.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4524e-05 - mae: 0.0036 - val_loss: 6.9208e-05 - val_mae: 0.0039 - learning_rate: 2.9104e-14\n",
      "Epoch 439/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4771e-05 - mae: 0.0036 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 1.4552e-14\n",
      "Epoch 440/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9489e-05 - mae: 0.0038 - val_loss: 6.9226e-05 - val_mae: 0.0039 - learning_rate: 1.4552e-14\n",
      "Epoch 441/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7731e-05 - mae: 0.0037 - val_loss: 6.9224e-05 - val_mae: 0.0039 - learning_rate: 1.4552e-14\n",
      "Epoch 442/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7511e-05 - mae: 0.0037 - val_loss: 6.9230e-05 - val_mae: 0.0039 - learning_rate: 1.4552e-14\n",
      "Epoch 443/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9917e-05 - mae: 0.0038 - val_loss: 6.9203e-05 - val_mae: 0.0039 - learning_rate: 1.4552e-14\n",
      "Epoch 444/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8713e-05 - mae: 0.0038 - val_loss: 6.9186e-05 - val_mae: 0.0039 - learning_rate: 1.4552e-14\n",
      "Epoch 445/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6250e-05 - mae: 0.0036 - val_loss: 6.9173e-05 - val_mae: 0.0039 - learning_rate: 1.4552e-14\n",
      "Epoch 446/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5701e-05 - mae: 0.0037 - val_loss: 6.9175e-05 - val_mae: 0.0039 - learning_rate: 1.4552e-14\n",
      "Epoch 447/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7213e-05 - mae: 0.0038 - val_loss: 6.9194e-05 - val_mae: 0.0039 - learning_rate: 1.4552e-14\n",
      "Epoch 448/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.4888e-05 - mae: 0.0036\n",
      "Epoch 448: ReduceLROnPlateau reducing learning rate to 7.275957959772868e-15.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5196e-05 - mae: 0.0036 - val_loss: 6.9157e-05 - val_mae: 0.0039 - learning_rate: 1.4552e-14\n",
      "Epoch 449/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5436e-05 - mae: 0.0037 - val_loss: 6.9169e-05 - val_mae: 0.0039 - learning_rate: 7.2760e-15\n",
      "Epoch 450/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6018e-05 - mae: 0.0037 - val_loss: 6.9167e-05 - val_mae: 0.0039 - learning_rate: 7.2760e-15\n",
      "Epoch 451/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6582e-05 - mae: 0.0037 - val_loss: 6.9178e-05 - val_mae: 0.0039 - learning_rate: 7.2760e-15\n",
      "Epoch 452/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9531e-05 - mae: 0.0038 - val_loss: 6.9178e-05 - val_mae: 0.0039 - learning_rate: 7.2760e-15\n",
      "Epoch 453/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0667e-05 - mae: 0.0038 - val_loss: 6.9190e-05 - val_mae: 0.0039 - learning_rate: 7.2760e-15\n",
      "Epoch 454/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7598e-05 - mae: 0.0037 - val_loss: 6.9192e-05 - val_mae: 0.0039 - learning_rate: 7.2760e-15\n",
      "Epoch 455/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4888e-05 - mae: 0.0036 - val_loss: 6.9189e-05 - val_mae: 0.0039 - learning_rate: 7.2760e-15\n",
      "Epoch 456/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8624e-05 - mae: 0.0038 - val_loss: 6.9182e-05 - val_mae: 0.0039 - learning_rate: 7.2760e-15\n",
      "Epoch 457/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7111e-05 - mae: 0.0037 - val_loss: 6.9190e-05 - val_mae: 0.0039 - learning_rate: 7.2760e-15\n",
      "Epoch 458/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.0758e-05 - mae: 0.0038\n",
      "Epoch 458: ReduceLROnPlateau reducing learning rate to 3.637978979886434e-15.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0482e-05 - mae: 0.0038 - val_loss: 6.9175e-05 - val_mae: 0.0039 - learning_rate: 7.2760e-15\n",
      "Epoch 459/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1645e-05 - mae: 0.0039 - val_loss: 6.9144e-05 - val_mae: 0.0039 - learning_rate: 3.6380e-15\n",
      "Epoch 460/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.4861e-05 - mae: 0.0036 - val_loss: 6.9144e-05 - val_mae: 0.0039 - learning_rate: 3.6380e-15\n",
      "Epoch 461/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6683e-05 - mae: 0.0037 - val_loss: 6.9160e-05 - val_mae: 0.0039 - learning_rate: 3.6380e-15\n",
      "Epoch 462/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0125e-05 - mae: 0.0038 - val_loss: 6.9131e-05 - val_mae: 0.0039 - learning_rate: 3.6380e-15\n",
      "Epoch 463/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9515e-05 - mae: 0.0038 - val_loss: 6.9153e-05 - val_mae: 0.0039 - learning_rate: 3.6380e-15\n",
      "Epoch 464/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6824e-05 - mae: 0.0037 - val_loss: 6.9183e-05 - val_mae: 0.0039 - learning_rate: 3.6380e-15\n",
      "Epoch 465/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9939e-05 - mae: 0.0038 - val_loss: 6.9178e-05 - val_mae: 0.0039 - learning_rate: 3.6380e-15\n",
      "Epoch 466/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7272e-05 - mae: 0.0037 - val_loss: 6.9174e-05 - val_mae: 0.0039 - learning_rate: 3.6380e-15\n",
      "Epoch 467/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9730e-05 - mae: 0.0038 - val_loss: 6.9188e-05 - val_mae: 0.0039 - learning_rate: 3.6380e-15\n",
      "Epoch 468/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.9099e-05 - mae: 0.0038\n",
      "Epoch 468: ReduceLROnPlateau reducing learning rate to 1.818989489943217e-15.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9008e-05 - mae: 0.0038 - val_loss: 6.9181e-05 - val_mae: 0.0039 - learning_rate: 3.6380e-15\n",
      "Epoch 469/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.2465e-05 - mae: 0.0039 - val_loss: 6.9160e-05 - val_mae: 0.0039 - learning_rate: 1.8190e-15\n",
      "Epoch 470/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.2751e-05 - mae: 0.0039 - val_loss: 6.9132e-05 - val_mae: 0.0039 - learning_rate: 1.8190e-15\n",
      "Epoch 471/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7776e-05 - mae: 0.0038 - val_loss: 6.9138e-05 - val_mae: 0.0039 - learning_rate: 1.8190e-15\n",
      "Epoch 472/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8505e-05 - mae: 0.0037 - val_loss: 6.9163e-05 - val_mae: 0.0039 - learning_rate: 1.8190e-15\n",
      "Epoch 473/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7254e-05 - mae: 0.0037 - val_loss: 6.9160e-05 - val_mae: 0.0039 - learning_rate: 1.8190e-15\n",
      "Epoch 474/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7268e-05 - mae: 0.0037 - val_loss: 6.9152e-05 - val_mae: 0.0039 - learning_rate: 1.8190e-15\n",
      "Epoch 475/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.1030e-05 - mae: 0.0039 - val_loss: 6.9126e-05 - val_mae: 0.0039 - learning_rate: 1.8190e-15\n",
      "Epoch 476/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4385e-05 - mae: 0.0036 - val_loss: 6.9133e-05 - val_mae: 0.0039 - learning_rate: 1.8190e-15\n",
      "Epoch 477/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6306e-05 - mae: 0.0037 - val_loss: 6.9163e-05 - val_mae: 0.0039 - learning_rate: 1.8190e-15\n",
      "Epoch 478/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.6134e-05 - mae: 0.0037\n",
      "Epoch 478: ReduceLROnPlateau reducing learning rate to 9.094947449716085e-16.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6340e-05 - mae: 0.0037 - val_loss: 6.9170e-05 - val_mae: 0.0039 - learning_rate: 1.8190e-15\n",
      "Epoch 479/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7949e-05 - mae: 0.0037 - val_loss: 6.9152e-05 - val_mae: 0.0039 - learning_rate: 9.0949e-16\n",
      "Epoch 480/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6138e-05 - mae: 0.0037 - val_loss: 6.9166e-05 - val_mae: 0.0039 - learning_rate: 9.0949e-16\n",
      "Epoch 481/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8137e-05 - mae: 0.0038 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 9.0949e-16\n",
      "Epoch 482/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9517e-05 - mae: 0.0038 - val_loss: 6.9180e-05 - val_mae: 0.0039 - learning_rate: 9.0949e-16\n",
      "Epoch 483/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5630e-05 - mae: 0.0037 - val_loss: 6.9200e-05 - val_mae: 0.0039 - learning_rate: 9.0949e-16\n",
      "Epoch 484/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7424e-05 - mae: 0.0037 - val_loss: 6.9200e-05 - val_mae: 0.0039 - learning_rate: 9.0949e-16\n",
      "Epoch 485/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5796e-05 - mae: 0.0037 - val_loss: 6.9207e-05 - val_mae: 0.0039 - learning_rate: 9.0949e-16\n",
      "Epoch 486/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.1742e-05 - mae: 0.0038 - val_loss: 6.9202e-05 - val_mae: 0.0039 - learning_rate: 9.0949e-16\n",
      "Epoch 487/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4386e-05 - mae: 0.0036 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 9.0949e-16\n",
      "Epoch 488/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.5847e-05 - mae: 0.0036\n",
      "Epoch 488: ReduceLROnPlateau reducing learning rate to 4.547473724858043e-16.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5990e-05 - mae: 0.0036 - val_loss: 6.9222e-05 - val_mae: 0.0039 - learning_rate: 9.0949e-16\n",
      "Epoch 489/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5938e-05 - mae: 0.0036 - val_loss: 6.9199e-05 - val_mae: 0.0039 - learning_rate: 4.5475e-16\n",
      "Epoch 490/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7273e-05 - mae: 0.0037 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 4.5475e-16\n",
      "Epoch 491/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0707e-05 - mae: 0.0039 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 4.5475e-16\n",
      "Epoch 492/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8813e-05 - mae: 0.0038 - val_loss: 6.9200e-05 - val_mae: 0.0039 - learning_rate: 4.5475e-16\n",
      "Epoch 493/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9253e-05 - mae: 0.0038 - val_loss: 6.9224e-05 - val_mae: 0.0039 - learning_rate: 4.5475e-16\n",
      "Epoch 494/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9041e-05 - mae: 0.0038 - val_loss: 6.9232e-05 - val_mae: 0.0039 - learning_rate: 4.5475e-16\n",
      "Epoch 495/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8407e-05 - mae: 0.0037 - val_loss: 6.9238e-05 - val_mae: 0.0039 - learning_rate: 4.5475e-16\n",
      "Epoch 496/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7176e-05 - mae: 0.0037 - val_loss: 6.9252e-05 - val_mae: 0.0039 - learning_rate: 4.5475e-16\n",
      "Epoch 497/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0186e-05 - mae: 0.0038 - val_loss: 6.9260e-05 - val_mae: 0.0039 - learning_rate: 4.5475e-16\n",
      "Epoch 498/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.4020e-05 - mae: 0.0036\n",
      "Epoch 498: ReduceLROnPlateau reducing learning rate to 2.2737368624290214e-16.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4347e-05 - mae: 0.0036 - val_loss: 6.9254e-05 - val_mae: 0.0039 - learning_rate: 4.5475e-16\n",
      "Epoch 499/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5674e-05 - mae: 0.0036 - val_loss: 6.9246e-05 - val_mae: 0.0039 - learning_rate: 2.2737e-16\n",
      "Epoch 500/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8812e-05 - mae: 0.0038 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 2.2737e-16\n",
      "Epoch 501/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5329e-05 - mae: 0.0036 - val_loss: 6.9195e-05 - val_mae: 0.0039 - learning_rate: 2.2737e-16\n",
      "Epoch 502/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5101e-05 - mae: 0.0037 - val_loss: 6.9188e-05 - val_mae: 0.0039 - learning_rate: 2.2737e-16\n",
      "Epoch 503/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9196e-05 - mae: 0.0038 - val_loss: 6.9195e-05 - val_mae: 0.0039 - learning_rate: 2.2737e-16\n",
      "Epoch 504/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6113e-05 - mae: 0.0037 - val_loss: 6.9166e-05 - val_mae: 0.0039 - learning_rate: 2.2737e-16\n",
      "Epoch 505/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6039e-05 - mae: 0.0037 - val_loss: 6.9178e-05 - val_mae: 0.0039 - learning_rate: 2.2737e-16\n",
      "Epoch 506/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8401e-05 - mae: 0.0037 - val_loss: 6.9157e-05 - val_mae: 0.0039 - learning_rate: 2.2737e-16\n",
      "Epoch 507/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7019e-05 - mae: 0.0037 - val_loss: 6.9153e-05 - val_mae: 0.0039 - learning_rate: 2.2737e-16\n",
      "Epoch 508/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.8448e-05 - mae: 0.0038\n",
      "Epoch 508: ReduceLROnPlateau reducing learning rate to 1.1368684312145107e-16.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8364e-05 - mae: 0.0038 - val_loss: 6.9137e-05 - val_mae: 0.0039 - learning_rate: 2.2737e-16\n",
      "Epoch 509/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6190e-05 - mae: 0.0037 - val_loss: 6.9138e-05 - val_mae: 0.0039 - learning_rate: 1.1369e-16\n",
      "Epoch 510/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7716e-05 - mae: 0.0038 - val_loss: 6.9141e-05 - val_mae: 0.0039 - learning_rate: 1.1369e-16\n",
      "Epoch 511/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4695e-05 - mae: 0.0036 - val_loss: 6.9160e-05 - val_mae: 0.0039 - learning_rate: 1.1369e-16\n",
      "Epoch 512/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5320e-05 - mae: 0.0036 - val_loss: 6.9173e-05 - val_mae: 0.0039 - learning_rate: 1.1369e-16\n",
      "Epoch 513/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6904e-05 - mae: 0.0037 - val_loss: 6.9170e-05 - val_mae: 0.0039 - learning_rate: 1.1369e-16\n",
      "Epoch 514/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.5306e-05 - mae: 0.0037 - val_loss: 6.9166e-05 - val_mae: 0.0039 - learning_rate: 1.1369e-16\n",
      "Epoch 515/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9897e-05 - mae: 0.0038 - val_loss: 6.9174e-05 - val_mae: 0.0039 - learning_rate: 1.1369e-16\n",
      "Epoch 516/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5966e-05 - mae: 0.0037 - val_loss: 6.9175e-05 - val_mae: 0.0039 - learning_rate: 1.1369e-16\n",
      "Epoch 517/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8261e-05 - mae: 0.0038 - val_loss: 6.9179e-05 - val_mae: 0.0039 - learning_rate: 1.1369e-16\n",
      "Epoch 518/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.4365e-05 - mae: 0.0036\n",
      "Epoch 518: ReduceLROnPlateau reducing learning rate to 5.684342156072553e-17.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4684e-05 - mae: 0.0036 - val_loss: 6.9179e-05 - val_mae: 0.0039 - learning_rate: 1.1369e-16\n",
      "Epoch 519/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8766e-05 - mae: 0.0037 - val_loss: 6.9169e-05 - val_mae: 0.0039 - learning_rate: 5.6843e-17\n",
      "Epoch 520/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6269e-05 - mae: 0.0037 - val_loss: 6.9188e-05 - val_mae: 0.0039 - learning_rate: 5.6843e-17\n",
      "Epoch 521/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7874e-05 - mae: 0.0037 - val_loss: 6.9206e-05 - val_mae: 0.0039 - learning_rate: 5.6843e-17\n",
      "Epoch 522/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.4427e-05 - mae: 0.0036 - val_loss: 6.9192e-05 - val_mae: 0.0039 - learning_rate: 5.6843e-17\n",
      "Epoch 523/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9605e-05 - mae: 0.0038 - val_loss: 6.9200e-05 - val_mae: 0.0039 - learning_rate: 5.6843e-17\n",
      "Epoch 524/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5370e-05 - mae: 0.0036 - val_loss: 6.9208e-05 - val_mae: 0.0039 - learning_rate: 5.6843e-17\n",
      "Epoch 525/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4498e-05 - mae: 0.0036 - val_loss: 6.9193e-05 - val_mae: 0.0039 - learning_rate: 5.6843e-17\n",
      "Epoch 526/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7989e-05 - mae: 0.0037 - val_loss: 6.9201e-05 - val_mae: 0.0039 - learning_rate: 5.6843e-17\n",
      "Epoch 527/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5533e-05 - mae: 0.0037 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 5.6843e-17\n",
      "Epoch 528/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.6623e-05 - mae: 0.0037\n",
      "Epoch 528: ReduceLROnPlateau reducing learning rate to 2.842171078036277e-17.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6750e-05 - mae: 0.0037 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 5.6843e-17\n",
      "Epoch 529/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8888e-05 - mae: 0.0038 - val_loss: 6.9215e-05 - val_mae: 0.0039 - learning_rate: 2.8422e-17\n",
      "Epoch 530/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1110e-05 - mae: 0.0038 - val_loss: 6.9233e-05 - val_mae: 0.0039 - learning_rate: 2.8422e-17\n",
      "Epoch 531/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8936e-05 - mae: 0.0038 - val_loss: 6.9228e-05 - val_mae: 0.0039 - learning_rate: 2.8422e-17\n",
      "Epoch 532/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0714e-05 - mae: 0.0038 - val_loss: 6.9236e-05 - val_mae: 0.0039 - learning_rate: 2.8422e-17\n",
      "Epoch 533/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.8219e-05 - mae: 0.0037 - val_loss: 6.9215e-05 - val_mae: 0.0039 - learning_rate: 2.8422e-17\n",
      "Epoch 534/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6288e-05 - mae: 0.0037 - val_loss: 6.9241e-05 - val_mae: 0.0039 - learning_rate: 2.8422e-17\n",
      "Epoch 535/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.3600e-05 - mae: 0.0036 - val_loss: 6.9221e-05 - val_mae: 0.0039 - learning_rate: 2.8422e-17\n",
      "Epoch 536/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.2174e-05 - mae: 0.0039 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 2.8422e-17\n",
      "Epoch 537/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6220e-05 - mae: 0.0037 - val_loss: 6.9185e-05 - val_mae: 0.0039 - learning_rate: 2.8422e-17\n",
      "Epoch 538/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.7652e-05 - mae: 0.0037\n",
      "Epoch 538: ReduceLROnPlateau reducing learning rate to 1.4210855390181384e-17.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7637e-05 - mae: 0.0037 - val_loss: 6.9179e-05 - val_mae: 0.0039 - learning_rate: 2.8422e-17\n",
      "Epoch 539/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7015e-05 - mae: 0.0037 - val_loss: 6.9155e-05 - val_mae: 0.0039 - learning_rate: 1.4211e-17\n",
      "Epoch 540/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7988e-05 - mae: 0.0038 - val_loss: 6.9198e-05 - val_mae: 0.0039 - learning_rate: 1.4211e-17\n",
      "Epoch 541/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9211e-05 - mae: 0.0038 - val_loss: 6.9201e-05 - val_mae: 0.0039 - learning_rate: 1.4211e-17\n",
      "Epoch 542/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8478e-05 - mae: 0.0038 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 1.4211e-17\n",
      "Epoch 543/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5250e-05 - mae: 0.0036 - val_loss: 6.9181e-05 - val_mae: 0.0039 - learning_rate: 1.4211e-17\n",
      "Epoch 544/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.2590e-05 - mae: 0.0040 - val_loss: 6.9170e-05 - val_mae: 0.0039 - learning_rate: 1.4211e-17\n",
      "Epoch 545/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.2760e-05 - mae: 0.0039 - val_loss: 6.9161e-05 - val_mae: 0.0039 - learning_rate: 1.4211e-17\n",
      "Epoch 546/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9259e-05 - mae: 0.0038 - val_loss: 6.9143e-05 - val_mae: 0.0039 - learning_rate: 1.4211e-17\n",
      "Epoch 547/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7519e-05 - mae: 0.0038 - val_loss: 6.9143e-05 - val_mae: 0.0039 - learning_rate: 1.4211e-17\n",
      "Epoch 548/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.9361e-05 - mae: 0.0038\n",
      "Epoch 548: ReduceLROnPlateau reducing learning rate to 7.105427695090692e-18.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9263e-05 - mae: 0.0038 - val_loss: 6.9161e-05 - val_mae: 0.0039 - learning_rate: 1.4211e-17\n",
      "Epoch 549/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6355e-05 - mae: 0.0037 - val_loss: 6.9136e-05 - val_mae: 0.0039 - learning_rate: 7.1054e-18\n",
      "Epoch 550/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7855e-05 - mae: 0.0037 - val_loss: 6.9124e-05 - val_mae: 0.0039 - learning_rate: 7.1054e-18\n",
      "Epoch 551/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1616e-05 - mae: 0.0039 - val_loss: 6.9149e-05 - val_mae: 0.0039 - learning_rate: 7.1054e-18\n",
      "Epoch 552/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7608e-05 - mae: 0.0037 - val_loss: 6.9172e-05 - val_mae: 0.0039 - learning_rate: 7.1054e-18\n",
      "Epoch 553/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.4282e-05 - mae: 0.0036 - val_loss: 6.9169e-05 - val_mae: 0.0039 - learning_rate: 7.1054e-18\n",
      "Epoch 554/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9324e-05 - mae: 0.0038 - val_loss: 6.9166e-05 - val_mae: 0.0039 - learning_rate: 7.1054e-18\n",
      "Epoch 555/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0199e-05 - mae: 0.0038 - val_loss: 6.9187e-05 - val_mae: 0.0039 - learning_rate: 7.1054e-18\n",
      "Epoch 556/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7740e-05 - mae: 0.0037 - val_loss: 6.9183e-05 - val_mae: 0.0039 - learning_rate: 7.1054e-18\n",
      "Epoch 557/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5563e-05 - mae: 0.0037 - val_loss: 6.9169e-05 - val_mae: 0.0039 - learning_rate: 7.1054e-18\n",
      "Epoch 558/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.1434e-05 - mae: 0.0039\n",
      "Epoch 558: ReduceLROnPlateau reducing learning rate to 3.552713847545346e-18.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1180e-05 - mae: 0.0039 - val_loss: 6.9154e-05 - val_mae: 0.0039 - learning_rate: 7.1054e-18\n",
      "Epoch 559/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6935e-05 - mae: 0.0037 - val_loss: 6.9135e-05 - val_mae: 0.0039 - learning_rate: 3.5527e-18\n",
      "Epoch 560/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5426e-05 - mae: 0.0036 - val_loss: 6.9132e-05 - val_mae: 0.0039 - learning_rate: 3.5527e-18\n",
      "Epoch 561/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6884e-05 - mae: 0.0036 - val_loss: 6.9145e-05 - val_mae: 0.0039 - learning_rate: 3.5527e-18\n",
      "Epoch 562/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6293e-05 - mae: 0.0037 - val_loss: 6.9164e-05 - val_mae: 0.0039 - learning_rate: 3.5527e-18\n",
      "Epoch 563/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7178e-05 - mae: 0.0037 - val_loss: 6.9173e-05 - val_mae: 0.0039 - learning_rate: 3.5527e-18\n",
      "Epoch 564/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7757e-05 - mae: 0.0038 - val_loss: 6.9189e-05 - val_mae: 0.0039 - learning_rate: 3.5527e-18\n",
      "Epoch 565/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7066e-05 - mae: 0.0037 - val_loss: 6.9202e-05 - val_mae: 0.0039 - learning_rate: 3.5527e-18\n",
      "Epoch 566/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8849e-05 - mae: 0.0038 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 3.5527e-18\n",
      "Epoch 567/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.3265e-05 - mae: 0.0035 - val_loss: 6.9196e-05 - val_mae: 0.0039 - learning_rate: 3.5527e-18\n",
      "Epoch 568/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.5670e-05 - mae: 0.0036\n",
      "Epoch 568: ReduceLROnPlateau reducing learning rate to 1.776356923772673e-18.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5867e-05 - mae: 0.0037 - val_loss: 6.9179e-05 - val_mae: 0.0039 - learning_rate: 3.5527e-18\n",
      "Epoch 569/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5722e-05 - mae: 0.0037 - val_loss: 6.9200e-05 - val_mae: 0.0039 - learning_rate: 1.7764e-18\n",
      "Epoch 570/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9156e-05 - mae: 0.0037 - val_loss: 6.9228e-05 - val_mae: 0.0039 - learning_rate: 1.7764e-18\n",
      "Epoch 571/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0495e-05 - mae: 0.0038 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 1.7764e-18\n",
      "Epoch 572/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7580e-05 - mae: 0.0037 - val_loss: 6.9220e-05 - val_mae: 0.0039 - learning_rate: 1.7764e-18\n",
      "Epoch 573/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6927e-05 - mae: 0.0037 - val_loss: 6.9240e-05 - val_mae: 0.0039 - learning_rate: 1.7764e-18\n",
      "Epoch 574/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8069e-05 - mae: 0.0037 - val_loss: 6.9222e-05 - val_mae: 0.0039 - learning_rate: 1.7764e-18\n",
      "Epoch 575/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5090e-05 - mae: 0.0036 - val_loss: 6.9234e-05 - val_mae: 0.0039 - learning_rate: 1.7764e-18\n",
      "Epoch 576/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9179e-05 - mae: 0.0038 - val_loss: 6.9266e-05 - val_mae: 0.0039 - learning_rate: 1.7764e-18\n",
      "Epoch 577/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7546e-05 - mae: 0.0037 - val_loss: 6.9263e-05 - val_mae: 0.0039 - learning_rate: 1.7764e-18\n",
      "Epoch 578/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.0625e-05 - mae: 0.0039\n",
      "Epoch 578: ReduceLROnPlateau reducing learning rate to 8.881784618863365e-19.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0381e-05 - mae: 0.0039 - val_loss: 6.9275e-05 - val_mae: 0.0039 - learning_rate: 1.7764e-18\n",
      "Epoch 579/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8345e-05 - mae: 0.0037 - val_loss: 6.9265e-05 - val_mae: 0.0039 - learning_rate: 8.8818e-19\n",
      "Epoch 580/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6624e-05 - mae: 0.0037 - val_loss: 6.9273e-05 - val_mae: 0.0039 - learning_rate: 8.8818e-19\n",
      "Epoch 581/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6403e-05 - mae: 0.0037 - val_loss: 6.9281e-05 - val_mae: 0.0039 - learning_rate: 8.8818e-19\n",
      "Epoch 582/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6266e-05 - mae: 0.0037 - val_loss: 6.9279e-05 - val_mae: 0.0039 - learning_rate: 8.8818e-19\n",
      "Epoch 583/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.2325e-05 - mae: 0.0039 - val_loss: 6.9262e-05 - val_mae: 0.0039 - learning_rate: 8.8818e-19\n",
      "Epoch 584/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8965e-05 - mae: 0.0038 - val_loss: 6.9272e-05 - val_mae: 0.0039 - learning_rate: 8.8818e-19\n",
      "Epoch 585/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9773e-05 - mae: 0.0038 - val_loss: 6.9276e-05 - val_mae: 0.0039 - learning_rate: 8.8818e-19\n",
      "Epoch 586/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9668e-05 - mae: 0.0038 - val_loss: 6.9272e-05 - val_mae: 0.0039 - learning_rate: 8.8818e-19\n",
      "Epoch 587/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6383e-05 - mae: 0.0037 - val_loss: 6.9235e-05 - val_mae: 0.0039 - learning_rate: 8.8818e-19\n",
      "Epoch 588/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.6838e-05 - mae: 0.0037\n",
      "Epoch 588: ReduceLROnPlateau reducing learning rate to 4.440892309431682e-19.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6988e-05 - mae: 0.0037 - val_loss: 6.9242e-05 - val_mae: 0.0039 - learning_rate: 8.8818e-19\n",
      "Epoch 589/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6190e-05 - mae: 0.0036 - val_loss: 6.9238e-05 - val_mae: 0.0039 - learning_rate: 4.4409e-19\n",
      "Epoch 590/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6045e-05 - mae: 0.0037 - val_loss: 6.9232e-05 - val_mae: 0.0039 - learning_rate: 4.4409e-19\n",
      "Epoch 591/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5132e-05 - mae: 0.0036 - val_loss: 6.9204e-05 - val_mae: 0.0039 - learning_rate: 4.4409e-19\n",
      "Epoch 592/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6983e-05 - mae: 0.0037 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 4.4409e-19\n",
      "Epoch 593/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5146e-05 - mae: 0.0037 - val_loss: 6.9215e-05 - val_mae: 0.0039 - learning_rate: 4.4409e-19\n",
      "Epoch 594/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5906e-05 - mae: 0.0037 - val_loss: 6.9221e-05 - val_mae: 0.0039 - learning_rate: 4.4409e-19\n",
      "Epoch 595/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9505e-05 - mae: 0.0038 - val_loss: 6.9215e-05 - val_mae: 0.0039 - learning_rate: 4.4409e-19\n",
      "Epoch 596/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6423e-05 - mae: 0.0037 - val_loss: 6.9219e-05 - val_mae: 0.0039 - learning_rate: 4.4409e-19\n",
      "Epoch 597/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6065e-05 - mae: 0.0037 - val_loss: 6.9237e-05 - val_mae: 0.0039 - learning_rate: 4.4409e-19\n",
      "Epoch 598/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.7089e-05 - mae: 0.0037\n",
      "Epoch 598: ReduceLROnPlateau reducing learning rate to 2.220446154715841e-19.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7122e-05 - mae: 0.0037 - val_loss: 6.9232e-05 - val_mae: 0.0039 - learning_rate: 4.4409e-19\n",
      "Epoch 599/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7062e-05 - mae: 0.0037 - val_loss: 6.9207e-05 - val_mae: 0.0039 - learning_rate: 2.2204e-19\n",
      "Epoch 600/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6798e-05 - mae: 0.0037 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 2.2204e-19\n",
      "Epoch 601/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.5911e-05 - mae: 0.0036 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 2.2204e-19\n",
      "Epoch 602/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1041e-05 - mae: 0.0038 - val_loss: 6.9227e-05 - val_mae: 0.0039 - learning_rate: 2.2204e-19\n",
      "Epoch 603/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1401e-05 - mae: 0.0039 - val_loss: 6.9203e-05 - val_mae: 0.0039 - learning_rate: 2.2204e-19\n",
      "Epoch 604/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9501e-05 - mae: 0.0038 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 2.2204e-19\n",
      "Epoch 605/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1978e-05 - mae: 0.0039 - val_loss: 6.9219e-05 - val_mae: 0.0039 - learning_rate: 2.2204e-19\n",
      "Epoch 606/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1268e-05 - mae: 0.0038 - val_loss: 6.9230e-05 - val_mae: 0.0039 - learning_rate: 2.2204e-19\n",
      "Epoch 607/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9408e-05 - mae: 0.0038 - val_loss: 6.9241e-05 - val_mae: 0.0039 - learning_rate: 2.2204e-19\n",
      "Epoch 608/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.8953e-05 - mae: 0.0037\n",
      "Epoch 608: ReduceLROnPlateau reducing learning rate to 1.1102230773579206e-19.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8817e-05 - mae: 0.0037 - val_loss: 6.9236e-05 - val_mae: 0.0039 - learning_rate: 2.2204e-19\n",
      "Epoch 609/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7123e-05 - mae: 0.0037 - val_loss: 6.9245e-05 - val_mae: 0.0039 - learning_rate: 1.1102e-19\n",
      "Epoch 610/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6895e-05 - mae: 0.0037 - val_loss: 6.9259e-05 - val_mae: 0.0039 - learning_rate: 1.1102e-19\n",
      "Epoch 611/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5867e-05 - mae: 0.0037 - val_loss: 6.9256e-05 - val_mae: 0.0039 - learning_rate: 1.1102e-19\n",
      "Epoch 612/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8224e-05 - mae: 0.0038 - val_loss: 6.9275e-05 - val_mae: 0.0039 - learning_rate: 1.1102e-19\n",
      "Epoch 613/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9796e-05 - mae: 0.0038 - val_loss: 6.9282e-05 - val_mae: 0.0039 - learning_rate: 1.1102e-19\n",
      "Epoch 614/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5785e-05 - mae: 0.0037 - val_loss: 6.9288e-05 - val_mae: 0.0039 - learning_rate: 1.1102e-19\n",
      "Epoch 615/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6692e-05 - mae: 0.0037 - val_loss: 6.9276e-05 - val_mae: 0.0039 - learning_rate: 1.1102e-19\n",
      "Epoch 616/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7649e-05 - mae: 0.0038 - val_loss: 6.9286e-05 - val_mae: 0.0039 - learning_rate: 1.1102e-19\n",
      "Epoch 617/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8187e-05 - mae: 0.0037 - val_loss: 6.9291e-05 - val_mae: 0.0039 - learning_rate: 1.1102e-19\n",
      "Epoch 618/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.4834e-05 - mae: 0.0039\n",
      "Epoch 618: ReduceLROnPlateau reducing learning rate to 5.551115386789603e-20.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.4149e-05 - mae: 0.0039 - val_loss: 6.9293e-05 - val_mae: 0.0039 - learning_rate: 1.1102e-19\n",
      "Epoch 619/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9877e-05 - mae: 0.0038 - val_loss: 6.9297e-05 - val_mae: 0.0039 - learning_rate: 5.5511e-20\n",
      "Epoch 620/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 7.0953e-05 - mae: 0.0038 - val_loss: 6.9284e-05 - val_mae: 0.0039 - learning_rate: 5.5511e-20\n",
      "Epoch 621/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7048e-05 - mae: 0.0037 - val_loss: 6.9264e-05 - val_mae: 0.0039 - learning_rate: 5.5511e-20\n",
      "Epoch 622/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7431e-05 - mae: 0.0037 - val_loss: 6.9250e-05 - val_mae: 0.0039 - learning_rate: 5.5511e-20\n",
      "Epoch 623/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5020e-05 - mae: 0.0036 - val_loss: 6.9253e-05 - val_mae: 0.0039 - learning_rate: 5.5511e-20\n",
      "Epoch 624/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0186e-05 - mae: 0.0038 - val_loss: 6.9250e-05 - val_mae: 0.0039 - learning_rate: 5.5511e-20\n",
      "Epoch 625/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8551e-05 - mae: 0.0038 - val_loss: 6.9248e-05 - val_mae: 0.0039 - learning_rate: 5.5511e-20\n",
      "Epoch 626/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5089e-05 - mae: 0.0036 - val_loss: 6.9228e-05 - val_mae: 0.0039 - learning_rate: 5.5511e-20\n",
      "Epoch 627/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9257e-05 - mae: 0.0038 - val_loss: 6.9228e-05 - val_mae: 0.0039 - learning_rate: 5.5511e-20\n",
      "Epoch 628/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.8438e-05 - mae: 0.0038\n",
      "Epoch 628: ReduceLROnPlateau reducing learning rate to 2.7755576933948015e-20.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8368e-05 - mae: 0.0038 - val_loss: 6.9230e-05 - val_mae: 0.0039 - learning_rate: 5.5511e-20\n",
      "Epoch 629/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6016e-05 - mae: 0.0036 - val_loss: 6.9224e-05 - val_mae: 0.0039 - learning_rate: 2.7756e-20\n",
      "Epoch 630/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6864e-05 - mae: 0.0037 - val_loss: 6.9223e-05 - val_mae: 0.0039 - learning_rate: 2.7756e-20\n",
      "Epoch 631/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5516e-05 - mae: 0.0037 - val_loss: 6.9242e-05 - val_mae: 0.0039 - learning_rate: 2.7756e-20\n",
      "Epoch 632/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5631e-05 - mae: 0.0037 - val_loss: 6.9230e-05 - val_mae: 0.0039 - learning_rate: 2.7756e-20\n",
      "Epoch 633/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0483e-05 - mae: 0.0038 - val_loss: 6.9226e-05 - val_mae: 0.0039 - learning_rate: 2.7756e-20\n",
      "Epoch 634/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8569e-05 - mae: 0.0038 - val_loss: 6.9200e-05 - val_mae: 0.0039 - learning_rate: 2.7756e-20\n",
      "Epoch 635/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1696e-05 - mae: 0.0038 - val_loss: 6.9208e-05 - val_mae: 0.0039 - learning_rate: 2.7756e-20\n",
      "Epoch 636/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6273e-05 - mae: 0.0037 - val_loss: 6.9210e-05 - val_mae: 0.0039 - learning_rate: 2.7756e-20\n",
      "Epoch 637/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.8975e-05 - mae: 0.0038 - val_loss: 6.9202e-05 - val_mae: 0.0039 - learning_rate: 2.7756e-20\n",
      "Epoch 638/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.6499e-05 - mae: 0.0037\n",
      "Epoch 638: ReduceLROnPlateau reducing learning rate to 1.3877788466974007e-20.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6654e-05 - mae: 0.0037 - val_loss: 6.9207e-05 - val_mae: 0.0039 - learning_rate: 2.7756e-20\n",
      "Epoch 639/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4113e-05 - mae: 0.0036 - val_loss: 6.9232e-05 - val_mae: 0.0039 - learning_rate: 1.3878e-20\n",
      "Epoch 640/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1456e-05 - mae: 0.0039 - val_loss: 6.9198e-05 - val_mae: 0.0039 - learning_rate: 1.3878e-20\n",
      "Epoch 641/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0856e-05 - mae: 0.0038 - val_loss: 6.9182e-05 - val_mae: 0.0039 - learning_rate: 1.3878e-20\n",
      "Epoch 642/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7337e-05 - mae: 0.0037 - val_loss: 6.9160e-05 - val_mae: 0.0039 - learning_rate: 1.3878e-20\n",
      "Epoch 643/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6607e-05 - mae: 0.0037 - val_loss: 6.9176e-05 - val_mae: 0.0039 - learning_rate: 1.3878e-20\n",
      "Epoch 644/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9468e-05 - mae: 0.0038 - val_loss: 6.9188e-05 - val_mae: 0.0039 - learning_rate: 1.3878e-20\n",
      "Epoch 645/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6455e-05 - mae: 0.0037 - val_loss: 6.9206e-05 - val_mae: 0.0039 - learning_rate: 1.3878e-20\n",
      "Epoch 646/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.6788e-05 - mae: 0.0037 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 1.3878e-20\n",
      "Epoch 647/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7778e-05 - mae: 0.0037 - val_loss: 6.9207e-05 - val_mae: 0.0039 - learning_rate: 1.3878e-20\n",
      "Epoch 648/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8263e-05 - mae: 0.0038\n",
      "Epoch 648: ReduceLROnPlateau reducing learning rate to 6.938894233487004e-21.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8151e-05 - mae: 0.0038 - val_loss: 6.9223e-05 - val_mae: 0.0039 - learning_rate: 1.3878e-20\n",
      "Epoch 649/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6955e-05 - mae: 0.0037 - val_loss: 6.9210e-05 - val_mae: 0.0039 - learning_rate: 6.9389e-21\n",
      "Epoch 650/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8887e-05 - mae: 0.0038 - val_loss: 6.9201e-05 - val_mae: 0.0039 - learning_rate: 6.9389e-21\n",
      "Epoch 651/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8431e-05 - mae: 0.0038 - val_loss: 6.9196e-05 - val_mae: 0.0039 - learning_rate: 6.9389e-21\n",
      "Epoch 652/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4388e-05 - mae: 0.0036 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 6.9389e-21\n",
      "Epoch 653/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7670e-05 - mae: 0.0037 - val_loss: 6.9229e-05 - val_mae: 0.0039 - learning_rate: 6.9389e-21\n",
      "Epoch 654/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.8484e-05 - mae: 0.0037 - val_loss: 6.9249e-05 - val_mae: 0.0039 - learning_rate: 6.9389e-21\n",
      "Epoch 655/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4888e-05 - mae: 0.0036 - val_loss: 6.9219e-05 - val_mae: 0.0039 - learning_rate: 6.9389e-21\n",
      "Epoch 656/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6544e-05 - mae: 0.0037 - val_loss: 6.9208e-05 - val_mae: 0.0039 - learning_rate: 6.9389e-21\n",
      "Epoch 657/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0125e-05 - mae: 0.0038 - val_loss: 6.9182e-05 - val_mae: 0.0039 - learning_rate: 6.9389e-21\n",
      "Epoch 658/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.8937e-05 - mae: 0.0038\n",
      "Epoch 658: ReduceLROnPlateau reducing learning rate to 3.469447116743502e-21.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8839e-05 - mae: 0.0038 - val_loss: 6.9199e-05 - val_mae: 0.0039 - learning_rate: 6.9389e-21\n",
      "Epoch 659/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8857e-05 - mae: 0.0038 - val_loss: 6.9183e-05 - val_mae: 0.0039 - learning_rate: 3.4694e-21\n",
      "Epoch 660/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0109e-05 - mae: 0.0038 - val_loss: 6.9205e-05 - val_mae: 0.0039 - learning_rate: 3.4694e-21\n",
      "Epoch 661/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6605e-05 - mae: 0.0037 - val_loss: 6.9171e-05 - val_mae: 0.0039 - learning_rate: 3.4694e-21\n",
      "Epoch 662/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8990e-05 - mae: 0.0038 - val_loss: 6.9194e-05 - val_mae: 0.0039 - learning_rate: 3.4694e-21\n",
      "Epoch 663/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7708e-05 - mae: 0.0038 - val_loss: 6.9169e-05 - val_mae: 0.0039 - learning_rate: 3.4694e-21\n",
      "Epoch 664/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7225e-05 - mae: 0.0038 - val_loss: 6.9134e-05 - val_mae: 0.0039 - learning_rate: 3.4694e-21\n",
      "Epoch 665/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5199e-05 - mae: 0.0036 - val_loss: 6.9173e-05 - val_mae: 0.0039 - learning_rate: 3.4694e-21\n",
      "Epoch 666/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9060e-05 - mae: 0.0037 - val_loss: 6.9180e-05 - val_mae: 0.0039 - learning_rate: 3.4694e-21\n",
      "Epoch 667/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8411e-05 - mae: 0.0038 - val_loss: 6.9198e-05 - val_mae: 0.0039 - learning_rate: 3.4694e-21\n",
      "Epoch 668/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8164e-05 - mae: 0.0038\n",
      "Epoch 668: ReduceLROnPlateau reducing learning rate to 1.734723558371751e-21.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8238e-05 - mae: 0.0038 - val_loss: 6.9219e-05 - val_mae: 0.0039 - learning_rate: 3.4694e-21\n",
      "Epoch 669/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8402e-05 - mae: 0.0037 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 1.7347e-21\n",
      "Epoch 670/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8951e-05 - mae: 0.0038 - val_loss: 6.9210e-05 - val_mae: 0.0039 - learning_rate: 1.7347e-21\n",
      "Epoch 671/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9554e-05 - mae: 0.0038 - val_loss: 6.9223e-05 - val_mae: 0.0039 - learning_rate: 1.7347e-21\n",
      "Epoch 672/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8235e-05 - mae: 0.0038 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 1.7347e-21\n",
      "Epoch 673/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7356e-05 - mae: 0.0037 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 1.7347e-21\n",
      "Epoch 674/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6005e-05 - mae: 0.0037 - val_loss: 6.9202e-05 - val_mae: 0.0039 - learning_rate: 1.7347e-21\n",
      "Epoch 675/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5366e-05 - mae: 0.0036 - val_loss: 6.9199e-05 - val_mae: 0.0039 - learning_rate: 1.7347e-21\n",
      "Epoch 676/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6623e-05 - mae: 0.0037 - val_loss: 6.9166e-05 - val_mae: 0.0039 - learning_rate: 1.7347e-21\n",
      "Epoch 677/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.8966e-05 - mae: 0.0038 - val_loss: 6.9187e-05 - val_mae: 0.0039 - learning_rate: 1.7347e-21\n",
      "Epoch 678/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.0356e-05 - mae: 0.0038\n",
      "Epoch 678: ReduceLROnPlateau reducing learning rate to 8.673617791858755e-22.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0053e-05 - mae: 0.0038 - val_loss: 6.9203e-05 - val_mae: 0.0039 - learning_rate: 1.7347e-21\n",
      "Epoch 679/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9904e-05 - mae: 0.0038 - val_loss: 6.9235e-05 - val_mae: 0.0039 - learning_rate: 8.6736e-22\n",
      "Epoch 680/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7125e-05 - mae: 0.0037 - val_loss: 6.9225e-05 - val_mae: 0.0039 - learning_rate: 8.6736e-22\n",
      "Epoch 681/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5789e-05 - mae: 0.0036 - val_loss: 6.9224e-05 - val_mae: 0.0039 - learning_rate: 8.6736e-22\n",
      "Epoch 682/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7770e-05 - mae: 0.0037 - val_loss: 6.9236e-05 - val_mae: 0.0039 - learning_rate: 8.6736e-22\n",
      "Epoch 683/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6371e-05 - mae: 0.0037 - val_loss: 6.9208e-05 - val_mae: 0.0039 - learning_rate: 8.6736e-22\n",
      "Epoch 684/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8825e-05 - mae: 0.0038 - val_loss: 6.9223e-05 - val_mae: 0.0039 - learning_rate: 8.6736e-22\n",
      "Epoch 685/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8720e-05 - mae: 0.0038 - val_loss: 6.9239e-05 - val_mae: 0.0039 - learning_rate: 8.6736e-22\n",
      "Epoch 686/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9211e-05 - mae: 0.0037 - val_loss: 6.9247e-05 - val_mae: 0.0039 - learning_rate: 8.6736e-22\n",
      "Epoch 687/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7689e-05 - mae: 0.0037 - val_loss: 6.9257e-05 - val_mae: 0.0039 - learning_rate: 8.6736e-22\n",
      "Epoch 688/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.2057e-05 - mae: 0.0039\n",
      "Epoch 688: ReduceLROnPlateau reducing learning rate to 4.336808895929377e-22.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1625e-05 - mae: 0.0039 - val_loss: 6.9267e-05 - val_mae: 0.0039 - learning_rate: 8.6736e-22\n",
      "Epoch 689/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5354e-05 - mae: 0.0036 - val_loss: 6.9266e-05 - val_mae: 0.0039 - learning_rate: 4.3368e-22\n",
      "Epoch 690/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5254e-05 - mae: 0.0036 - val_loss: 6.9263e-05 - val_mae: 0.0039 - learning_rate: 4.3368e-22\n",
      "Epoch 691/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6538e-05 - mae: 0.0037 - val_loss: 6.9248e-05 - val_mae: 0.0039 - learning_rate: 4.3368e-22\n",
      "Epoch 692/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7726e-05 - mae: 0.0038 - val_loss: 6.9231e-05 - val_mae: 0.0039 - learning_rate: 4.3368e-22\n",
      "Epoch 693/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6660e-05 - mae: 0.0037 - val_loss: 6.9246e-05 - val_mae: 0.0039 - learning_rate: 4.3368e-22\n",
      "Epoch 694/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6346e-05 - mae: 0.0037 - val_loss: 6.9255e-05 - val_mae: 0.0039 - learning_rate: 4.3368e-22\n",
      "Epoch 695/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8549e-05 - mae: 0.0038 - val_loss: 6.9226e-05 - val_mae: 0.0039 - learning_rate: 4.3368e-22\n",
      "Epoch 696/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5068e-05 - mae: 0.0037 - val_loss: 6.9215e-05 - val_mae: 0.0039 - learning_rate: 4.3368e-22\n",
      "Epoch 697/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8113e-05 - mae: 0.0037 - val_loss: 6.9197e-05 - val_mae: 0.0039 - learning_rate: 4.3368e-22\n",
      "Epoch 698/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.4660e-05 - mae: 0.0037\n",
      "Epoch 698: ReduceLROnPlateau reducing learning rate to 2.1684044479646887e-22.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.4977e-05 - mae: 0.0037 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 4.3368e-22\n",
      "Epoch 699/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9627e-05 - mae: 0.0038 - val_loss: 6.9204e-05 - val_mae: 0.0039 - learning_rate: 2.1684e-22\n",
      "Epoch 700/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.0767e-05 - mae: 0.0038 - val_loss: 6.9187e-05 - val_mae: 0.0039 - learning_rate: 2.1684e-22\n",
      "Epoch 701/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9419e-05 - mae: 0.0038 - val_loss: 6.9190e-05 - val_mae: 0.0039 - learning_rate: 2.1684e-22\n",
      "Epoch 702/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4570e-05 - mae: 0.0036 - val_loss: 6.9197e-05 - val_mae: 0.0039 - learning_rate: 2.1684e-22\n",
      "Epoch 703/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7307e-05 - mae: 0.0037 - val_loss: 6.9193e-05 - val_mae: 0.0039 - learning_rate: 2.1684e-22\n",
      "Epoch 704/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7381e-05 - mae: 0.0037 - val_loss: 6.9194e-05 - val_mae: 0.0039 - learning_rate: 2.1684e-22\n",
      "Epoch 705/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.4141e-05 - mae: 0.0036 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 2.1684e-22\n",
      "Epoch 706/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9866e-05 - mae: 0.0038 - val_loss: 6.9219e-05 - val_mae: 0.0039 - learning_rate: 2.1684e-22\n",
      "Epoch 707/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6101e-05 - mae: 0.0037 - val_loss: 6.9188e-05 - val_mae: 0.0039 - learning_rate: 2.1684e-22\n",
      "Epoch 708/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.0015e-05 - mae: 0.0038\n",
      "Epoch 708: ReduceLROnPlateau reducing learning rate to 1.0842022239823443e-22.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9827e-05 - mae: 0.0038 - val_loss: 6.9192e-05 - val_mae: 0.0039 - learning_rate: 2.1684e-22\n",
      "Epoch 709/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9114e-05 - mae: 0.0038 - val_loss: 6.9223e-05 - val_mae: 0.0039 - learning_rate: 1.0842e-22\n",
      "Epoch 710/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7457e-05 - mae: 0.0037 - val_loss: 6.9216e-05 - val_mae: 0.0039 - learning_rate: 1.0842e-22\n",
      "Epoch 711/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.4598e-05 - mae: 0.0036 - val_loss: 6.9216e-05 - val_mae: 0.0039 - learning_rate: 1.0842e-22\n",
      "Epoch 712/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6824e-05 - mae: 0.0037 - val_loss: 6.9220e-05 - val_mae: 0.0039 - learning_rate: 1.0842e-22\n",
      "Epoch 713/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8093e-05 - mae: 0.0037 - val_loss: 6.9206e-05 - val_mae: 0.0039 - learning_rate: 1.0842e-22\n",
      "Epoch 714/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1103e-05 - mae: 0.0038 - val_loss: 6.9235e-05 - val_mae: 0.0039 - learning_rate: 1.0842e-22\n",
      "Epoch 715/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4817e-05 - mae: 0.0036 - val_loss: 6.9254e-05 - val_mae: 0.0039 - learning_rate: 1.0842e-22\n",
      "Epoch 716/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8693e-05 - mae: 0.0038 - val_loss: 6.9250e-05 - val_mae: 0.0039 - learning_rate: 1.0842e-22\n",
      "Epoch 717/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6149e-05 - mae: 0.0037 - val_loss: 6.9238e-05 - val_mae: 0.0039 - learning_rate: 1.0842e-22\n",
      "Epoch 718/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8865e-05 - mae: 0.0038\n",
      "Epoch 718: ReduceLROnPlateau reducing learning rate to 5.421011119911722e-23.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.8802e-05 - mae: 0.0038 - val_loss: 6.9219e-05 - val_mae: 0.0039 - learning_rate: 1.0842e-22\n",
      "Epoch 719/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8266e-05 - mae: 0.0038 - val_loss: 6.9236e-05 - val_mae: 0.0039 - learning_rate: 5.4210e-23\n",
      "Epoch 720/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6416e-05 - mae: 0.0037 - val_loss: 6.9237e-05 - val_mae: 0.0039 - learning_rate: 5.4210e-23\n",
      "Epoch 721/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9305e-05 - mae: 0.0038 - val_loss: 6.9211e-05 - val_mae: 0.0039 - learning_rate: 5.4210e-23\n",
      "Epoch 722/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9310e-05 - mae: 0.0038 - val_loss: 6.9229e-05 - val_mae: 0.0039 - learning_rate: 5.4210e-23\n",
      "Epoch 723/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6146e-05 - mae: 0.0037 - val_loss: 6.9235e-05 - val_mae: 0.0039 - learning_rate: 5.4210e-23\n",
      "Epoch 724/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7292e-05 - mae: 0.0038 - val_loss: 6.9200e-05 - val_mae: 0.0039 - learning_rate: 5.4210e-23\n",
      "Epoch 725/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7854e-05 - mae: 0.0037 - val_loss: 6.9201e-05 - val_mae: 0.0039 - learning_rate: 5.4210e-23\n",
      "Epoch 726/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9586e-05 - mae: 0.0038 - val_loss: 6.9202e-05 - val_mae: 0.0039 - learning_rate: 5.4210e-23\n",
      "Epoch 727/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6444e-05 - mae: 0.0037 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 5.4210e-23\n",
      "Epoch 728/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.7959e-05 - mae: 0.0037\n",
      "Epoch 728: ReduceLROnPlateau reducing learning rate to 2.710505559955861e-23.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7905e-05 - mae: 0.0037 - val_loss: 6.9194e-05 - val_mae: 0.0039 - learning_rate: 5.4210e-23\n",
      "Epoch 729/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0244e-05 - mae: 0.0038 - val_loss: 6.9181e-05 - val_mae: 0.0039 - learning_rate: 2.7105e-23\n",
      "Epoch 730/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5353e-05 - mae: 0.0036 - val_loss: 6.9204e-05 - val_mae: 0.0039 - learning_rate: 2.7105e-23\n",
      "Epoch 731/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1769e-05 - mae: 0.0038 - val_loss: 6.9180e-05 - val_mae: 0.0039 - learning_rate: 2.7105e-23\n",
      "Epoch 732/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.2670e-05 - mae: 0.0039 - val_loss: 6.9157e-05 - val_mae: 0.0039 - learning_rate: 2.7105e-23\n",
      "Epoch 733/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6225e-05 - mae: 0.0037 - val_loss: 6.9164e-05 - val_mae: 0.0039 - learning_rate: 2.7105e-23\n",
      "Epoch 734/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8642e-05 - mae: 0.0037 - val_loss: 6.9153e-05 - val_mae: 0.0039 - learning_rate: 2.7105e-23\n",
      "Epoch 735/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8612e-05 - mae: 0.0038 - val_loss: 6.9172e-05 - val_mae: 0.0039 - learning_rate: 2.7105e-23\n",
      "Epoch 736/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8259e-05 - mae: 0.0037 - val_loss: 6.9157e-05 - val_mae: 0.0039 - learning_rate: 2.7105e-23\n",
      "Epoch 737/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6486e-05 - mae: 0.0037 - val_loss: 6.9155e-05 - val_mae: 0.0039 - learning_rate: 2.7105e-23\n",
      "Epoch 738/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 7.1347e-05 - mae: 0.0039\n",
      "Epoch 738: ReduceLROnPlateau reducing learning rate to 1.3552527799779304e-23.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.1073e-05 - mae: 0.0039 - val_loss: 6.9155e-05 - val_mae: 0.0039 - learning_rate: 2.7105e-23\n",
      "Epoch 739/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7130e-05 - mae: 0.0037 - val_loss: 6.9149e-05 - val_mae: 0.0039 - learning_rate: 1.3553e-23\n",
      "Epoch 740/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7579e-05 - mae: 0.0037 - val_loss: 6.9143e-05 - val_mae: 0.0039 - learning_rate: 1.3553e-23\n",
      "Epoch 741/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8706e-05 - mae: 0.0038 - val_loss: 6.9156e-05 - val_mae: 0.0039 - learning_rate: 1.3553e-23\n",
      "Epoch 742/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9304e-05 - mae: 0.0038 - val_loss: 6.9166e-05 - val_mae: 0.0039 - learning_rate: 1.3553e-23\n",
      "Epoch 743/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8195e-05 - mae: 0.0037 - val_loss: 6.9158e-05 - val_mae: 0.0039 - learning_rate: 1.3553e-23\n",
      "Epoch 744/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6934e-05 - mae: 0.0037 - val_loss: 6.9173e-05 - val_mae: 0.0039 - learning_rate: 1.3553e-23\n",
      "Epoch 745/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6686e-05 - mae: 0.0037 - val_loss: 6.9169e-05 - val_mae: 0.0039 - learning_rate: 1.3553e-23\n",
      "Epoch 746/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7532e-05 - mae: 0.0037 - val_loss: 6.9165e-05 - val_mae: 0.0039 - learning_rate: 1.3553e-23\n",
      "Epoch 747/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8508e-05 - mae: 0.0038 - val_loss: 6.9194e-05 - val_mae: 0.0039 - learning_rate: 1.3553e-23\n",
      "Epoch 748/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.9195e-05 - mae: 0.0038\n",
      "Epoch 748: ReduceLROnPlateau reducing learning rate to 6.776263899889652e-24.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9074e-05 - mae: 0.0038 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 1.3553e-23\n",
      "Epoch 749/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5153e-05 - mae: 0.0037 - val_loss: 6.9224e-05 - val_mae: 0.0039 - learning_rate: 6.7763e-24\n",
      "Epoch 750/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8713e-05 - mae: 0.0038 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 6.7763e-24\n",
      "Epoch 751/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5476e-05 - mae: 0.0036 - val_loss: 6.9205e-05 - val_mae: 0.0039 - learning_rate: 6.7763e-24\n",
      "Epoch 752/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7771e-05 - mae: 0.0038 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 6.7763e-24\n",
      "Epoch 753/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8543e-05 - mae: 0.0038 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 6.7763e-24\n",
      "Epoch 754/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8544e-05 - mae: 0.0038 - val_loss: 6.9223e-05 - val_mae: 0.0039 - learning_rate: 6.7763e-24\n",
      "Epoch 755/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.0886e-05 - mae: 0.0038 - val_loss: 6.9221e-05 - val_mae: 0.0039 - learning_rate: 6.7763e-24\n",
      "Epoch 756/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7698e-05 - mae: 0.0037 - val_loss: 6.9207e-05 - val_mae: 0.0039 - learning_rate: 6.7763e-24\n",
      "Epoch 757/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6639e-05 - mae: 0.0037 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 6.7763e-24\n",
      "Epoch 758/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.8662e-05 - mae: 0.0037\n",
      "Epoch 758: ReduceLROnPlateau reducing learning rate to 3.388131949944826e-24.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8537e-05 - mae: 0.0037 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 6.7763e-24\n",
      "Epoch 759/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.0112e-05 - mae: 0.0038 - val_loss: 6.9226e-05 - val_mae: 0.0039 - learning_rate: 3.3881e-24\n",
      "Epoch 760/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9507e-05 - mae: 0.0038 - val_loss: 6.9228e-05 - val_mae: 0.0039 - learning_rate: 3.3881e-24\n",
      "Epoch 761/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7760e-05 - mae: 0.0037 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 3.3881e-24\n",
      "Epoch 762/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0100e-05 - mae: 0.0038 - val_loss: 6.9202e-05 - val_mae: 0.0039 - learning_rate: 3.3881e-24\n",
      "Epoch 763/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6856e-05 - mae: 0.0037 - val_loss: 6.9202e-05 - val_mae: 0.0039 - learning_rate: 3.3881e-24\n",
      "Epoch 764/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7808e-05 - mae: 0.0038 - val_loss: 6.9216e-05 - val_mae: 0.0039 - learning_rate: 3.3881e-24\n",
      "Epoch 765/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4651e-05 - mae: 0.0036 - val_loss: 6.9227e-05 - val_mae: 0.0039 - learning_rate: 3.3881e-24\n",
      "Epoch 766/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5887e-05 - mae: 0.0037 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 3.3881e-24\n",
      "Epoch 767/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0479e-05 - mae: 0.0038 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 3.3881e-24\n",
      "Epoch 768/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.8008e-05 - mae: 0.0037\n",
      "Epoch 768: ReduceLROnPlateau reducing learning rate to 1.694065974972413e-24.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7977e-05 - mae: 0.0037 - val_loss: 6.9236e-05 - val_mae: 0.0039 - learning_rate: 3.3881e-24\n",
      "Epoch 769/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6528e-05 - mae: 0.0037 - val_loss: 6.9242e-05 - val_mae: 0.0039 - learning_rate: 1.6941e-24\n",
      "Epoch 770/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9652e-05 - mae: 0.0038 - val_loss: 6.9229e-05 - val_mae: 0.0039 - learning_rate: 1.6941e-24\n",
      "Epoch 771/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9162e-05 - mae: 0.0038 - val_loss: 6.9246e-05 - val_mae: 0.0039 - learning_rate: 1.6941e-24\n",
      "Epoch 772/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9760e-05 - mae: 0.0038 - val_loss: 6.9235e-05 - val_mae: 0.0039 - learning_rate: 1.6941e-24\n",
      "Epoch 773/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6993e-05 - mae: 0.0037 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 1.6941e-24\n",
      "Epoch 774/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.0945e-05 - mae: 0.0038 - val_loss: 6.9232e-05 - val_mae: 0.0039 - learning_rate: 1.6941e-24\n",
      "Epoch 775/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7107e-05 - mae: 0.0037 - val_loss: 6.9252e-05 - val_mae: 0.0039 - learning_rate: 1.6941e-24\n",
      "Epoch 776/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9595e-05 - mae: 0.0038 - val_loss: 6.9270e-05 - val_mae: 0.0039 - learning_rate: 1.6941e-24\n",
      "Epoch 777/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8245e-05 - mae: 0.0038 - val_loss: 6.9259e-05 - val_mae: 0.0039 - learning_rate: 1.6941e-24\n",
      "Epoch 778/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.6045e-05 - mae: 0.0037\n",
      "Epoch 778: ReduceLROnPlateau reducing learning rate to 8.470329874862065e-25.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6190e-05 - mae: 0.0037 - val_loss: 6.9252e-05 - val_mae: 0.0039 - learning_rate: 1.6941e-24\n",
      "Epoch 779/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7420e-05 - mae: 0.0038 - val_loss: 6.9242e-05 - val_mae: 0.0039 - learning_rate: 8.4703e-25\n",
      "Epoch 780/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6274e-05 - mae: 0.0036 - val_loss: 6.9252e-05 - val_mae: 0.0039 - learning_rate: 8.4703e-25\n",
      "Epoch 781/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6380e-05 - mae: 0.0037 - val_loss: 6.9267e-05 - val_mae: 0.0039 - learning_rate: 8.4703e-25\n",
      "Epoch 782/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8483e-05 - mae: 0.0037 - val_loss: 6.9227e-05 - val_mae: 0.0039 - learning_rate: 8.4703e-25\n",
      "Epoch 783/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7804e-05 - mae: 0.0038 - val_loss: 6.9228e-05 - val_mae: 0.0039 - learning_rate: 8.4703e-25\n",
      "Epoch 784/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9579e-05 - mae: 0.0038 - val_loss: 6.9230e-05 - val_mae: 0.0039 - learning_rate: 8.4703e-25\n",
      "Epoch 785/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9133e-05 - mae: 0.0038 - val_loss: 6.9226e-05 - val_mae: 0.0039 - learning_rate: 8.4703e-25\n",
      "Epoch 786/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0419e-05 - mae: 0.0038 - val_loss: 6.9238e-05 - val_mae: 0.0039 - learning_rate: 8.4703e-25\n",
      "Epoch 787/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8880e-05 - mae: 0.0038 - val_loss: 6.9223e-05 - val_mae: 0.0039 - learning_rate: 8.4703e-25\n",
      "Epoch 788/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.5419e-05 - mae: 0.0037\n",
      "Epoch 788: ReduceLROnPlateau reducing learning rate to 4.2351649374310325e-25.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5664e-05 - mae: 0.0037 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 8.4703e-25\n",
      "Epoch 789/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 7.0504e-05 - mae: 0.0038 - val_loss: 6.9199e-05 - val_mae: 0.0039 - learning_rate: 4.2352e-25\n",
      "Epoch 790/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6332e-05 - mae: 0.0037 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 4.2352e-25\n",
      "Epoch 791/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5829e-05 - mae: 0.0037 - val_loss: 6.9196e-05 - val_mae: 0.0039 - learning_rate: 4.2352e-25\n",
      "Epoch 792/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6287e-05 - mae: 0.0037 - val_loss: 6.9190e-05 - val_mae: 0.0039 - learning_rate: 4.2352e-25\n",
      "Epoch 793/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5117e-05 - mae: 0.0036 - val_loss: 6.9166e-05 - val_mae: 0.0039 - learning_rate: 4.2352e-25\n",
      "Epoch 794/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.2691e-05 - mae: 0.0039 - val_loss: 6.9159e-05 - val_mae: 0.0039 - learning_rate: 4.2352e-25\n",
      "Epoch 795/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8612e-05 - mae: 0.0038 - val_loss: 6.9139e-05 - val_mae: 0.0039 - learning_rate: 4.2352e-25\n",
      "Epoch 796/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7999e-05 - mae: 0.0037 - val_loss: 6.9125e-05 - val_mae: 0.0039 - learning_rate: 4.2352e-25\n",
      "Epoch 797/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6206e-05 - mae: 0.0037 - val_loss: 6.9140e-05 - val_mae: 0.0039 - learning_rate: 4.2352e-25\n",
      "Epoch 798/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.5601e-05 - mae: 0.0036\n",
      "Epoch 798: ReduceLROnPlateau reducing learning rate to 2.1175824687155163e-25.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5781e-05 - mae: 0.0036 - val_loss: 6.9156e-05 - val_mae: 0.0039 - learning_rate: 4.2352e-25\n",
      "Epoch 799/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5810e-05 - mae: 0.0037 - val_loss: 6.9154e-05 - val_mae: 0.0039 - learning_rate: 2.1176e-25\n",
      "Epoch 800/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1477e-05 - mae: 0.0038 - val_loss: 6.9167e-05 - val_mae: 0.0039 - learning_rate: 2.1176e-25\n",
      "Epoch 801/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9642e-05 - mae: 0.0038 - val_loss: 6.9151e-05 - val_mae: 0.0039 - learning_rate: 2.1176e-25\n",
      "Epoch 802/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6592e-05 - mae: 0.0037 - val_loss: 6.9167e-05 - val_mae: 0.0039 - learning_rate: 2.1176e-25\n",
      "Epoch 803/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6164e-05 - mae: 0.0037 - val_loss: 6.9144e-05 - val_mae: 0.0039 - learning_rate: 2.1176e-25\n",
      "Epoch 804/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7014e-05 - mae: 0.0037 - val_loss: 6.9117e-05 - val_mae: 0.0039 - learning_rate: 2.1176e-25\n",
      "Epoch 805/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6830e-05 - mae: 0.0037 - val_loss: 6.9132e-05 - val_mae: 0.0039 - learning_rate: 2.1176e-25\n",
      "Epoch 806/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7949e-05 - mae: 0.0038 - val_loss: 6.9118e-05 - val_mae: 0.0039 - learning_rate: 2.1176e-25\n",
      "Epoch 807/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7508e-05 - mae: 0.0037 - val_loss: 6.9141e-05 - val_mae: 0.0039 - learning_rate: 2.1176e-25\n",
      "Epoch 808/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.6576e-05 - mae: 0.0037\n",
      "Epoch 808: ReduceLROnPlateau reducing learning rate to 1.0587912343577581e-25.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6741e-05 - mae: 0.0037 - val_loss: 6.9163e-05 - val_mae: 0.0039 - learning_rate: 2.1176e-25\n",
      "Epoch 809/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7320e-05 - mae: 0.0038 - val_loss: 6.9183e-05 - val_mae: 0.0039 - learning_rate: 1.0588e-25\n",
      "Epoch 810/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.8760e-05 - mae: 0.0038 - val_loss: 6.9176e-05 - val_mae: 0.0039 - learning_rate: 1.0588e-25\n",
      "Epoch 811/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6963e-05 - mae: 0.0037 - val_loss: 6.9193e-05 - val_mae: 0.0039 - learning_rate: 1.0588e-25\n",
      "Epoch 812/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7997e-05 - mae: 0.0038 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 1.0588e-25\n",
      "Epoch 813/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8519e-05 - mae: 0.0038 - val_loss: 6.9201e-05 - val_mae: 0.0039 - learning_rate: 1.0588e-25\n",
      "Epoch 814/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7720e-05 - mae: 0.0038 - val_loss: 6.9198e-05 - val_mae: 0.0039 - learning_rate: 1.0588e-25\n",
      "Epoch 815/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6217e-05 - mae: 0.0037 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 1.0588e-25\n",
      "Epoch 816/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6734e-05 - mae: 0.0037 - val_loss: 6.9194e-05 - val_mae: 0.0039 - learning_rate: 1.0588e-25\n",
      "Epoch 817/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7267e-05 - mae: 0.0037 - val_loss: 6.9188e-05 - val_mae: 0.0039 - learning_rate: 1.0588e-25\n",
      "Epoch 818/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.1085e-05 - mae: 0.0038\n",
      "Epoch 818: ReduceLROnPlateau reducing learning rate to 5.293956171788791e-26.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0782e-05 - mae: 0.0038 - val_loss: 6.9200e-05 - val_mae: 0.0039 - learning_rate: 1.0588e-25\n",
      "Epoch 819/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7524e-05 - mae: 0.0037 - val_loss: 6.9204e-05 - val_mae: 0.0039 - learning_rate: 5.2940e-26\n",
      "Epoch 820/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0358e-05 - mae: 0.0038 - val_loss: 6.9179e-05 - val_mae: 0.0039 - learning_rate: 5.2940e-26\n",
      "Epoch 821/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9463e-05 - mae: 0.0038 - val_loss: 6.9163e-05 - val_mae: 0.0039 - learning_rate: 5.2940e-26\n",
      "Epoch 822/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9700e-05 - mae: 0.0038 - val_loss: 6.9170e-05 - val_mae: 0.0039 - learning_rate: 5.2940e-26\n",
      "Epoch 823/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6372e-05 - mae: 0.0037 - val_loss: 6.9186e-05 - val_mae: 0.0039 - learning_rate: 5.2940e-26\n",
      "Epoch 824/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7882e-05 - mae: 0.0037 - val_loss: 6.9200e-05 - val_mae: 0.0039 - learning_rate: 5.2940e-26\n",
      "Epoch 825/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8691e-05 - mae: 0.0038 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 5.2940e-26\n",
      "Epoch 826/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0458e-05 - mae: 0.0039 - val_loss: 6.9200e-05 - val_mae: 0.0039 - learning_rate: 5.2940e-26\n",
      "Epoch 827/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6698e-05 - mae: 0.0037 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 5.2940e-26\n",
      "Epoch 828/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.7215e-05 - mae: 0.0037\n",
      "Epoch 828: ReduceLROnPlateau reducing learning rate to 2.6469780858943953e-26.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7256e-05 - mae: 0.0037 - val_loss: 6.9215e-05 - val_mae: 0.0039 - learning_rate: 5.2940e-26\n",
      "Epoch 829/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7744e-05 - mae: 0.0037 - val_loss: 6.9212e-05 - val_mae: 0.0039 - learning_rate: 2.6470e-26\n",
      "Epoch 830/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7181e-05 - mae: 0.0037 - val_loss: 6.9233e-05 - val_mae: 0.0039 - learning_rate: 2.6470e-26\n",
      "Epoch 831/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.6454e-05 - mae: 0.0037 - val_loss: 6.9239e-05 - val_mae: 0.0039 - learning_rate: 2.6470e-26\n",
      "Epoch 832/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5858e-05 - mae: 0.0037 - val_loss: 6.9228e-05 - val_mae: 0.0039 - learning_rate: 2.6470e-26\n",
      "Epoch 833/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5059e-05 - mae: 0.0036 - val_loss: 6.9223e-05 - val_mae: 0.0039 - learning_rate: 2.6470e-26\n",
      "Epoch 834/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5565e-05 - mae: 0.0037 - val_loss: 6.9228e-05 - val_mae: 0.0039 - learning_rate: 2.6470e-26\n",
      "Epoch 835/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7601e-05 - mae: 0.0037 - val_loss: 6.9255e-05 - val_mae: 0.0039 - learning_rate: 2.6470e-26\n",
      "Epoch 836/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6302e-05 - mae: 0.0037 - val_loss: 6.9270e-05 - val_mae: 0.0039 - learning_rate: 2.6470e-26\n",
      "Epoch 837/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8530e-05 - mae: 0.0037 - val_loss: 6.9258e-05 - val_mae: 0.0039 - learning_rate: 2.6470e-26\n",
      "Epoch 838/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6288e-05 - mae: 0.0037\n",
      "Epoch 838: ReduceLROnPlateau reducing learning rate to 1.3234890429471977e-26.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6414e-05 - mae: 0.0037 - val_loss: 6.9261e-05 - val_mae: 0.0039 - learning_rate: 2.6470e-26\n",
      "Epoch 839/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5578e-05 - mae: 0.0036 - val_loss: 6.9273e-05 - val_mae: 0.0039 - learning_rate: 1.3235e-26\n",
      "Epoch 840/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7177e-05 - mae: 0.0037 - val_loss: 6.9258e-05 - val_mae: 0.0039 - learning_rate: 1.3235e-26\n",
      "Epoch 841/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.3837e-05 - mae: 0.0039 - val_loss: 6.9251e-05 - val_mae: 0.0039 - learning_rate: 1.3235e-26\n",
      "Epoch 842/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7544e-05 - mae: 0.0037 - val_loss: 6.9246e-05 - val_mae: 0.0039 - learning_rate: 1.3235e-26\n",
      "Epoch 843/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6045e-05 - mae: 0.0037 - val_loss: 6.9264e-05 - val_mae: 0.0039 - learning_rate: 1.3235e-26\n",
      "Epoch 844/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.4555e-05 - mae: 0.0036 - val_loss: 6.9262e-05 - val_mae: 0.0039 - learning_rate: 1.3235e-26\n",
      "Epoch 845/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0199e-05 - mae: 0.0038 - val_loss: 6.9265e-05 - val_mae: 0.0039 - learning_rate: 1.3235e-26\n",
      "Epoch 846/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5837e-05 - mae: 0.0036 - val_loss: 6.9269e-05 - val_mae: 0.0039 - learning_rate: 1.3235e-26\n",
      "Epoch 847/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0951e-05 - mae: 0.0038 - val_loss: 6.9275e-05 - val_mae: 0.0039 - learning_rate: 1.3235e-26\n",
      "Epoch 848/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.0939e-05 - mae: 0.0038\n",
      "Epoch 848: ReduceLROnPlateau reducing learning rate to 6.617445214735988e-27.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.0588e-05 - mae: 0.0038 - val_loss: 6.9277e-05 - val_mae: 0.0039 - learning_rate: 1.3235e-26\n",
      "Epoch 849/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5546e-05 - mae: 0.0036 - val_loss: 6.9245e-05 - val_mae: 0.0039 - learning_rate: 6.6174e-27\n",
      "Epoch 850/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.2179e-05 - mae: 0.0039 - val_loss: 6.9253e-05 - val_mae: 0.0039 - learning_rate: 6.6174e-27\n",
      "Epoch 851/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.8702e-05 - mae: 0.0037 - val_loss: 6.9247e-05 - val_mae: 0.0039 - learning_rate: 6.6174e-27\n",
      "Epoch 852/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6808e-05 - mae: 0.0037 - val_loss: 6.9235e-05 - val_mae: 0.0039 - learning_rate: 6.6174e-27\n",
      "Epoch 853/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8323e-05 - mae: 0.0038 - val_loss: 6.9210e-05 - val_mae: 0.0039 - learning_rate: 6.6174e-27\n",
      "Epoch 854/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6221e-05 - mae: 0.0037 - val_loss: 6.9177e-05 - val_mae: 0.0039 - learning_rate: 6.6174e-27\n",
      "Epoch 855/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6683e-05 - mae: 0.0037 - val_loss: 6.9175e-05 - val_mae: 0.0039 - learning_rate: 6.6174e-27\n",
      "Epoch 856/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6897e-05 - mae: 0.0037 - val_loss: 6.9174e-05 - val_mae: 0.0039 - learning_rate: 6.6174e-27\n",
      "Epoch 857/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.4696e-05 - mae: 0.0036 - val_loss: 6.9190e-05 - val_mae: 0.0039 - learning_rate: 6.6174e-27\n",
      "Epoch 858/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6267e-05 - mae: 0.0037\n",
      "Epoch 858: ReduceLROnPlateau reducing learning rate to 3.308722607367994e-27.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.6404e-05 - mae: 0.0037 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 6.6174e-27\n",
      "Epoch 859/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7098e-05 - mae: 0.0037 - val_loss: 6.9202e-05 - val_mae: 0.0039 - learning_rate: 3.3087e-27\n",
      "Epoch 860/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4660e-05 - mae: 0.0036 - val_loss: 6.9207e-05 - val_mae: 0.0039 - learning_rate: 3.3087e-27\n",
      "Epoch 861/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0371e-05 - mae: 0.0038 - val_loss: 6.9223e-05 - val_mae: 0.0039 - learning_rate: 3.3087e-27\n",
      "Epoch 862/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8097e-05 - mae: 0.0038 - val_loss: 6.9236e-05 - val_mae: 0.0039 - learning_rate: 3.3087e-27\n",
      "Epoch 863/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9464e-05 - mae: 0.0038 - val_loss: 6.9234e-05 - val_mae: 0.0039 - learning_rate: 3.3087e-27\n",
      "Epoch 864/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6166e-05 - mae: 0.0037 - val_loss: 6.9246e-05 - val_mae: 0.0039 - learning_rate: 3.3087e-27\n",
      "Epoch 865/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7933e-05 - mae: 0.0037 - val_loss: 6.9241e-05 - val_mae: 0.0039 - learning_rate: 3.3087e-27\n",
      "Epoch 866/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7292e-05 - mae: 0.0037 - val_loss: 6.9246e-05 - val_mae: 0.0039 - learning_rate: 3.3087e-27\n",
      "Epoch 867/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5292e-05 - mae: 0.0036 - val_loss: 6.9254e-05 - val_mae: 0.0039 - learning_rate: 3.3087e-27\n",
      "Epoch 868/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.7853e-05 - mae: 0.0037\n",
      "Epoch 868: ReduceLROnPlateau reducing learning rate to 1.654361303683997e-27.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7878e-05 - mae: 0.0037 - val_loss: 6.9251e-05 - val_mae: 0.0039 - learning_rate: 3.3087e-27\n",
      "Epoch 869/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4432e-05 - mae: 0.0036 - val_loss: 6.9253e-05 - val_mae: 0.0039 - learning_rate: 1.6544e-27\n",
      "Epoch 870/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0000e-05 - mae: 0.0038 - val_loss: 6.9267e-05 - val_mae: 0.0039 - learning_rate: 1.6544e-27\n",
      "Epoch 871/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8128e-05 - mae: 0.0038 - val_loss: 6.9273e-05 - val_mae: 0.0039 - learning_rate: 1.6544e-27\n",
      "Epoch 872/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6182e-05 - mae: 0.0037 - val_loss: 6.9285e-05 - val_mae: 0.0039 - learning_rate: 1.6544e-27\n",
      "Epoch 873/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8376e-05 - mae: 0.0038 - val_loss: 6.9297e-05 - val_mae: 0.0039 - learning_rate: 1.6544e-27\n",
      "Epoch 874/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0904e-05 - mae: 0.0039 - val_loss: 6.9323e-05 - val_mae: 0.0039 - learning_rate: 1.6544e-27\n",
      "Epoch 875/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6806e-05 - mae: 0.0037 - val_loss: 6.9313e-05 - val_mae: 0.0039 - learning_rate: 1.6544e-27\n",
      "Epoch 876/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4955e-05 - mae: 0.0036 - val_loss: 6.9280e-05 - val_mae: 0.0039 - learning_rate: 1.6544e-27\n",
      "Epoch 877/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.0522e-05 - mae: 0.0038 - val_loss: 6.9290e-05 - val_mae: 0.0039 - learning_rate: 1.6544e-27\n",
      "Epoch 878/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8452e-05 - mae: 0.0038\n",
      "Epoch 878: ReduceLROnPlateau reducing learning rate to 8.271806518419985e-28.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8363e-05 - mae: 0.0038 - val_loss: 6.9278e-05 - val_mae: 0.0039 - learning_rate: 1.6544e-27\n",
      "Epoch 879/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8806e-05 - mae: 0.0038 - val_loss: 6.9256e-05 - val_mae: 0.0039 - learning_rate: 8.2718e-28\n",
      "Epoch 880/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7297e-05 - mae: 0.0037 - val_loss: 6.9250e-05 - val_mae: 0.0039 - learning_rate: 8.2718e-28\n",
      "Epoch 881/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8900e-05 - mae: 0.0037 - val_loss: 6.9270e-05 - val_mae: 0.0039 - learning_rate: 8.2718e-28\n",
      "Epoch 882/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7203e-05 - mae: 0.0037 - val_loss: 6.9274e-05 - val_mae: 0.0039 - learning_rate: 8.2718e-28\n",
      "Epoch 883/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.4238e-05 - mae: 0.0036 - val_loss: 6.9282e-05 - val_mae: 0.0039 - learning_rate: 8.2718e-28\n",
      "Epoch 884/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9897e-05 - mae: 0.0038 - val_loss: 6.9254e-05 - val_mae: 0.0039 - learning_rate: 8.2718e-28\n",
      "Epoch 885/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8230e-05 - mae: 0.0038 - val_loss: 6.9280e-05 - val_mae: 0.0039 - learning_rate: 8.2718e-28\n",
      "Epoch 886/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7465e-05 - mae: 0.0037 - val_loss: 6.9285e-05 - val_mae: 0.0039 - learning_rate: 8.2718e-28\n",
      "Epoch 887/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6641e-05 - mae: 0.0037 - val_loss: 6.9295e-05 - val_mae: 0.0039 - learning_rate: 8.2718e-28\n",
      "Epoch 888/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.5423e-05 - mae: 0.0037\n",
      "Epoch 888: ReduceLROnPlateau reducing learning rate to 4.135903259209993e-28.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5663e-05 - mae: 0.0037 - val_loss: 6.9315e-05 - val_mae: 0.0039 - learning_rate: 8.2718e-28\n",
      "Epoch 889/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7549e-05 - mae: 0.0037 - val_loss: 6.9319e-05 - val_mae: 0.0039 - learning_rate: 4.1359e-28\n",
      "Epoch 890/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.0467e-05 - mae: 0.0038 - val_loss: 6.9305e-05 - val_mae: 0.0039 - learning_rate: 4.1359e-28\n",
      "Epoch 891/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8144e-05 - mae: 0.0038 - val_loss: 6.9262e-05 - val_mae: 0.0039 - learning_rate: 4.1359e-28\n",
      "Epoch 892/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9920e-05 - mae: 0.0039 - val_loss: 6.9254e-05 - val_mae: 0.0039 - learning_rate: 4.1359e-28\n",
      "Epoch 893/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.2007e-05 - mae: 0.0039 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 4.1359e-28\n",
      "Epoch 894/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7323e-05 - mae: 0.0038 - val_loss: 6.9207e-05 - val_mae: 0.0039 - learning_rate: 4.1359e-28\n",
      "Epoch 895/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1348e-05 - mae: 0.0038 - val_loss: 6.9183e-05 - val_mae: 0.0039 - learning_rate: 4.1359e-28\n",
      "Epoch 896/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7110e-05 - mae: 0.0037 - val_loss: 6.9189e-05 - val_mae: 0.0039 - learning_rate: 4.1359e-28\n",
      "Epoch 897/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8314e-05 - mae: 0.0037 - val_loss: 6.9188e-05 - val_mae: 0.0039 - learning_rate: 4.1359e-28\n",
      "Epoch 898/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.4922e-05 - mae: 0.0036\n",
      "Epoch 898: ReduceLROnPlateau reducing learning rate to 2.0679516296049964e-28.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5246e-05 - mae: 0.0036 - val_loss: 6.9167e-05 - val_mae: 0.0039 - learning_rate: 4.1359e-28\n",
      "Epoch 899/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6648e-05 - mae: 0.0037 - val_loss: 6.9191e-05 - val_mae: 0.0039 - learning_rate: 2.0680e-28\n",
      "Epoch 900/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6945e-05 - mae: 0.0038 - val_loss: 6.9177e-05 - val_mae: 0.0039 - learning_rate: 2.0680e-28\n",
      "Epoch 901/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7673e-05 - mae: 0.0037 - val_loss: 6.9185e-05 - val_mae: 0.0039 - learning_rate: 2.0680e-28\n",
      "Epoch 902/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.4187e-05 - mae: 0.0036 - val_loss: 6.9185e-05 - val_mae: 0.0039 - learning_rate: 2.0680e-28\n",
      "Epoch 903/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9247e-05 - mae: 0.0038 - val_loss: 6.9155e-05 - val_mae: 0.0039 - learning_rate: 2.0680e-28\n",
      "Epoch 904/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7992e-05 - mae: 0.0037 - val_loss: 6.9167e-05 - val_mae: 0.0039 - learning_rate: 2.0680e-28\n",
      "Epoch 905/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.1491e-05 - mae: 0.0039 - val_loss: 6.9164e-05 - val_mae: 0.0039 - learning_rate: 2.0680e-28\n",
      "Epoch 906/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.2329e-05 - mae: 0.0039 - val_loss: 6.9186e-05 - val_mae: 0.0039 - learning_rate: 2.0680e-28\n",
      "Epoch 907/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9678e-05 - mae: 0.0038 - val_loss: 6.9163e-05 - val_mae: 0.0039 - learning_rate: 2.0680e-28\n",
      "Epoch 908/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.6749e-05 - mae: 0.0037\n",
      "Epoch 908: ReduceLROnPlateau reducing learning rate to 1.0339758148024982e-28.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6829e-05 - mae: 0.0037 - val_loss: 6.9179e-05 - val_mae: 0.0039 - learning_rate: 2.0680e-28\n",
      "Epoch 909/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9172e-05 - mae: 0.0038 - val_loss: 6.9179e-05 - val_mae: 0.0039 - learning_rate: 1.0340e-28\n",
      "Epoch 910/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7940e-05 - mae: 0.0037 - val_loss: 6.9156e-05 - val_mae: 0.0039 - learning_rate: 1.0340e-28\n",
      "Epoch 911/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5425e-05 - mae: 0.0036 - val_loss: 6.9170e-05 - val_mae: 0.0039 - learning_rate: 1.0340e-28\n",
      "Epoch 912/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5872e-05 - mae: 0.0037 - val_loss: 6.9154e-05 - val_mae: 0.0039 - learning_rate: 1.0340e-28\n",
      "Epoch 913/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7618e-05 - mae: 0.0037 - val_loss: 6.9175e-05 - val_mae: 0.0039 - learning_rate: 1.0340e-28\n",
      "Epoch 914/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7469e-05 - mae: 0.0037 - val_loss: 6.9193e-05 - val_mae: 0.0039 - learning_rate: 1.0340e-28\n",
      "Epoch 915/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7655e-05 - mae: 0.0037 - val_loss: 6.9183e-05 - val_mae: 0.0039 - learning_rate: 1.0340e-28\n",
      "Epoch 916/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7891e-05 - mae: 0.0037 - val_loss: 6.9181e-05 - val_mae: 0.0039 - learning_rate: 1.0340e-28\n",
      "Epoch 917/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.8326e-05 - mae: 0.0038 - val_loss: 6.9162e-05 - val_mae: 0.0039 - learning_rate: 1.0340e-28\n",
      "Epoch 918/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.9175e-05 - mae: 0.0038\n",
      "Epoch 918: ReduceLROnPlateau reducing learning rate to 5.169879074012491e-29.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9014e-05 - mae: 0.0038 - val_loss: 6.9161e-05 - val_mae: 0.0039 - learning_rate: 1.0340e-28\n",
      "Epoch 919/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6687e-05 - mae: 0.0037 - val_loss: 6.9166e-05 - val_mae: 0.0039 - learning_rate: 5.1699e-29\n",
      "Epoch 920/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9469e-05 - mae: 0.0038 - val_loss: 6.9188e-05 - val_mae: 0.0039 - learning_rate: 5.1699e-29\n",
      "Epoch 921/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7798e-05 - mae: 0.0037 - val_loss: 6.9210e-05 - val_mae: 0.0039 - learning_rate: 5.1699e-29\n",
      "Epoch 922/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7016e-05 - mae: 0.0037 - val_loss: 6.9223e-05 - val_mae: 0.0039 - learning_rate: 5.1699e-29\n",
      "Epoch 923/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.6610e-05 - mae: 0.0037 - val_loss: 6.9221e-05 - val_mae: 0.0039 - learning_rate: 5.1699e-29\n",
      "Epoch 924/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5224e-05 - mae: 0.0036 - val_loss: 6.9224e-05 - val_mae: 0.0039 - learning_rate: 5.1699e-29\n",
      "Epoch 925/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9907e-05 - mae: 0.0039 - val_loss: 6.9224e-05 - val_mae: 0.0039 - learning_rate: 5.1699e-29\n",
      "Epoch 926/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.8516e-05 - mae: 0.0038 - val_loss: 6.9214e-05 - val_mae: 0.0039 - learning_rate: 5.1699e-29\n",
      "Epoch 927/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5985e-05 - mae: 0.0037 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 5.1699e-29\n",
      "Epoch 928/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.6661e-05 - mae: 0.0037\n",
      "Epoch 928: ReduceLROnPlateau reducing learning rate to 2.5849395370062454e-29.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6782e-05 - mae: 0.0037 - val_loss: 6.9205e-05 - val_mae: 0.0039 - learning_rate: 5.1699e-29\n",
      "Epoch 929/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.6289e-05 - mae: 0.0037 - val_loss: 6.9227e-05 - val_mae: 0.0039 - learning_rate: 2.5849e-29\n",
      "Epoch 930/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9684e-05 - mae: 0.0038 - val_loss: 6.9241e-05 - val_mae: 0.0039 - learning_rate: 2.5849e-29\n",
      "Epoch 931/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1656e-05 - mae: 0.0038 - val_loss: 6.9225e-05 - val_mae: 0.0039 - learning_rate: 2.5849e-29\n",
      "Epoch 932/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5940e-05 - mae: 0.0037 - val_loss: 6.9219e-05 - val_mae: 0.0039 - learning_rate: 2.5849e-29\n",
      "Epoch 933/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5002e-05 - mae: 0.0036 - val_loss: 6.9231e-05 - val_mae: 0.0039 - learning_rate: 2.5849e-29\n",
      "Epoch 934/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6227e-05 - mae: 0.0037 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 2.5849e-29\n",
      "Epoch 935/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8709e-05 - mae: 0.0038 - val_loss: 6.9193e-05 - val_mae: 0.0039 - learning_rate: 2.5849e-29\n",
      "Epoch 936/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9477e-05 - mae: 0.0038 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 2.5849e-29\n",
      "Epoch 937/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7834e-05 - mae: 0.0038 - val_loss: 6.9201e-05 - val_mae: 0.0039 - learning_rate: 2.5849e-29\n",
      "Epoch 938/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.7212e-05 - mae: 0.0037\n",
      "Epoch 938: ReduceLROnPlateau reducing learning rate to 1.2924697685031227e-29.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7253e-05 - mae: 0.0037 - val_loss: 6.9226e-05 - val_mae: 0.0039 - learning_rate: 2.5849e-29\n",
      "Epoch 939/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6440e-05 - mae: 0.0037 - val_loss: 6.9210e-05 - val_mae: 0.0039 - learning_rate: 1.2925e-29\n",
      "Epoch 940/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7014e-05 - mae: 0.0037 - val_loss: 6.9201e-05 - val_mae: 0.0039 - learning_rate: 1.2925e-29\n",
      "Epoch 941/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.0700e-05 - mae: 0.0038 - val_loss: 6.9181e-05 - val_mae: 0.0039 - learning_rate: 1.2925e-29\n",
      "Epoch 942/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7779e-05 - mae: 0.0038 - val_loss: 6.9164e-05 - val_mae: 0.0039 - learning_rate: 1.2925e-29\n",
      "Epoch 943/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8316e-05 - mae: 0.0037 - val_loss: 6.9172e-05 - val_mae: 0.0039 - learning_rate: 1.2925e-29\n",
      "Epoch 944/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9309e-05 - mae: 0.0038 - val_loss: 6.9156e-05 - val_mae: 0.0039 - learning_rate: 1.2925e-29\n",
      "Epoch 945/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6498e-05 - mae: 0.0037 - val_loss: 6.9177e-05 - val_mae: 0.0039 - learning_rate: 1.2925e-29\n",
      "Epoch 946/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0945e-05 - mae: 0.0038 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 1.2925e-29\n",
      "Epoch 947/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.0099e-05 - mae: 0.0038 - val_loss: 6.9208e-05 - val_mae: 0.0039 - learning_rate: 1.2925e-29\n",
      "Epoch 948/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.7116e-05 - mae: 0.0037\n",
      "Epoch 948: ReduceLROnPlateau reducing learning rate to 6.462348842515614e-30.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7191e-05 - mae: 0.0037 - val_loss: 6.9229e-05 - val_mae: 0.0039 - learning_rate: 1.2925e-29\n",
      "Epoch 949/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6625e-05 - mae: 0.0037 - val_loss: 6.9239e-05 - val_mae: 0.0039 - learning_rate: 6.4623e-30\n",
      "Epoch 950/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5651e-05 - mae: 0.0037 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 6.4623e-30\n",
      "Epoch 951/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8590e-05 - mae: 0.0038 - val_loss: 6.9204e-05 - val_mae: 0.0039 - learning_rate: 6.4623e-30\n",
      "Epoch 952/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5359e-05 - mae: 0.0037 - val_loss: 6.9217e-05 - val_mae: 0.0039 - learning_rate: 6.4623e-30\n",
      "Epoch 953/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8682e-05 - mae: 0.0038 - val_loss: 6.9189e-05 - val_mae: 0.0039 - learning_rate: 6.4623e-30\n",
      "Epoch 954/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5401e-05 - mae: 0.0036 - val_loss: 6.9195e-05 - val_mae: 0.0039 - learning_rate: 6.4623e-30\n",
      "Epoch 955/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5727e-05 - mae: 0.0037 - val_loss: 6.9201e-05 - val_mae: 0.0039 - learning_rate: 6.4623e-30\n",
      "Epoch 956/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7434e-05 - mae: 0.0037 - val_loss: 6.9173e-05 - val_mae: 0.0039 - learning_rate: 6.4623e-30\n",
      "Epoch 957/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5771e-05 - mae: 0.0036 - val_loss: 6.9163e-05 - val_mae: 0.0039 - learning_rate: 6.4623e-30\n",
      "Epoch 958/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.1246e-05 - mae: 0.0039\n",
      "Epoch 958: ReduceLROnPlateau reducing learning rate to 3.231174421257807e-30.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0872e-05 - mae: 0.0039 - val_loss: 6.9136e-05 - val_mae: 0.0039 - learning_rate: 6.4623e-30\n",
      "Epoch 959/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6576e-05 - mae: 0.0037 - val_loss: 6.9132e-05 - val_mae: 0.0039 - learning_rate: 3.2312e-30\n",
      "Epoch 960/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9254e-05 - mae: 0.0037 - val_loss: 6.9141e-05 - val_mae: 0.0039 - learning_rate: 3.2312e-30\n",
      "Epoch 961/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8087e-05 - mae: 0.0038 - val_loss: 6.9135e-05 - val_mae: 0.0039 - learning_rate: 3.2312e-30\n",
      "Epoch 962/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.2256e-05 - mae: 0.0039 - val_loss: 6.9145e-05 - val_mae: 0.0039 - learning_rate: 3.2312e-30\n",
      "Epoch 963/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6366e-05 - mae: 0.0037 - val_loss: 6.9140e-05 - val_mae: 0.0039 - learning_rate: 3.2312e-30\n",
      "Epoch 964/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7036e-05 - mae: 0.0037 - val_loss: 6.9131e-05 - val_mae: 0.0039 - learning_rate: 3.2312e-30\n",
      "Epoch 965/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7850e-05 - mae: 0.0038 - val_loss: 6.9118e-05 - val_mae: 0.0039 - learning_rate: 3.2312e-30\n",
      "Epoch 966/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7449e-05 - mae: 0.0037 - val_loss: 6.9156e-05 - val_mae: 0.0039 - learning_rate: 3.2312e-30\n",
      "Epoch 967/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9232e-05 - mae: 0.0038 - val_loss: 6.9114e-05 - val_mae: 0.0039 - learning_rate: 3.2312e-30\n",
      "Epoch 968/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8015e-05 - mae: 0.0038\n",
      "Epoch 968: ReduceLROnPlateau reducing learning rate to 1.6155872106289034e-30.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7959e-05 - mae: 0.0038 - val_loss: 6.9104e-05 - val_mae: 0.0039 - learning_rate: 3.2312e-30\n",
      "Epoch 969/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.5063e-05 - mae: 0.0036 - val_loss: 6.9109e-05 - val_mae: 0.0039 - learning_rate: 1.6156e-30\n",
      "Epoch 970/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0436e-05 - mae: 0.0038 - val_loss: 6.9125e-05 - val_mae: 0.0039 - learning_rate: 1.6156e-30\n",
      "Epoch 971/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0004e-05 - mae: 0.0038 - val_loss: 6.9152e-05 - val_mae: 0.0039 - learning_rate: 1.6156e-30\n",
      "Epoch 972/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9224e-05 - mae: 0.0038 - val_loss: 6.9161e-05 - val_mae: 0.0039 - learning_rate: 1.6156e-30\n",
      "Epoch 973/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5508e-05 - mae: 0.0037 - val_loss: 6.9177e-05 - val_mae: 0.0039 - learning_rate: 1.6156e-30\n",
      "Epoch 974/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.9759e-05 - mae: 0.0038 - val_loss: 6.9156e-05 - val_mae: 0.0039 - learning_rate: 1.6156e-30\n",
      "Epoch 975/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7763e-05 - mae: 0.0037 - val_loss: 6.9183e-05 - val_mae: 0.0039 - learning_rate: 1.6156e-30\n",
      "Epoch 976/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9447e-05 - mae: 0.0038 - val_loss: 6.9181e-05 - val_mae: 0.0039 - learning_rate: 1.6156e-30\n",
      "Epoch 977/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8652e-05 - mae: 0.0038 - val_loss: 6.9178e-05 - val_mae: 0.0039 - learning_rate: 1.6156e-30\n",
      "Epoch 978/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.4204e-05 - mae: 0.0036\n",
      "Epoch 978: ReduceLROnPlateau reducing learning rate to 8.077936053144517e-31.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4499e-05 - mae: 0.0036 - val_loss: 6.9180e-05 - val_mae: 0.0039 - learning_rate: 1.6156e-30\n",
      "Epoch 979/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.1338e-05 - mae: 0.0039 - val_loss: 6.9172e-05 - val_mae: 0.0039 - learning_rate: 8.0779e-31\n",
      "Epoch 980/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9226e-05 - mae: 0.0038 - val_loss: 6.9178e-05 - val_mae: 0.0039 - learning_rate: 8.0779e-31\n",
      "Epoch 981/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9042e-05 - mae: 0.0038 - val_loss: 6.9185e-05 - val_mae: 0.0039 - learning_rate: 8.0779e-31\n",
      "Epoch 982/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7179e-05 - mae: 0.0037 - val_loss: 6.9208e-05 - val_mae: 0.0039 - learning_rate: 8.0779e-31\n",
      "Epoch 983/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7380e-05 - mae: 0.0037 - val_loss: 6.9218e-05 - val_mae: 0.0039 - learning_rate: 8.0779e-31\n",
      "Epoch 984/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.0242e-05 - mae: 0.0038 - val_loss: 6.9228e-05 - val_mae: 0.0039 - learning_rate: 8.0779e-31\n",
      "Epoch 985/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8061e-05 - mae: 0.0038 - val_loss: 6.9222e-05 - val_mae: 0.0039 - learning_rate: 8.0779e-31\n",
      "Epoch 986/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7283e-05 - mae: 0.0037 - val_loss: 6.9209e-05 - val_mae: 0.0039 - learning_rate: 8.0779e-31\n",
      "Epoch 987/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4611e-05 - mae: 0.0036 - val_loss: 6.9207e-05 - val_mae: 0.0039 - learning_rate: 8.0779e-31\n",
      "Epoch 988/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8105e-05 - mae: 0.0038\n",
      "Epoch 988: ReduceLROnPlateau reducing learning rate to 4.0389680265722585e-31.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8118e-05 - mae: 0.0038 - val_loss: 6.9213e-05 - val_mae: 0.0039 - learning_rate: 8.0779e-31\n",
      "Epoch 989/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.8840e-05 - mae: 0.0038 - val_loss: 6.9230e-05 - val_mae: 0.0039 - learning_rate: 4.0390e-31\n",
      "Epoch 990/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9581e-05 - mae: 0.0038 - val_loss: 6.9247e-05 - val_mae: 0.0039 - learning_rate: 4.0390e-31\n",
      "Epoch 991/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8968e-05 - mae: 0.0037 - val_loss: 6.9236e-05 - val_mae: 0.0039 - learning_rate: 4.0390e-31\n",
      "Epoch 992/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9618e-05 - mae: 0.0038 - val_loss: 6.9245e-05 - val_mae: 0.0039 - learning_rate: 4.0390e-31\n",
      "Epoch 993/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7435e-05 - mae: 0.0037 - val_loss: 6.9240e-05 - val_mae: 0.0039 - learning_rate: 4.0390e-31\n",
      "Epoch 994/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7289e-05 - mae: 0.0037 - val_loss: 6.9232e-05 - val_mae: 0.0039 - learning_rate: 4.0390e-31\n",
      "Epoch 995/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0987e-05 - mae: 0.0039 - val_loss: 6.9235e-05 - val_mae: 0.0039 - learning_rate: 4.0390e-31\n",
      "Epoch 996/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6819e-05 - mae: 0.0037 - val_loss: 6.9254e-05 - val_mae: 0.0039 - learning_rate: 4.0390e-31\n",
      "Epoch 997/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0761e-05 - mae: 0.0038 - val_loss: 6.9263e-05 - val_mae: 0.0039 - learning_rate: 4.0390e-31\n",
      "Epoch 998/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8960e-05 - mae: 0.0037\n",
      "Epoch 998: ReduceLROnPlateau reducing learning rate to 2.0194840132861292e-31.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8807e-05 - mae: 0.0037 - val_loss: 6.9276e-05 - val_mae: 0.0039 - learning_rate: 4.0390e-31\n",
      "Epoch 999/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.2016e-05 - mae: 0.0039 - val_loss: 6.9261e-05 - val_mae: 0.0039 - learning_rate: 2.0195e-31\n",
      "Epoch 1000/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5887e-05 - mae: 0.0037 - val_loss: 6.9230e-05 - val_mae: 0.0039 - learning_rate: 2.0195e-31\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with a different optimizer and a learning rate scheduler\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "\n",
    "model3.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model with a learning rate scheduler\n",
    "history = model3.fit(X_train, y_train, epochs=1000, batch_size=16, validation_split=0.2, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ab56c-bd06-418e-8c30-d62aee89c397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9632b842-84d7-4682-a3d0-1ef031a1c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.7733e-05 - mae: 0.0037\n",
      "Model Loss: 6.717447831761092e-05, Model MAE: 0.0036949573550373316\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, mae = model3.evaluate(X_train, y_train)\n",
    "print(f'Model Loss: {loss}, Model MAE: {mae}')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation MAE values\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('Model Mean Absolute Error')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "792f0f6e-638b-4533-a041-e063878bf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"model1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0306043-f9cb-447d-9e26-da0f0ee9acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"model2.keras\")\n",
    "model3.save(\"model3.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736039e8-31f1-480f-9b9c-7e82217ad421",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "684ca69c-7fe8-4cd4-adcf-b56f3e00b11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "SKU_1L: [{'x': -5.681393e-05, 'y': -0.00039864564, 'z': 0.00087068713}, {'x': -0.0018281225, 'y': 0.00022958325, 'z': 0.1877604}, {'x': -0.00090617675, 'y': 0.00081575214, 'z': 0.36506426}, {'x': -0.00018426236, 'y': -0.0014295169, 'z': 0.544562}, {'x': 9.821801e-05, 'y': -0.0011893762, 'z': 0.73143506}]\n",
      "SKU_4L: [{'x': 0.53635365, 'y': 0.4310443, 'z': 0.5353486}, {'x': 0.53382134, 'y': 0.43304253, 'z': 0.6983415}, {'x': 0.5469605, 'y': 0.34849215, 'z': 0.74686366}, {'x': 0.54794765, 'y': 0.35847765, 'z': 0.7070441}, {'x': 0.5521937, 'y': 0.34124887, 'z': 0.66037756}]\n",
      "SKU_10L_DTS: [{'x': 0.86935145, 'y': 0.4742374, 'z': 0.43957523}, {'x': 0.8721701, 'y': 0.46514407, 'z': 0.49745324}, {'x': 0.8767318, 'y': 0.4555812, 'z': 0.5699282}, {'x': 0.878675, 'y': 0.44839793, 'z': 0.7117595}, {'x': 0.8828234, 'y': 0.46183386, 'z': 0.60610116}]\n",
      "SKU_10L_NDTS: [{'x': 1.7544872, 'y': 0.3869229, 'z': 0.5566132}, {'x': 1.762311, 'y': 0.37293622, 'z': 0.64355886}, {'x': 1.7621639, 'y': 0.39066207, 'z': 0.6221144}, {'x': 1.7613707, 'y': 0.42177996, 'z': 0.5644631}, {'x': 1.762781, 'y': 0.42771903, 'z': 0.61198556}]\n",
      "SKU_20L_DTS: [{'x': 2.7645502, 'y': 0.38489503, 'z': 0.63169974}, {'x': 2.7718983, 'y': 0.31613827, 'z': 0.5633661}, {'x': 2.7754118, 'y': 0.31560415, 'z': 0.61430025}, {'x': 2.77528, 'y': 0.32132462, 'z': 0.64816445}, {'x': 2.7752657, 'y': 0.3291729, 'z': 0.59962666}]\n",
      "SKU_20L_NDTS: [{'x': 2.7538128, 'y': 0.3591417, 'z': 0.45015436}, {'x': 2.7390816, 'y': 0.35885468, 'z': 0.43694514}, {'x': 2.7427602, 'y': 0.34070852, 'z': 0.4877808}, {'x': 2.7097824, 'y': 0.3005226, 'z': 0.5398156}, {'x': 2.7108324, 'y': 0.30515346, 'z': 0.51722276}]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the model is already trained and loaded\n",
    "\n",
    "# Example input data\n",
    "input_data = {\n",
    "    'Length': 80,\n",
    "    'Width': 31,\n",
    "    'Height': 20,\n",
    "    'SKU_1L': 200,\n",
    "    'SKU_4L': 120,\n",
    "    'SKU_10L_DTS': 300,\n",
    "    'SKU_10L_NDTS': 396,\n",
    "    'SKU_20L_DTS': 231,\n",
    "    'SKU_20L_NDTS': 165\n",
    "}\n",
    "\n",
    "# Convert input data to list\n",
    "input_list = [\n",
    "    input_data['Length'],\n",
    "    input_data['Width'],\n",
    "    input_data['Height'],\n",
    "    input_data['SKU_1L'],\n",
    "    input_data['SKU_4L'],\n",
    "    input_data['SKU_10L_DTS'],\n",
    "    input_data['SKU_10L_NDTS'],\n",
    "    input_data['SKU_20L_DTS'],\n",
    "    input_data['SKU_20L_NDTS']\n",
    "]\n",
    "\n",
    "# Normalize input data if needed\n",
    "input_list = tf.keras.utils.normalize([input_list], axis=1)\n",
    "\n",
    "# Make predictions using the model\n",
    "predicted_positions = model1.predict(input_list)\n",
    "\n",
    "# Assuming y_train was normalized, denormalize the predictions\n",
    "max_position_value = 100  # Replace with the actual value used for normalization\n",
    "predicted_positions = predicted_positions * max_y_train_value\n",
    "\n",
    "# Flatten the predicted positions if needed\n",
    "predicted_positions = predicted_positions.flatten()\n",
    "\n",
    "# Convert the predicted positions into a structured format\n",
    "sku_names = [\"SKU_1L\", \"SKU_4L\", \"SKU_10L_DTS\", \"SKU_10L_NDTS\", \"SKU_20L_DTS\", \"SKU_20L_NDTS\"]\n",
    "predicted_output = {}\n",
    "position_index = 0\n",
    "\n",
    "# Assuming each SKU has multiple positions, distribute the positions accordingly\n",
    "for sku in sku_names:\n",
    "    sku_positions = []\n",
    "    for _ in range(input_data[sku]):\n",
    "        position = {\n",
    "            'x': predicted_positions[position_index],\n",
    "            'y': predicted_positions[position_index + 1],\n",
    "            'z': predicted_positions[position_index + 2]\n",
    "        }\n",
    "        sku_positions.append(position)\n",
    "        position_index += 3\n",
    "    predicted_output[sku] = sku_positions\n",
    "\n",
    "# Print the predicted output\n",
    "for sku, positions in predicted_output.items():\n",
    "    print(f\"{sku}: {positions[:5]}\")  # Print the first 5 positions for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95935b5a-7e30-4a40-80b0-d1ceb94dedf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
